{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93dc2c6f",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6360918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df94f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import wandb\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9add1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neumeta.models import create_densenet_model as create_model\n",
    "from neumeta.utils import (\n",
    "    parse_args, print_omegaconf,\n",
    "    load_checkpoint, save_checkpoint,\n",
    "    set_seed,\n",
    "    get_dataset,\n",
    "    sample_coordinates, sample_subset, shuffle_coordinates_all,\n",
    "    get_hypernetwork, get_optimizer,\n",
    "    sample_weights,\n",
    "    weighted_regression_loss, validate_single, AverageMeter, EMA,\n",
    "    sample_merge_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3531f8",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df3637",
   "metadata": {},
   "source": [
    "### Find max dimension of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9f1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_dim(model_cls):\n",
    "    \"\"\"Find maximum dimension of the model\"\"\"\n",
    "    # Get the learnable parameters of the model\n",
    "    checkpoint = model_cls.learnable_parameter \n",
    "\n",
    "    # Set the maximum value to the length of the checkpoint\n",
    "    max_value = len(checkpoint)\n",
    "\n",
    "    # Iterate over the new model's weight\n",
    "    for i, (k, tensor) in enumerate(checkpoint.items()):\n",
    "        # Handle 2D tensors (e.g., weight matrices) \n",
    "        if len(tensor.shape) == 4:\n",
    "            coords = [tensor.shape[0], tensor.shape[1]]\n",
    "            max_value = max(max_value, max(coords))\n",
    "        # Handle 1D tensors (e.g., biases)\n",
    "        elif len(tensor.shape) == 1:\n",
    "            max_value = max(max_value, tensor.shape[0])\n",
    "    \n",
    "    return max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c098c",
   "metadata": {},
   "source": [
    "### Initialize wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e70c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wandb(config):\n",
    "    import time\n",
    "    \"\"\"\n",
    "    Initializes Weights and Biases (wandb) with the given configuration.\n",
    "    \n",
    "    Args:\n",
    "        configuration (dict): Configuration parameters for the run.\n",
    "    \"\"\"\n",
    "    # Name the run using current time and configuration name\n",
    "    run_name = f\"{time.strftime('%Y%m%d%H%M%S')}-{config.experiment.name}\"\n",
    "    \n",
    "    wandb.init(project=\"dense-inr-trial\", name=run_name, config=dict(config), group='cifar10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0bc703",
   "metadata": {},
   "source": [
    "### Init model dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319a2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_dict(args, device):\n",
    "    \"\"\"\n",
    "    Initializes a dictionary of models for each dimension in the given range, along with ground truth models for the starting dimension.\n",
    "\n",
    "    Args:\n",
    "        args: An object containing the arguments for initializing the models.\n",
    "\n",
    "    Returns:\n",
    "        dim_dict: A dictionary containing the models for each dimension, along with their corresponding coordinates, keys, indices, size, and ground truth models.\n",
    "        gt_model_dict: A dictionary containing the ground truth models for the starting dimension.\n",
    "    \"\"\"\n",
    "    dim_dict = {}\n",
    "    gt_model_dict = {}\n",
    "    \n",
    "    # Create a model for each dimension in dimensions range\n",
    "    for dim in args.dimensions.range:\n",
    "        model_cls = create_model(args.model.type,\n",
    "                                 layers=args.model.layers,\n",
    "                                 growth=args.model.growth,\n",
    "                                 compression=args.model.compression,\n",
    "                                 bottleneck=args.model.bottleneck,\n",
    "                                 drop_rate=args.model.drop_rate,\n",
    "                                 hidden_dim=dim,\n",
    "                                 path=args.model.pretrained_path).to(device)\n",
    "        # Sample the coordinates, keys, indices, and the size for the model\n",
    "        coords_tensor, keys_list, indices_list, size_list = sample_coordinates(model_cls)\n",
    "        # Add the model, coordinates, keys, indices, size, and key mask to the dictionary\n",
    "        dim_dict[f\"{dim}\"] = (model_cls, coords_tensor, keys_list, indices_list, size_list, None)\n",
    "\n",
    "        # Print to makes line better\n",
    "        print('\\n')\n",
    "        \n",
    "        # If the dimension is the starting dimension (the dimension of pretrained_model), add the ground truth model to the dictionary\n",
    "        if dim == args.dimensions.start:\n",
    "            print(f\"Loading model for dim {dim}\")\n",
    "            model_trained = create_model(args.model.type,\n",
    "                                         layers=args.model.layers,\n",
    "                                         growth=args.model.growth,\n",
    "                                         compression=args.model.compression,\n",
    "                                         bottleneck=args.model.bottleneck,\n",
    "                                         drop_rate=args.model.drop_rate,\n",
    "                                         path=args.model.pretrained_path,\n",
    "                                         smooth=True,\n",
    "                                         hidden_dim=dim).to(device)\n",
    "            model_trained.eval()\n",
    "            gt_model_dict[f'{dim}'] = model_trained\n",
    "\n",
    "    \n",
    "    return dim_dict, gt_model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93b863",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fde4cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model for one epoch\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, dim_dict, gt_model_dict, epoch_idx, ema=None, args=None, device='cpu'):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize AverageMeter objects to track the losses\n",
    "    losses = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    reg_losses = AverageMeter()\n",
    "    reconstruct_losses = AverageMeter()\n",
    "\n",
    "    # Training accuracy\n",
    "    preds = []\n",
    "    gt = []\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Preprocess input\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        # Move the data to the device\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        # Choose a random hidden dimension\n",
    "        hidden_dim = random.choice(args.dimensions.range)\n",
    "        # Get the model class, coordinates, keys, indices, size, and key mask for the chosen dimension\n",
    "        model_cls, coords_tensor, keys_list, indices_list, size_list, key_mask = dim_dict[f\"{hidden_dim}\"]\n",
    "        # Sample a subset the input tensor of the coordinates, keys, indices, size, and selected keys\n",
    "        coords_tensor, keys_list, indices_list, size_list, selected_keys = sample_subset(coords_tensor,\n",
    "                                                                                         keys_list,\n",
    "                                                                                         indices_list,\n",
    "                                                                                         size_list,\n",
    "                                                                                         key_mask,\n",
    "                                                                                         ratio=args.ratio)\n",
    "        # Add noise to the coordinates if specified\n",
    "        if args.training.coordinate_noise > 0.0:\n",
    "            coords_tensor = coords_tensor + (torch.rand_like(coords_tensor) - 0.5) * args.training.coordinate_noise\n",
    "\n",
    "\n",
    "        # Main task of hypernetwork and target network\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        # Sample the weights for the target model using hypernetwork\n",
    "        model_cls, reconstructed_weights = sample_weights(model, model_cls,\n",
    "                                                          coords_tensor, keys_list, indices_list, size_list, key_mask, selected_keys,\n",
    "                                                          device=device, NORM=args.dimensions.norm)\n",
    "        # Forward pass\n",
    "        predict = model_cls(x)\n",
    "        \n",
    "        # Sample test model to see training accuracy\n",
    "\n",
    "        pred = torch.argmax(predict, dim=-1)\n",
    "\n",
    "        preds.append(pred)\n",
    "        gt.append(target)\n",
    "\n",
    "        # Compute losses\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        # Compute classification loss\n",
    "        cls_loss = criterion(predict, target) \n",
    "        # Compute regularization loss\n",
    "        reg_loss = sum([torch.norm(w, p=2) for w in reconstructed_weights])\n",
    "        # Compute reconstruction loss if ground truth model is available\n",
    "        if f\"{hidden_dim}\" in gt_model_dict:\n",
    "            gt_model = gt_model_dict[f\"{hidden_dim}\"]\n",
    "            gt_selected_weights = [\n",
    "                w for k, w in gt_model.learnable_parameter.items() if k in selected_keys]\n",
    "\n",
    "            reconstruct_loss = weighted_regression_loss(\n",
    "                reconstructed_weights, gt_selected_weights)\n",
    "        else:\n",
    "            reconstruct_loss = torch.tensor(0.0)\n",
    "        # Compute the total loss\n",
    "        loss = args.hyper_model.loss_weight.ce_weight * cls_loss + args.hyper_model.loss_weight.reg_weight * \\\n",
    "            reg_loss + args.hyper_model.loss_weight.recon_weight * reconstruct_loss\n",
    "\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        # Zero the gradients of the updated weights\n",
    "        for updated_weight in model_cls.parameters():\n",
    "            updated_weight.grad = None\n",
    "\n",
    "        # Compute the gradients of the reconstructed weights\n",
    "        loss.backward(retain_graph=True)\n",
    "        torch.autograd.backward(reconstructed_weights, [\n",
    "                                w.grad for k, w in model_cls.named_parameters() if k in selected_keys])\n",
    "        \n",
    "        # Clip the gradients if specified\n",
    "        if args.training.get('clip_grad', 0.0) > 0:\n",
    "            torch.nn.utils.clip_grad_value_(\n",
    "                model.parameters(), args.training.clip_grad)\n",
    "            \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the EMA if specified\n",
    "        if ema:\n",
    "            ema.update()  # Update the EMA after each training step\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update the AverageMeter objects\n",
    "        losses.update(loss.item())\n",
    "        cls_losses.update(cls_loss.item())\n",
    "        reg_losses.update(reg_loss.item())\n",
    "        reconstruct_losses.update(reconstruct_loss.item())\n",
    "\n",
    "        # Log (or plot) losses\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        # Log the losses and learning rate to wandb\n",
    "        if batch_idx % args.experiment.log_interval == 0:\n",
    "            wandb.log({\n",
    "                \"Loss\": losses.avg,\n",
    "                \"Cls Loss\": cls_losses.avg,\n",
    "                \"Reg Loss\": reg_losses.avg,\n",
    "                \"Reconstruct Loss\": reconstruct_losses.avg,\n",
    "                \"Learning rate\": optimizer.param_groups[0]['lr']\n",
    "            }, step=batch_idx + epoch_idx * len(train_loader))\n",
    "            # Print the losses and learning rate\n",
    "            print(\n",
    "                f\"Iteration {batch_idx}: Loss = {losses.avg:.4f}, Reg Loss = {reg_losses.avg:.4f}, Reconstruct Loss = {reconstruct_losses.avg:.4f}, Cls Loss = {cls_losses.avg:.4f}, Learning rate = {optimizer.param_groups[0]['lr']:.4e}\")\n",
    "    \n",
    "    train_acc = accuracy_score(torch.cat(gt).cpu().numpy(), torch.cat(preds).cpu().numpy())\n",
    "\n",
    "    wandb.log({\n",
    "        \"Training accuracy\": train_acc\n",
    "    })\n",
    "\n",
    "    # Returns the training loss, structure of network in each dimension, and the original structure of pretrained network\n",
    "    return losses.avg, dim_dict, gt_model_dict, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb560cd",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041282c",
   "metadata": {},
   "source": [
    "### 0 Set device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5bcf973",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6df804",
   "metadata": {},
   "source": [
    "### 1 Parsing arguments for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5570d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = 'neumeta/config/densenet_inr_train/dense_21th_experiment.yaml'\n",
    "RATIO = '1.0'\n",
    "CHECKPOINT_PATH = 'toy/experiments_densenet/dense_21th_experiment/cifar10_nerf_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92954510",
   "metadata": {},
   "outputs": [],
   "source": [
    "argv_train = ['--config', CONFIG_PATH, '--ratio', RATIO, '--resume_from', CHECKPOINT_PATH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1867bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+---------------------------------------------------------------------------------------------------+\n",
      "|                 Key                  |                                               Value                                               |\n",
      "+--------------------------------------+---------------------------------------------------------------------------------------------------+\n",
      "|           experiment.name            |                    densenet_train_36_48_mlp_256_4_coordnoise_unsmooth_50e_bs64                    |\n",
      "|        experiment.num_epochs         |                                                 50                                                |\n",
      "|       experiment.log_interval        |                                                 50                                                |\n",
      "|       experiment.eval_interval       |                                                 1                                                 |\n",
      "|           experiment.seed            |                                                 42                                                |\n",
      "|              model.type              |                                              DenseNet                                             |\n",
      "|        model.pretrained_path         |       toy/experiments/densenet_bc_40_12_baseline/densenet_bc_40_12_cifar10_baseline_best.pth      |\n",
      "|             model.layers             |                                                 40                                                |\n",
      "|             model.growth             |                                                 12                                                |\n",
      "|          model.compression           |                                                0.5                                                |\n",
      "|           model.bottleneck           |                                                True                                               |\n",
      "|           model.drop_rate            |                                                0.0                                                |\n",
      "|           training.dataset           |                                              cifar10                                              |\n",
      "|         training.batch_size          |                                                 64                                                |\n",
      "|      training.coordinate_noise       |                                                0.1                                                |\n",
      "|        training.learning_rate        |                                               0.001                                               |\n",
      "|          training.lr_steps           |                                             [100, 150]                                            |\n",
      "|          training.optimizer          |                                               adamw                                               |\n",
      "|        training.weight_decay         |                                                0.01                                               |\n",
      "|          training.clip_grad          |                                                10.0                                               |\n",
      "|       training.save_model_path       |            toy/experiments/densenet_train_36_48_mlp_256_4_coordnoise_unsmooth_50e_bs64            |\n",
      "|           hyper_model.type           |                                               resmlp                                              |\n",
      "|        hyper_model.input_dim         |                                                 6                                                 |\n",
      "|        hyper_model.hidden_dim        |                                                128                                                |\n",
      "|        hyper_model.num_layers        |                                                 5                                                 |\n",
      "|        hyper_model.num_freqs         |                                                 16                                                |\n",
      "|        hyper_model.output_dim        |                                                 9                                                 |\n",
      "|        hyper_model.ema_decay         |                                               0.995                                               |\n",
      "|  hyper_model.loss_weight.ce_weight   |                                                1.0                                                |\n",
      "|  hyper_model.loss_weight.reg_weight  |                                               1e-05                                               |\n",
      "| hyper_model.loss_weight.recon_weight |                                                0.1                                                |\n",
      "|  hyper_model.loss_weight.kd_weight   |                                                0.1                                                |\n",
      "|           dimensions.range           |                                                [48]                                               |\n",
      "|           dimensions.test            |                                                 24                                                |\n",
      "|           dimensions.norm            |                                                120                                                |\n",
      "|           dimensions.start           |                                                 48                                                |\n",
      "|                config                |      neumeta/config/densenet/densenet_train_36_48_mlp_256_4_coordnoise_unsmooth_50e_bs64.yaml     |\n",
      "|                ratio                 |                                                1.0                                                |\n",
      "|             resume_from              | toy/experiments/densenet_train_36_48_mlp_256_4_coordnoise_unsmooth_50e_bs64/cifar10_nerf_best.pth |\n",
      "|              load_from               |                                                None                                               |\n",
      "|           test_result_path           |                                                None                                               |\n",
      "|                 test                 |                                               False                                               |\n",
      "+--------------------------------------+---------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "args = parse_args(argv_train)  # Parse arguments\n",
    "print_omegaconf(args)  # Print arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d63002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed... 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.experiment.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1941f5",
   "metadata": {},
   "source": [
    "### 2 Get training and validation dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1732d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: cifar10 with batch size: 64 and strong transform: None\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_dataset('cifar10', args.training.batch_size, strong_transform=args.training.get('strong_aug', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1754c36",
   "metadata": {},
   "source": [
    "### 3 Create target model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f26b5",
   "metadata": {},
   "source": [
    "#### 3.0 Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12bde19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from toy/experiments/densenet_bc_40_12_baseline/densenet_bc_40_12_cifar10_baseline_best.pth\n"
     ]
    }
   ],
   "source": [
    "model = create_model(args.model.type,\n",
    "                     layers=args.model.layers,\n",
    "                     growth=args.model.growth,\n",
    "                     compression=args.model.compression,\n",
    "                     bottleneck=args.model.bottleneck,\n",
    "                     drop_rate=args.model.drop_rate,\n",
    "                     hidden_dim=args.dimensions.start,\n",
    "                     path=args.model.pretrained_path).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f2944",
   "metadata": {},
   "source": [
    "#### 3.1 Print the structure and shape of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2d72666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet3(\n",
       "  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (block1): DenseBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans1): TransitionBlock(\n",
       "    (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (block2): DenseBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans2): TransitionBlock(\n",
       "    (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv1): Conv2d(120, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (block3): DenseBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=132, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68fc0e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block3.layer.5.conv1.weight torch.Size([48, 120, 1, 1])\n",
      "block3.layer.5.conv1.bias torch.Size([48])\n",
      "block3.layer.5.conv2.weight torch.Size([12, 48, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for i, (k, tensor) in enumerate(model.learnable_parameter.items()):\n",
    "    print(k, tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18a708ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum DIM: 120\n"
     ]
    }
   ],
   "source": [
    "# Print the maximum dimension of the model\n",
    "print(f'Maximum DIM: {find_max_dim(model)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9451adb",
   "metadata": {},
   "source": [
    "#### 3.2 Validate the accuracy of pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cca4d5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 41.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Permutated model Validation Loss: 0.3239, Validation Accuracy: 91.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate the model for the starting dimension (its pretrained form)\n",
    "val_loss, acc = validate_single(model, val_loader, nn.CrossEntropyLoss(), args=args)\n",
    "print(f'Initial Permutated model Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d526b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = model.learnable_parameter\n",
    "number_param = len(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1a8b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters keys: ['block3.layer.5.conv1.weight', 'block3.layer.5.conv1.bias', 'block3.layer.5.conv2.weight']\n",
      "Number of parameters to be learned: 3\n"
     ]
    }
   ],
   "source": [
    "# Print the keys of the parameters and the number of parameters\n",
    "print(f\"Parameters keys: {model.keys}\")\n",
    "print(f\"Number of parameters to be learned: {number_param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a522bbe",
   "metadata": {},
   "source": [
    "### 4 Create hypernetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17279a4b",
   "metadata": {},
   "source": [
    "#### 4.0 Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6041a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper model type: resmlp\n",
      "Using scalar 0.1\n",
      "num_freqs:  16 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Get the hypermodel\n",
    "hyper_model = get_hypernetwork(args, number_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9ba74",
   "metadata": {},
   "source": [
    "#### 4.1 Print model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cd60865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeRF_ResMLP_Compose(\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (model): ModuleList(\n",
       "    (0-2): 3 x NeRF_MLP_Residual_Scaled(\n",
       "      (initial_layer): Linear(in_features=198, out_features=128, bias=True)\n",
       "      (residual_blocks): ModuleList(\n",
       "        (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (scalars): ParameterList(\n",
       "          (0): Parameter containing: [torch.float32 of size  (cuda:0)]\n",
       "          (1): Parameter containing: [torch.float32 of size  (cuda:0)]\n",
       "          (2): Parameter containing: [torch.float32 of size  (cuda:0)]\n",
       "          (3): Parameter containing: [torch.float32 of size  (cuda:0)]\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "      (output_layer): Linear(in_features=128, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e2cdf",
   "metadata": {},
   "source": [
    "#### 4.2 Initialize EMA to track only a smooth version of the model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afb25ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the EMA\n",
    "ema = EMA(hyper_model, decay=args.hyper_model.ema_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccb913",
   "metadata": {},
   "source": [
    "### 5 Get loss function, optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "308f44b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion, val_criterion, optimizer, scheduler = get_optimizer(args, hyper_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45181795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: CrossEntropyLoss()\n",
      "Val_criterion: CrossEntropyLoss()\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.MultiStepLR object at 0x000001AD2A6D4490>\n"
     ]
    }
   ],
   "source": [
    "print(f'Criterion: {criterion}\\nVal_criterion: {val_criterion}\\nOptimizer: {optimizer}\\nScheduler: {scheduler}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991ee56",
   "metadata": {},
   "source": [
    "### 6 Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81099a8",
   "metadata": {},
   "source": [
    "#### 6.1 Initialize training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "731bcd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the starting epoch and best accuracy\n",
    "start_epoch = 0\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b1a8f",
   "metadata": {},
   "source": [
    "#### 6.2 Directory to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91995285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory to save the model\n",
    "os.makedirs(args.training.save_model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1632f",
   "metadata": {},
   "source": [
    "#### 6.3 Resume training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cae570b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toy/experiments/densenet_train_36_48_mlp_256_4_coordnoise_unsmooth_50e_bs64/cifar10_nerf_best.pth'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf17e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.resume_from = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55b68075",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.resume_from:\n",
    "        print(f\"Resuming from checkpoint: {args.resume_from}\")\n",
    "        checkpoint_info = load_checkpoint(args.resume_from, hyper_model, optimizer, ema)\n",
    "        start_epoch = checkpoint_info['epoch']\n",
    "        best_acc = checkpoint_info['best_acc']\n",
    "        print(f\"Resuming from epoch: {start_epoch}, best accuracy: {best_acc*100:.2f}%\")\n",
    "        # Note: If there are more elements to retrieve, do so here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5024e53f",
   "metadata": {},
   "source": [
    "#### 6.4 Initialize model dictionary for each dimension and shuffle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3558b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from toy/experiments/densenet_bc_40_12_baseline/densenet_bc_40_12_cifar10_baseline_best.pth\n",
      "\n",
      "\n",
      "Loading model for dim 48\n",
      "Loading model from toy/experiments/densenet_bc_40_12_baseline/densenet_bc_40_12_cifar10_baseline_best.pth\n",
      "Smooth the parameters of the model\n",
      "Old TV original model: 428.7051086425781\n",
      "Permuted TV original model: 394.62835693359375\n"
     ]
    }
   ],
   "source": [
    "# Initialize model dictionary\n",
    "dim_dict, gt_model_dict = init_model_dict(args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4566bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet3(\n",
       "  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (block1): DenseBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans1): TransitionBlock(\n",
       "    (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (block2): DenseBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans2): TransitionBlock(\n",
       "    (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv1): Conv2d(120, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (block3): DenseBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): BottleneckBlock(\n",
       "        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=132, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_model_dict['48']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c64f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 44.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Permutated model Validation Loss: 0.3239, Validation Accuracy: 91.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate the model for the starting dimension (its pretrained form)\n",
    "val_loss, acc = validate_single(gt_model_dict['48'], val_loader, nn.CrossEntropyLoss(), args=args)\n",
    "print(f'Initial Permutated model Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8981361b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'48': (DenseNet3(\n",
       "    (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (block1): DenseBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (trans1): TransitionBlock(\n",
       "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (block2): DenseBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (trans2): TransitionBlock(\n",
       "      (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(120, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (block3): DenseBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "    (fc): Linear(in_features=132, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[  0.,   0.,   0.,   3.,  48., 120.],\n",
       "          [  0.,   0.,   1.,   3.,  48., 120.],\n",
       "          [  0.,   0.,   2.,   3.,  48., 120.],\n",
       "          ...,\n",
       "          [  2.,  11.,  45.,   3.,  12.,  48.],\n",
       "          [  2.,  11.,  46.,   3.,  12.,  48.],\n",
       "          [  2.,  11.,  47.,   3.,  12.,  48.]]),\n",
       "  array(['block3.layer.5.conv1.weight', 'block3.layer.5.conv1.weight',\n",
       "         'block3.layer.5.conv1.weight', ..., 'block3.layer.5.conv2.weight',\n",
       "         'block3.layer.5.conv2.weight', 'block3.layer.5.conv2.weight'],\n",
       "        dtype='<U27'),\n",
       "  tensor([[ 0,  0,  1,  1],\n",
       "          [ 0,  1,  1,  1],\n",
       "          [ 0,  2,  1,  1],\n",
       "          ...,\n",
       "          [11, 45,  3,  3],\n",
       "          [11, 46,  3,  3],\n",
       "          [11, 47,  3,  3]]),\n",
       "  tensor([4, 4, 4,  ..., 4, 4, 4]),\n",
       "  None)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b93cb60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'48': (DenseNet3(\n",
       "    (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (block1): DenseBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (trans1): TransitionBlock(\n",
       "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (block2): DenseBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (trans2): TransitionBlock(\n",
       "      (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(120, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (block3): DenseBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "    (fc): Linear(in_features=132, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[  0.,   0.,   0.,   3.,  48., 120.],\n",
       "          [  0.,   0.,   1.,   3.,  48., 120.],\n",
       "          [  0.,   0.,   2.,   3.,  48., 120.],\n",
       "          ...,\n",
       "          [  2.,  11.,  45.,   3.,  12.,  48.],\n",
       "          [  2.,  11.,  46.,   3.,  12.,  48.],\n",
       "          [  2.,  11.,  47.,   3.,  12.,  48.]]),\n",
       "  array(['block3.layer.5.conv1.weight', 'block3.layer.5.conv1.weight',\n",
       "         'block3.layer.5.conv1.weight', ..., 'block3.layer.5.conv2.weight',\n",
       "         'block3.layer.5.conv2.weight', 'block3.layer.5.conv2.weight'],\n",
       "        dtype='<U27'),\n",
       "  tensor([[ 0,  0,  1,  1],\n",
       "          [ 0,  1,  1,  1],\n",
       "          [ 0,  2,  1,  1],\n",
       "          ...,\n",
       "          [11, 45,  3,  3],\n",
       "          [11, 46,  3,  3],\n",
       "          [11, 47,  3,  3]]),\n",
       "  tensor([4, 4, 4,  ..., 4, 4, 4]),\n",
       "  {np.str_('block3.layer.5.conv1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('block3.layer.5.conv1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('block3.layer.5.conv2.weight'): tensor([0, 0, 0,  ..., 1, 1, 1])})}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_dict = shuffle_coordinates_all(dim_dict)\n",
    "dim_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20088d",
   "metadata": {},
   "source": [
    "#### 6.5 Initialize wandb for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "044696ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mefradosuryadi\u001b[0m (\u001b[33mefradosuryadi-universitas-indonesia\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Main\\Education\\Skripsi\\Codes\\1-experiment\\code-trials\\neumeta-trial\\wandb\\run-20250528_202813-ovmh4iyt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efradosuryadi-universitas-indonesia/dense-cifar-trial/runs/ovmh4iyt' target=\"_blank\">20250528202813-densenet_train_36_48_mlp_256_4_coordnoise_unsmooth_50e_bs64</a></strong> to <a href='https://wandb.ai/efradosuryadi-universitas-indonesia/dense-cifar-trial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efradosuryadi-universitas-indonesia/dense-cifar-trial' target=\"_blank\">https://wandb.ai/efradosuryadi-universitas-indonesia/dense-cifar-trial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efradosuryadi-universitas-indonesia/dense-cifar-trial/runs/ovmh4iyt' target=\"_blank\">https://wandb.ai/efradosuryadi-universitas-indonesia/dense-cifar-trial/runs/ovmh4iyt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initialize_wandb(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f2004",
   "metadata": {},
   "source": [
    "#### 6.6 Hypernetwork training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b78a21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.experiment.num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0b94e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 0.4199, Reg Loss = 1.7900, Reconstruct Loss = 0.0359, Cls Loss = 0.4163, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2422, Reg Loss = 6.5068, Reconstruct Loss = 0.1154, Cls Loss = 0.2306, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2397, Reg Loss = 5.9539, Reconstruct Loss = 0.0980, Cls Loss = 0.2299, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2359, Reg Loss = 5.3496, Reconstruct Loss = 0.0831, Cls Loss = 0.2275, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2359, Reg Loss = 5.0005, Reconstruct Loss = 0.0728, Cls Loss = 0.2286, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2336, Reg Loss = 4.7687, Reconstruct Loss = 0.0671, Cls Loss = 0.2269, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2319, Reg Loss = 4.6707, Reconstruct Loss = 0.0630, Cls Loss = 0.2256, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2315, Reg Loss = 4.5630, Reconstruct Loss = 0.0596, Cls Loss = 0.2255, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2315, Reg Loss = 4.5226, Reconstruct Loss = 0.0572, Cls Loss = 0.2257, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2305, Reg Loss = 4.5098, Reconstruct Loss = 0.0547, Cls Loss = 0.2250, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.2298, Reg Loss = 4.6599, Reconstruct Loss = 0.0533, Cls Loss = 0.2244, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.2286, Reg Loss = 4.7199, Reconstruct Loss = 0.0517, Cls Loss = 0.2234, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.2297, Reg Loss = 4.9414, Reconstruct Loss = 0.0551, Cls Loss = 0.2242, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.2302, Reg Loss = 5.1217, Reconstruct Loss = 0.0582, Cls Loss = 0.2243, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.2293, Reg Loss = 5.1580, Reconstruct Loss = 0.0575, Cls Loss = 0.2235, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.2295, Reg Loss = 5.1478, Reconstruct Loss = 0.0560, Cls Loss = 0.2238, Learning rate = 1.0000e-03\n",
      "Epoch [1/50], Training Loss: 0.2299, Training Accuracy: 93.17, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 43.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Validation Loss: 1.4880, Validation Accuracy: 60.90%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 0 with accuracy: 60.90%\n",
      "Iteration 0: Loss = 0.1565, Reg Loss = 9.5124, Reconstruct Loss = 0.1594, Cls Loss = 0.1405, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2335, Reg Loss = 7.1058, Reconstruct Loss = 0.0895, Cls Loss = 0.2245, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2257, Reg Loss = 7.5040, Reconstruct Loss = 0.0722, Cls Loss = 0.2184, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2254, Reg Loss = 7.8336, Reconstruct Loss = 0.0684, Cls Loss = 0.2185, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2214, Reg Loss = 7.5744, Reconstruct Loss = 0.0632, Cls Loss = 0.2150, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2212, Reg Loss = 7.1430, Reconstruct Loss = 0.0582, Cls Loss = 0.2153, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2203, Reg Loss = 6.9225, Reconstruct Loss = 0.0560, Cls Loss = 0.2146, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2203, Reg Loss = 6.7939, Reconstruct Loss = 0.0538, Cls Loss = 0.2149, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2190, Reg Loss = 6.6171, Reconstruct Loss = 0.0512, Cls Loss = 0.2138, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2191, Reg Loss = 6.5396, Reconstruct Loss = 0.0491, Cls Loss = 0.2141, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.2198, Reg Loss = 6.5611, Reconstruct Loss = 0.0485, Cls Loss = 0.2149, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.2202, Reg Loss = 6.4847, Reconstruct Loss = 0.0484, Cls Loss = 0.2153, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.2196, Reg Loss = 6.4050, Reconstruct Loss = 0.0474, Cls Loss = 0.2148, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.2191, Reg Loss = 6.3404, Reconstruct Loss = 0.0465, Cls Loss = 0.2144, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.2188, Reg Loss = 6.3892, Reconstruct Loss = 0.0467, Cls Loss = 0.2140, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.2188, Reg Loss = 6.3462, Reconstruct Loss = 0.0459, Cls Loss = 0.2142, Learning rate = 1.0000e-03\n",
      "Epoch [2/50], Training Loss: 0.2183, Training Accuracy: 93.41, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 43.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Validation Loss: 1.4482, Validation Accuracy: 61.41%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 1 with accuracy: 61.41%\n",
      "Iteration 0: Loss = 0.1785, Reg Loss = 6.9306, Reconstruct Loss = 0.0593, Cls Loss = 0.1725, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2091, Reg Loss = 6.9708, Reconstruct Loss = 0.0658, Cls Loss = 0.2024, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2055, Reg Loss = 6.8291, Reconstruct Loss = 0.0540, Cls Loss = 0.2000, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2066, Reg Loss = 6.5154, Reconstruct Loss = 0.0523, Cls Loss = 0.2013, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2054, Reg Loss = 6.4983, Reconstruct Loss = 0.0496, Cls Loss = 0.2004, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2057, Reg Loss = 6.3105, Reconstruct Loss = 0.0470, Cls Loss = 0.2009, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2025, Reg Loss = 6.2882, Reconstruct Loss = 0.0457, Cls Loss = 0.1978, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2010, Reg Loss = 6.1574, Reconstruct Loss = 0.0442, Cls Loss = 0.1965, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2003, Reg Loss = 6.1595, Reconstruct Loss = 0.0433, Cls Loss = 0.1959, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2012, Reg Loss = 6.0972, Reconstruct Loss = 0.0429, Cls Loss = 0.1968, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.1990, Reg Loss = 6.0743, Reconstruct Loss = 0.0428, Cls Loss = 0.1946, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.1972, Reg Loss = 6.0539, Reconstruct Loss = 0.0428, Cls Loss = 0.1929, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.1970, Reg Loss = 6.0394, Reconstruct Loss = 0.0428, Cls Loss = 0.1927, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.1967, Reg Loss = 6.1413, Reconstruct Loss = 0.0427, Cls Loss = 0.1923, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.1960, Reg Loss = 6.1769, Reconstruct Loss = 0.0426, Cls Loss = 0.1917, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.1952, Reg Loss = 6.1397, Reconstruct Loss = 0.0421, Cls Loss = 0.1909, Learning rate = 1.0000e-03\n",
      "Epoch [3/50], Training Loss: 0.1949, Training Accuracy: 94.00, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 36.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Validation Loss: 1.4584, Validation Accuracy: 61.34%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1734, Reg Loss = 4.9650, Reconstruct Loss = 0.0353, Cls Loss = 0.1698, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.1867, Reg Loss = 5.0048, Reconstruct Loss = 0.0401, Cls Loss = 0.1826, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.1875, Reg Loss = 5.9744, Reconstruct Loss = 0.0382, Cls Loss = 0.1837, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.1874, Reg Loss = 6.3277, Reconstruct Loss = 0.0369, Cls Loss = 0.1837, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.1886, Reg Loss = 6.4871, Reconstruct Loss = 0.0386, Cls Loss = 0.1847, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.1870, Reg Loss = 6.4372, Reconstruct Loss = 0.0381, Cls Loss = 0.1831, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.1879, Reg Loss = 6.3617, Reconstruct Loss = 0.0396, Cls Loss = 0.1838, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.1884, Reg Loss = 6.2992, Reconstruct Loss = 0.0392, Cls Loss = 0.1844, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.1885, Reg Loss = 6.3206, Reconstruct Loss = 0.0388, Cls Loss = 0.1845, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.1881, Reg Loss = 6.4083, Reconstruct Loss = 0.0396, Cls Loss = 0.1841, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.1870, Reg Loss = 6.4611, Reconstruct Loss = 0.0398, Cls Loss = 0.1830, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.1852, Reg Loss = 6.4591, Reconstruct Loss = 0.0395, Cls Loss = 0.1812, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.1848, Reg Loss = 6.4135, Reconstruct Loss = 0.0398, Cls Loss = 0.1808, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.1828, Reg Loss = 6.3663, Reconstruct Loss = 0.0396, Cls Loss = 0.1787, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.1820, Reg Loss = 6.3192, Reconstruct Loss = 0.0395, Cls Loss = 0.1780, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.1821, Reg Loss = 6.3009, Reconstruct Loss = 0.0393, Cls Loss = 0.1781, Learning rate = 1.0000e-03\n",
      "Epoch [4/50], Training Loss: 0.1823, Training Accuracy: 94.40, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 35.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Validation Loss: 1.5025, Validation Accuracy: 60.66%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.2825, Reg Loss = 6.4510, Reconstruct Loss = 0.0236, Cls Loss = 0.2800, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.1641, Reg Loss = 6.5973, Reconstruct Loss = 0.0254, Cls Loss = 0.1615, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.1680, Reg Loss = 7.3025, Reconstruct Loss = 0.0324, Cls Loss = 0.1647, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.1688, Reg Loss = 7.7567, Reconstruct Loss = 0.0379, Cls Loss = 0.1649, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.1714, Reg Loss = 7.7500, Reconstruct Loss = 0.0383, Cls Loss = 0.1675, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.1708, Reg Loss = 7.7048, Reconstruct Loss = 0.0382, Cls Loss = 0.1669, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.1703, Reg Loss = 7.6106, Reconstruct Loss = 0.0382, Cls Loss = 0.1664, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.1707, Reg Loss = 7.5283, Reconstruct Loss = 0.0380, Cls Loss = 0.1669, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.1697, Reg Loss = 7.4868, Reconstruct Loss = 0.0379, Cls Loss = 0.1658, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.1688, Reg Loss = 7.4433, Reconstruct Loss = 0.0372, Cls Loss = 0.1650, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.1683, Reg Loss = 7.4048, Reconstruct Loss = 0.0375, Cls Loss = 0.1644, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.1691, Reg Loss = 7.3748, Reconstruct Loss = 0.0374, Cls Loss = 0.1653, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.1692, Reg Loss = 7.3544, Reconstruct Loss = 0.0369, Cls Loss = 0.1655, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.1696, Reg Loss = 7.3525, Reconstruct Loss = 0.0367, Cls Loss = 0.1659, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.1689, Reg Loss = 7.3323, Reconstruct Loss = 0.0366, Cls Loss = 0.1651, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.1690, Reg Loss = 7.3166, Reconstruct Loss = 0.0362, Cls Loss = 0.1653, Learning rate = 1.0000e-03\n",
      "Epoch [5/50], Training Loss: 0.1691, Training Accuracy: 94.81, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 35.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Validation Loss: 1.4285, Validation Accuracy: 62.01%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 4 with accuracy: 62.01%\n",
      "Iteration 0: Loss = 0.2319, Reg Loss = 7.4496, Reconstruct Loss = 0.0228, Cls Loss = 0.2296, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.1647, Reg Loss = 7.9139, Reconstruct Loss = 0.0456, Cls Loss = 0.1601, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.1710, Reg Loss = 7.7706, Reconstruct Loss = 0.0438, Cls Loss = 0.1665, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.1719, Reg Loss = 7.5936, Reconstruct Loss = 0.0438, Cls Loss = 0.1674, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.1718, Reg Loss = 7.5298, Reconstruct Loss = 0.0420, Cls Loss = 0.1675, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.1700, Reg Loss = 7.4261, Reconstruct Loss = 0.0407, Cls Loss = 0.1658, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.1690, Reg Loss = 7.3555, Reconstruct Loss = 0.0394, Cls Loss = 0.1650, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.1692, Reg Loss = 7.3440, Reconstruct Loss = 0.0392, Cls Loss = 0.1652, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.1685, Reg Loss = 7.3223, Reconstruct Loss = 0.0387, Cls Loss = 0.1646, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.1684, Reg Loss = 7.3312, Reconstruct Loss = 0.0382, Cls Loss = 0.1645, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.1681, Reg Loss = 7.3593, Reconstruct Loss = 0.0374, Cls Loss = 0.1643, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.1676, Reg Loss = 7.3278, Reconstruct Loss = 0.0368, Cls Loss = 0.1638, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.1673, Reg Loss = 7.2979, Reconstruct Loss = 0.0364, Cls Loss = 0.1636, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.1661, Reg Loss = 7.2650, Reconstruct Loss = 0.0363, Cls Loss = 0.1624, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.1654, Reg Loss = 7.2252, Reconstruct Loss = 0.0362, Cls Loss = 0.1617, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.1653, Reg Loss = 7.2279, Reconstruct Loss = 0.0364, Cls Loss = 0.1615, Learning rate = 1.0000e-03\n",
      "Epoch [6/50], Training Loss: 0.1650, Training Accuracy: 95.16, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 36.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Validation Loss: 1.4395, Validation Accuracy: 66.20%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 5 with accuracy: 66.20%\n",
      "Iteration 0: Loss = 0.2190, Reg Loss = 7.0245, Reconstruct Loss = 0.0333, Cls Loss = 0.2156, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.1679, Reg Loss = 7.6529, Reconstruct Loss = 0.0320, Cls Loss = 0.1646, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.1628, Reg Loss = 7.5069, Reconstruct Loss = 0.0324, Cls Loss = 0.1595, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.1628, Reg Loss = 7.3067, Reconstruct Loss = 0.0335, Cls Loss = 0.1594, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.1605, Reg Loss = 7.2519, Reconstruct Loss = 0.0338, Cls Loss = 0.1570, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.1608, Reg Loss = 7.2631, Reconstruct Loss = 0.0333, Cls Loss = 0.1573, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.1587, Reg Loss = 7.2863, Reconstruct Loss = 0.0336, Cls Loss = 0.1553, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.1581, Reg Loss = 7.3171, Reconstruct Loss = 0.0329, Cls Loss = 0.1547, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.1597, Reg Loss = 7.3567, Reconstruct Loss = 0.0320, Cls Loss = 0.1564, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.1594, Reg Loss = 7.3032, Reconstruct Loss = 0.0317, Cls Loss = 0.1561, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.1606, Reg Loss = 7.2762, Reconstruct Loss = 0.0320, Cls Loss = 0.1573, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.1614, Reg Loss = 7.2218, Reconstruct Loss = 0.0325, Cls Loss = 0.1581, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.1615, Reg Loss = 7.3156, Reconstruct Loss = 0.0337, Cls Loss = 0.1581, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.1623, Reg Loss = 7.3018, Reconstruct Loss = 0.0342, Cls Loss = 0.1588, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.1610, Reg Loss = 7.2922, Reconstruct Loss = 0.0344, Cls Loss = 0.1575, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.1613, Reg Loss = 7.2752, Reconstruct Loss = 0.0344, Cls Loss = 0.1578, Learning rate = 1.0000e-03\n",
      "Epoch [7/50], Training Loss: 0.1619, Training Accuracy: 95.16, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 36.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Validation Loss: 1.4902, Validation Accuracy: 68.50%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 6 with accuracy: 68.50%\n",
      "Iteration 0: Loss = 0.2469, Reg Loss = 6.0895, Reconstruct Loss = 0.0509, Cls Loss = 0.2418, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.1644, Reg Loss = 6.5459, Reconstruct Loss = 0.0372, Cls Loss = 0.1607, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.1577, Reg Loss = 7.0021, Reconstruct Loss = 0.0381, Cls Loss = 0.1538, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.1547, Reg Loss = 7.0699, Reconstruct Loss = 0.0368, Cls Loss = 0.1510, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.1534, Reg Loss = 7.1596, Reconstruct Loss = 0.0363, Cls Loss = 0.1497, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.1550, Reg Loss = 7.2139, Reconstruct Loss = 0.0358, Cls Loss = 0.1514, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.1561, Reg Loss = 7.1443, Reconstruct Loss = 0.0359, Cls Loss = 0.1525, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.1552, Reg Loss = 7.1674, Reconstruct Loss = 0.0356, Cls Loss = 0.1516, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.1553, Reg Loss = 7.3152, Reconstruct Loss = 0.0355, Cls Loss = 0.1516, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.1544, Reg Loss = 7.4030, Reconstruct Loss = 0.0352, Cls Loss = 0.1508, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.1524, Reg Loss = 7.4100, Reconstruct Loss = 0.0350, Cls Loss = 0.1489, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.1526, Reg Loss = 7.5076, Reconstruct Loss = 0.0348, Cls Loss = 0.1490, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.1516, Reg Loss = 7.4998, Reconstruct Loss = 0.0346, Cls Loss = 0.1481, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.1514, Reg Loss = 7.5309, Reconstruct Loss = 0.0350, Cls Loss = 0.1478, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.1511, Reg Loss = 7.6416, Reconstruct Loss = 0.0350, Cls Loss = 0.1476, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.1508, Reg Loss = 7.6482, Reconstruct Loss = 0.0349, Cls Loss = 0.1472, Learning rate = 1.0000e-03\n",
      "Epoch [8/50], Training Loss: 0.1507, Training Accuracy: 95.35, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 43.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Validation Loss: 2.3154, Validation Accuracy: 55.65%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1141, Reg Loss = 8.1294, Reconstruct Loss = 0.0318, Cls Loss = 0.1108, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.1437, Reg Loss = 8.1876, Reconstruct Loss = 0.0359, Cls Loss = 0.1400, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.1350, Reg Loss = 8.6424, Reconstruct Loss = 0.0329, Cls Loss = 0.1317, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.1316, Reg Loss = 9.1379, Reconstruct Loss = 0.0328, Cls Loss = 0.1282, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.1302, Reg Loss = 9.2481, Reconstruct Loss = 0.0334, Cls Loss = 0.1268, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.1309, Reg Loss = 9.1750, Reconstruct Loss = 0.0337, Cls Loss = 0.1275, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.1305, Reg Loss = 9.2185, Reconstruct Loss = 0.0339, Cls Loss = 0.1270, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.1306, Reg Loss = 9.1390, Reconstruct Loss = 0.0341, Cls Loss = 0.1271, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.1299, Reg Loss = 9.1364, Reconstruct Loss = 0.0344, Cls Loss = 0.1264, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.1302, Reg Loss = 9.0871, Reconstruct Loss = 0.0346, Cls Loss = 0.1267, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.1294, Reg Loss = 9.0715, Reconstruct Loss = 0.0350, Cls Loss = 0.1258, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.1284, Reg Loss = 9.1761, Reconstruct Loss = 0.0350, Cls Loss = 0.1248, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.1289, Reg Loss = 9.3314, Reconstruct Loss = 0.0352, Cls Loss = 0.1253, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.1286, Reg Loss = 9.2842, Reconstruct Loss = 0.0356, Cls Loss = 0.1249, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.1281, Reg Loss = 9.3180, Reconstruct Loss = 0.0359, Cls Loss = 0.1244, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.1277, Reg Loss = 9.2913, Reconstruct Loss = 0.0363, Cls Loss = 0.1240, Learning rate = 1.0000e-03\n",
      "Epoch [9/50], Training Loss: 0.1275, Training Accuracy: 96.12, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Validation Loss: 7.9931, Validation Accuracy: 22.33%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0903, Reg Loss = 9.1710, Reconstruct Loss = 0.0445, Cls Loss = 0.0858, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.1246, Reg Loss = 9.8249, Reconstruct Loss = 0.0355, Cls Loss = 0.1210, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.1202, Reg Loss = 9.8165, Reconstruct Loss = 0.0386, Cls Loss = 0.1163, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.1207, Reg Loss = 9.8800, Reconstruct Loss = 0.0407, Cls Loss = 0.1165, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.1224, Reg Loss = 10.0822, Reconstruct Loss = 0.0395, Cls Loss = 0.1183, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.1209, Reg Loss = 10.3821, Reconstruct Loss = 0.0391, Cls Loss = 0.1169, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.1197, Reg Loss = 10.4945, Reconstruct Loss = 0.0386, Cls Loss = 0.1157, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.1204, Reg Loss = 10.5370, Reconstruct Loss = 0.0379, Cls Loss = 0.1165, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.1207, Reg Loss = 10.5853, Reconstruct Loss = 0.0374, Cls Loss = 0.1168, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.1200, Reg Loss = 10.5088, Reconstruct Loss = 0.0370, Cls Loss = 0.1162, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.1202, Reg Loss = 10.5120, Reconstruct Loss = 0.0368, Cls Loss = 0.1165, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.1200, Reg Loss = 10.6186, Reconstruct Loss = 0.0366, Cls Loss = 0.1162, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.1198, Reg Loss = 10.6842, Reconstruct Loss = 0.0364, Cls Loss = 0.1161, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.1188, Reg Loss = 10.6417, Reconstruct Loss = 0.0370, Cls Loss = 0.1150, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.1190, Reg Loss = 10.7356, Reconstruct Loss = 0.0370, Cls Loss = 0.1152, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.1187, Reg Loss = 10.8400, Reconstruct Loss = 0.0371, Cls Loss = 0.1149, Learning rate = 1.0000e-03\n",
      "Epoch [10/50], Training Loss: 0.1182, Training Accuracy: 96.47, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Validation Loss: 9.5388, Validation Accuracy: 21.35%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1109, Reg Loss = 10.0591, Reconstruct Loss = 0.0477, Cls Loss = 0.1060, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.1104, Reg Loss = 11.0659, Reconstruct Loss = 0.0375, Cls Loss = 0.1066, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.1109, Reg Loss = 11.6304, Reconstruct Loss = 0.0375, Cls Loss = 0.1071, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.1126, Reg Loss = 12.3156, Reconstruct Loss = 0.0366, Cls Loss = 0.1088, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.1117, Reg Loss = 12.2741, Reconstruct Loss = 0.0366, Cls Loss = 0.1079, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.1097, Reg Loss = 12.1598, Reconstruct Loss = 0.0365, Cls Loss = 0.1059, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.1104, Reg Loss = 12.1495, Reconstruct Loss = 0.0363, Cls Loss = 0.1067, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.1096, Reg Loss = 12.1328, Reconstruct Loss = 0.0362, Cls Loss = 0.1058, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.1093, Reg Loss = 12.0576, Reconstruct Loss = 0.0362, Cls Loss = 0.1056, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.1086, Reg Loss = 11.9061, Reconstruct Loss = 0.0368, Cls Loss = 0.1048, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.1082, Reg Loss = 11.7080, Reconstruct Loss = 0.0373, Cls Loss = 0.1044, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.1077, Reg Loss = 11.6556, Reconstruct Loss = 0.0375, Cls Loss = 0.1039, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.1070, Reg Loss = 11.7465, Reconstruct Loss = 0.0376, Cls Loss = 0.1031, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.1058, Reg Loss = 11.6536, Reconstruct Loss = 0.0377, Cls Loss = 0.1019, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.1052, Reg Loss = 11.6616, Reconstruct Loss = 0.0378, Cls Loss = 0.1013, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.1044, Reg Loss = 11.8281, Reconstruct Loss = 0.0377, Cls Loss = 0.1005, Learning rate = 1.0000e-03\n",
      "Epoch [11/50], Training Loss: 0.1039, Training Accuracy: 97.01, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Validation Loss: 8.5594, Validation Accuracy: 22.52%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0761, Reg Loss = 11.2656, Reconstruct Loss = 0.0398, Cls Loss = 0.0721, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0967, Reg Loss = 11.4373, Reconstruct Loss = 0.0384, Cls Loss = 0.0927, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0958, Reg Loss = 12.1807, Reconstruct Loss = 0.0385, Cls Loss = 0.0918, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0942, Reg Loss = 12.1014, Reconstruct Loss = 0.0384, Cls Loss = 0.0902, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0963, Reg Loss = 12.1642, Reconstruct Loss = 0.0380, Cls Loss = 0.0924, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0983, Reg Loss = 12.3138, Reconstruct Loss = 0.0381, Cls Loss = 0.0944, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0982, Reg Loss = 12.2096, Reconstruct Loss = 0.0377, Cls Loss = 0.0943, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0970, Reg Loss = 12.0931, Reconstruct Loss = 0.0372, Cls Loss = 0.0931, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0982, Reg Loss = 11.9987, Reconstruct Loss = 0.0371, Cls Loss = 0.0944, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0993, Reg Loss = 11.8322, Reconstruct Loss = 0.0370, Cls Loss = 0.0954, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.1008, Reg Loss = 11.8416, Reconstruct Loss = 0.0367, Cls Loss = 0.0970, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0998, Reg Loss = 11.7978, Reconstruct Loss = 0.0366, Cls Loss = 0.0961, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0991, Reg Loss = 11.8822, Reconstruct Loss = 0.0366, Cls Loss = 0.0954, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0992, Reg Loss = 11.8444, Reconstruct Loss = 0.0365, Cls Loss = 0.0954, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0986, Reg Loss = 11.7975, Reconstruct Loss = 0.0363, Cls Loss = 0.0949, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0982, Reg Loss = 11.7447, Reconstruct Loss = 0.0361, Cls Loss = 0.0945, Learning rate = 1.0000e-03\n",
      "Epoch [12/50], Training Loss: 0.0981, Training Accuracy: 97.12, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Validation Loss: 4.3655, Validation Accuracy: 39.92%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1216, Reg Loss = 13.3130, Reconstruct Loss = 0.0362, Cls Loss = 0.1178, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0897, Reg Loss = 12.8028, Reconstruct Loss = 0.0329, Cls Loss = 0.0863, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0948, Reg Loss = 11.6395, Reconstruct Loss = 0.0328, Cls Loss = 0.0914, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0945, Reg Loss = 11.3305, Reconstruct Loss = 0.0337, Cls Loss = 0.0911, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0941, Reg Loss = 11.3179, Reconstruct Loss = 0.0342, Cls Loss = 0.0906, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0956, Reg Loss = 11.4335, Reconstruct Loss = 0.0343, Cls Loss = 0.0920, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0951, Reg Loss = 11.4407, Reconstruct Loss = 0.0341, Cls Loss = 0.0916, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0953, Reg Loss = 11.4210, Reconstruct Loss = 0.0348, Cls Loss = 0.0918, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0951, Reg Loss = 11.5178, Reconstruct Loss = 0.0348, Cls Loss = 0.0915, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0951, Reg Loss = 11.5732, Reconstruct Loss = 0.0350, Cls Loss = 0.0915, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0944, Reg Loss = 11.5332, Reconstruct Loss = 0.0349, Cls Loss = 0.0908, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0950, Reg Loss = 11.4106, Reconstruct Loss = 0.0353, Cls Loss = 0.0914, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0949, Reg Loss = 11.3254, Reconstruct Loss = 0.0355, Cls Loss = 0.0913, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0939, Reg Loss = 11.2856, Reconstruct Loss = 0.0356, Cls Loss = 0.0902, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0939, Reg Loss = 11.1797, Reconstruct Loss = 0.0358, Cls Loss = 0.0902, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0934, Reg Loss = 11.1242, Reconstruct Loss = 0.0357, Cls Loss = 0.0897, Learning rate = 1.0000e-03\n",
      "Epoch [13/50], Training Loss: 0.0936, Training Accuracy: 97.21, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Validation Loss: 2.8987, Validation Accuracy: 58.67%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1091, Reg Loss = 10.7780, Reconstruct Loss = 0.0337, Cls Loss = 0.1056, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0886, Reg Loss = 11.2049, Reconstruct Loss = 0.0376, Cls Loss = 0.0847, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0925, Reg Loss = 10.6962, Reconstruct Loss = 0.0377, Cls Loss = 0.0886, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0911, Reg Loss = 10.6241, Reconstruct Loss = 0.0371, Cls Loss = 0.0873, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0905, Reg Loss = 10.5425, Reconstruct Loss = 0.0372, Cls Loss = 0.0867, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0906, Reg Loss = 10.4565, Reconstruct Loss = 0.0370, Cls Loss = 0.0868, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0910, Reg Loss = 10.4193, Reconstruct Loss = 0.0370, Cls Loss = 0.0872, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0894, Reg Loss = 10.4817, Reconstruct Loss = 0.0369, Cls Loss = 0.0856, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0907, Reg Loss = 10.3658, Reconstruct Loss = 0.0368, Cls Loss = 0.0869, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0903, Reg Loss = 10.2609, Reconstruct Loss = 0.0367, Cls Loss = 0.0866, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0906, Reg Loss = 10.1544, Reconstruct Loss = 0.0367, Cls Loss = 0.0869, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0907, Reg Loss = 10.1740, Reconstruct Loss = 0.0365, Cls Loss = 0.0870, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0908, Reg Loss = 10.0974, Reconstruct Loss = 0.0363, Cls Loss = 0.0870, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0911, Reg Loss = 10.0092, Reconstruct Loss = 0.0365, Cls Loss = 0.0874, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0908, Reg Loss = 9.9509, Reconstruct Loss = 0.0365, Cls Loss = 0.0871, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0906, Reg Loss = 9.9036, Reconstruct Loss = 0.0365, Cls Loss = 0.0869, Learning rate = 1.0000e-03\n",
      "Epoch [14/50], Training Loss: 0.0915, Training Accuracy: 97.39, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Validation Loss: 3.8130, Validation Accuracy: 52.63%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0459, Reg Loss = 10.8438, Reconstruct Loss = 0.0317, Cls Loss = 0.0426, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0906, Reg Loss = 9.9746, Reconstruct Loss = 0.0321, Cls Loss = 0.0873, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0938, Reg Loss = 9.8265, Reconstruct Loss = 0.0336, Cls Loss = 0.0903, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0897, Reg Loss = 9.8930, Reconstruct Loss = 0.0339, Cls Loss = 0.0862, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0884, Reg Loss = 10.0226, Reconstruct Loss = 0.0343, Cls Loss = 0.0848, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0879, Reg Loss = 10.1111, Reconstruct Loss = 0.0346, Cls Loss = 0.0843, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0875, Reg Loss = 10.1662, Reconstruct Loss = 0.0345, Cls Loss = 0.0840, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0880, Reg Loss = 10.1917, Reconstruct Loss = 0.0350, Cls Loss = 0.0844, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0897, Reg Loss = 10.1720, Reconstruct Loss = 0.0352, Cls Loss = 0.0861, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0885, Reg Loss = 10.2161, Reconstruct Loss = 0.0350, Cls Loss = 0.0849, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0882, Reg Loss = 10.2139, Reconstruct Loss = 0.0349, Cls Loss = 0.0846, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0882, Reg Loss = 10.2000, Reconstruct Loss = 0.0347, Cls Loss = 0.0846, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0884, Reg Loss = 10.1531, Reconstruct Loss = 0.0344, Cls Loss = 0.0849, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0882, Reg Loss = 10.1103, Reconstruct Loss = 0.0341, Cls Loss = 0.0847, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0890, Reg Loss = 10.0897, Reconstruct Loss = 0.0339, Cls Loss = 0.0855, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0888, Reg Loss = 10.0861, Reconstruct Loss = 0.0339, Cls Loss = 0.0853, Learning rate = 1.0000e-03\n",
      "Epoch [15/50], Training Loss: 0.0889, Training Accuracy: 97.35, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Validation Loss: 4.0393, Validation Accuracy: 50.49%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0963, Reg Loss = 9.6251, Reconstruct Loss = 0.0457, Cls Loss = 0.0916, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0859, Reg Loss = 9.6408, Reconstruct Loss = 0.0355, Cls Loss = 0.0823, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0914, Reg Loss = 9.8560, Reconstruct Loss = 0.0329, Cls Loss = 0.0880, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0908, Reg Loss = 9.9580, Reconstruct Loss = 0.0324, Cls Loss = 0.0875, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0899, Reg Loss = 9.9340, Reconstruct Loss = 0.0330, Cls Loss = 0.0865, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0887, Reg Loss = 10.0142, Reconstruct Loss = 0.0330, Cls Loss = 0.0853, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0888, Reg Loss = 10.0569, Reconstruct Loss = 0.0322, Cls Loss = 0.0854, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0877, Reg Loss = 10.0015, Reconstruct Loss = 0.0317, Cls Loss = 0.0844, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0869, Reg Loss = 9.9146, Reconstruct Loss = 0.0317, Cls Loss = 0.0837, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0876, Reg Loss = 9.8561, Reconstruct Loss = 0.0321, Cls Loss = 0.0843, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0868, Reg Loss = 9.8821, Reconstruct Loss = 0.0320, Cls Loss = 0.0835, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0873, Reg Loss = 9.8747, Reconstruct Loss = 0.0319, Cls Loss = 0.0840, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0868, Reg Loss = 9.8454, Reconstruct Loss = 0.0316, Cls Loss = 0.0835, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0867, Reg Loss = 9.8805, Reconstruct Loss = 0.0315, Cls Loss = 0.0835, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0864, Reg Loss = 9.8538, Reconstruct Loss = 0.0316, Cls Loss = 0.0831, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0859, Reg Loss = 9.8534, Reconstruct Loss = 0.0316, Cls Loss = 0.0827, Learning rate = 1.0000e-03\n",
      "Epoch [16/50], Training Loss: 0.0859, Training Accuracy: 97.48, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Validation Loss: 2.8281, Validation Accuracy: 60.21%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1027, Reg Loss = 9.1541, Reconstruct Loss = 0.0410, Cls Loss = 0.0985, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0840, Reg Loss = 8.7340, Reconstruct Loss = 0.0313, Cls Loss = 0.0807, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0833, Reg Loss = 9.1506, Reconstruct Loss = 0.0302, Cls Loss = 0.0802, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0845, Reg Loss = 9.0991, Reconstruct Loss = 0.0306, Cls Loss = 0.0813, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0827, Reg Loss = 9.0983, Reconstruct Loss = 0.0312, Cls Loss = 0.0795, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0855, Reg Loss = 8.9953, Reconstruct Loss = 0.0314, Cls Loss = 0.0823, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0863, Reg Loss = 9.0816, Reconstruct Loss = 0.0313, Cls Loss = 0.0831, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0867, Reg Loss = 9.1543, Reconstruct Loss = 0.0308, Cls Loss = 0.0835, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0867, Reg Loss = 9.2649, Reconstruct Loss = 0.0305, Cls Loss = 0.0836, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0861, Reg Loss = 9.3180, Reconstruct Loss = 0.0303, Cls Loss = 0.0830, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0860, Reg Loss = 9.2893, Reconstruct Loss = 0.0302, Cls Loss = 0.0828, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0855, Reg Loss = 9.3285, Reconstruct Loss = 0.0303, Cls Loss = 0.0824, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0858, Reg Loss = 9.3368, Reconstruct Loss = 0.0303, Cls Loss = 0.0827, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0859, Reg Loss = 9.3639, Reconstruct Loss = 0.0305, Cls Loss = 0.0828, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0862, Reg Loss = 9.4164, Reconstruct Loss = 0.0305, Cls Loss = 0.0830, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0860, Reg Loss = 9.4688, Reconstruct Loss = 0.0304, Cls Loss = 0.0828, Learning rate = 1.0000e-03\n",
      "Epoch [17/50], Training Loss: 0.0857, Training Accuracy: 97.51, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Validation Loss: 5.3849, Validation Accuracy: 43.46%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1004, Reg Loss = 10.3459, Reconstruct Loss = 0.0248, Cls Loss = 0.0978, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0937, Reg Loss = 10.6937, Reconstruct Loss = 0.0300, Cls Loss = 0.0906, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0895, Reg Loss = 10.8857, Reconstruct Loss = 0.0301, Cls Loss = 0.0863, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0862, Reg Loss = 10.5293, Reconstruct Loss = 0.0298, Cls Loss = 0.0831, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0849, Reg Loss = 10.1538, Reconstruct Loss = 0.0299, Cls Loss = 0.0818, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0851, Reg Loss = 10.0624, Reconstruct Loss = 0.0299, Cls Loss = 0.0820, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0861, Reg Loss = 9.9945, Reconstruct Loss = 0.0297, Cls Loss = 0.0831, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0867, Reg Loss = 9.9744, Reconstruct Loss = 0.0303, Cls Loss = 0.0836, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0873, Reg Loss = 9.9196, Reconstruct Loss = 0.0307, Cls Loss = 0.0841, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0878, Reg Loss = 9.9107, Reconstruct Loss = 0.0307, Cls Loss = 0.0846, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0880, Reg Loss = 9.8628, Reconstruct Loss = 0.0307, Cls Loss = 0.0849, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0885, Reg Loss = 9.8127, Reconstruct Loss = 0.0311, Cls Loss = 0.0852, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0882, Reg Loss = 9.7622, Reconstruct Loss = 0.0313, Cls Loss = 0.0850, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0879, Reg Loss = 9.7250, Reconstruct Loss = 0.0311, Cls Loss = 0.0847, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0878, Reg Loss = 9.6858, Reconstruct Loss = 0.0310, Cls Loss = 0.0846, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0873, Reg Loss = 9.6388, Reconstruct Loss = 0.0310, Cls Loss = 0.0842, Learning rate = 1.0000e-03\n",
      "Epoch [18/50], Training Loss: 0.0876, Training Accuracy: 97.37, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Validation Loss: 5.3948, Validation Accuracy: 44.81%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1000, Reg Loss = 9.0071, Reconstruct Loss = 0.0378, Cls Loss = 0.0961, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0862, Reg Loss = 9.1554, Reconstruct Loss = 0.0311, Cls Loss = 0.0830, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0817, Reg Loss = 9.2371, Reconstruct Loss = 0.0302, Cls Loss = 0.0786, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0838, Reg Loss = 9.4051, Reconstruct Loss = 0.0294, Cls Loss = 0.0808, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0844, Reg Loss = 9.3606, Reconstruct Loss = 0.0290, Cls Loss = 0.0814, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0850, Reg Loss = 9.4340, Reconstruct Loss = 0.0293, Cls Loss = 0.0819, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0850, Reg Loss = 9.5038, Reconstruct Loss = 0.0293, Cls Loss = 0.0820, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0844, Reg Loss = 9.5835, Reconstruct Loss = 0.0291, Cls Loss = 0.0814, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0845, Reg Loss = 9.5665, Reconstruct Loss = 0.0291, Cls Loss = 0.0815, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0834, Reg Loss = 9.5058, Reconstruct Loss = 0.0290, Cls Loss = 0.0805, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0836, Reg Loss = 9.5758, Reconstruct Loss = 0.0289, Cls Loss = 0.0806, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0840, Reg Loss = 9.6323, Reconstruct Loss = 0.0288, Cls Loss = 0.0810, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0837, Reg Loss = 9.6259, Reconstruct Loss = 0.0288, Cls Loss = 0.0807, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0842, Reg Loss = 9.6416, Reconstruct Loss = 0.0289, Cls Loss = 0.0812, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0843, Reg Loss = 9.6536, Reconstruct Loss = 0.0288, Cls Loss = 0.0813, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0846, Reg Loss = 9.7385, Reconstruct Loss = 0.0287, Cls Loss = 0.0816, Learning rate = 1.0000e-03\n",
      "Epoch [19/50], Training Loss: 0.0846, Training Accuracy: 97.48, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Validation Loss: 5.9511, Validation Accuracy: 42.90%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0660, Reg Loss = 10.6118, Reconstruct Loss = 0.0215, Cls Loss = 0.0637, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0870, Reg Loss = 11.1892, Reconstruct Loss = 0.0282, Cls Loss = 0.0841, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0886, Reg Loss = 10.7937, Reconstruct Loss = 0.0297, Cls Loss = 0.0855, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0920, Reg Loss = 10.4922, Reconstruct Loss = 0.0301, Cls Loss = 0.0889, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0909, Reg Loss = 10.1723, Reconstruct Loss = 0.0301, Cls Loss = 0.0878, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0898, Reg Loss = 10.1425, Reconstruct Loss = 0.0295, Cls Loss = 0.0867, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0894, Reg Loss = 10.0047, Reconstruct Loss = 0.0298, Cls Loss = 0.0863, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0889, Reg Loss = 9.9198, Reconstruct Loss = 0.0295, Cls Loss = 0.0858, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0870, Reg Loss = 9.9130, Reconstruct Loss = 0.0292, Cls Loss = 0.0840, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0876, Reg Loss = 9.9511, Reconstruct Loss = 0.0291, Cls Loss = 0.0846, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0879, Reg Loss = 9.9083, Reconstruct Loss = 0.0292, Cls Loss = 0.0848, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0878, Reg Loss = 9.9352, Reconstruct Loss = 0.0291, Cls Loss = 0.0848, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0872, Reg Loss = 9.9749, Reconstruct Loss = 0.0290, Cls Loss = 0.0842, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0873, Reg Loss = 9.9927, Reconstruct Loss = 0.0287, Cls Loss = 0.0843, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0871, Reg Loss = 9.9838, Reconstruct Loss = 0.0287, Cls Loss = 0.0842, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0876, Reg Loss = 9.9795, Reconstruct Loss = 0.0285, Cls Loss = 0.0846, Learning rate = 1.0000e-03\n",
      "Epoch [20/50], Training Loss: 0.0872, Training Accuracy: 97.40, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Validation Loss: 7.0488, Validation Accuracy: 42.25%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0400, Reg Loss = 9.2674, Reconstruct Loss = 0.0339, Cls Loss = 0.0365, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0802, Reg Loss = 9.6478, Reconstruct Loss = 0.0278, Cls Loss = 0.0773, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0823, Reg Loss = 10.1281, Reconstruct Loss = 0.0286, Cls Loss = 0.0793, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0854, Reg Loss = 10.1767, Reconstruct Loss = 0.0300, Cls Loss = 0.0823, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0864, Reg Loss = 9.9936, Reconstruct Loss = 0.0312, Cls Loss = 0.0832, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0873, Reg Loss = 9.8845, Reconstruct Loss = 0.0329, Cls Loss = 0.0839, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0849, Reg Loss = 9.9565, Reconstruct Loss = 0.0326, Cls Loss = 0.0815, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0830, Reg Loss = 9.9208, Reconstruct Loss = 0.0322, Cls Loss = 0.0797, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0837, Reg Loss = 9.8961, Reconstruct Loss = 0.0319, Cls Loss = 0.0804, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0844, Reg Loss = 9.9059, Reconstruct Loss = 0.0316, Cls Loss = 0.0812, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0838, Reg Loss = 9.9189, Reconstruct Loss = 0.0314, Cls Loss = 0.0806, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0828, Reg Loss = 9.8991, Reconstruct Loss = 0.0310, Cls Loss = 0.0796, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0831, Reg Loss = 9.9291, Reconstruct Loss = 0.0308, Cls Loss = 0.0799, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0833, Reg Loss = 9.9491, Reconstruct Loss = 0.0306, Cls Loss = 0.0801, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0837, Reg Loss = 9.9501, Reconstruct Loss = 0.0304, Cls Loss = 0.0806, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0838, Reg Loss = 9.9478, Reconstruct Loss = 0.0303, Cls Loss = 0.0807, Learning rate = 1.0000e-03\n",
      "Epoch [21/50], Training Loss: 0.0840, Training Accuracy: 97.49, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Validation Loss: 4.7418, Validation Accuracy: 48.76%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0759, Reg Loss = 10.9435, Reconstruct Loss = 0.0259, Cls Loss = 0.0732, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0807, Reg Loss = 9.6737, Reconstruct Loss = 0.0286, Cls Loss = 0.0777, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0759, Reg Loss = 9.8323, Reconstruct Loss = 0.0271, Cls Loss = 0.0731, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0765, Reg Loss = 9.6278, Reconstruct Loss = 0.0283, Cls Loss = 0.0736, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0793, Reg Loss = 9.6891, Reconstruct Loss = 0.0283, Cls Loss = 0.0764, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0795, Reg Loss = 9.8983, Reconstruct Loss = 0.0285, Cls Loss = 0.0766, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0788, Reg Loss = 10.0045, Reconstruct Loss = 0.0287, Cls Loss = 0.0758, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0806, Reg Loss = 9.9443, Reconstruct Loss = 0.0289, Cls Loss = 0.0776, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0816, Reg Loss = 9.9569, Reconstruct Loss = 0.0287, Cls Loss = 0.0786, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0820, Reg Loss = 9.9067, Reconstruct Loss = 0.0289, Cls Loss = 0.0790, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0833, Reg Loss = 9.8151, Reconstruct Loss = 0.0296, Cls Loss = 0.0803, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0836, Reg Loss = 9.8005, Reconstruct Loss = 0.0300, Cls Loss = 0.0805, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0827, Reg Loss = 9.7530, Reconstruct Loss = 0.0304, Cls Loss = 0.0796, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0829, Reg Loss = 9.7099, Reconstruct Loss = 0.0310, Cls Loss = 0.0797, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0829, Reg Loss = 9.7398, Reconstruct Loss = 0.0309, Cls Loss = 0.0797, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0825, Reg Loss = 9.7734, Reconstruct Loss = 0.0310, Cls Loss = 0.0793, Learning rate = 1.0000e-03\n",
      "Epoch [22/50], Training Loss: 0.0829, Training Accuracy: 97.51, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Validation Loss: 6.0743, Validation Accuracy: 42.64%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1271, Reg Loss = 10.1052, Reconstruct Loss = 0.0335, Cls Loss = 0.1237, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0833, Reg Loss = 10.0462, Reconstruct Loss = 0.0313, Cls Loss = 0.0801, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0802, Reg Loss = 10.0843, Reconstruct Loss = 0.0310, Cls Loss = 0.0770, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0817, Reg Loss = 10.0049, Reconstruct Loss = 0.0300, Cls Loss = 0.0786, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0839, Reg Loss = 9.8252, Reconstruct Loss = 0.0300, Cls Loss = 0.0808, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0833, Reg Loss = 9.6370, Reconstruct Loss = 0.0300, Cls Loss = 0.0802, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0832, Reg Loss = 9.5799, Reconstruct Loss = 0.0299, Cls Loss = 0.0801, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0833, Reg Loss = 9.5642, Reconstruct Loss = 0.0299, Cls Loss = 0.0802, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0811, Reg Loss = 9.4858, Reconstruct Loss = 0.0295, Cls Loss = 0.0781, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0817, Reg Loss = 9.3706, Reconstruct Loss = 0.0296, Cls Loss = 0.0786, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0808, Reg Loss = 9.3982, Reconstruct Loss = 0.0291, Cls Loss = 0.0778, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0813, Reg Loss = 9.3880, Reconstruct Loss = 0.0288, Cls Loss = 0.0783, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0817, Reg Loss = 9.3208, Reconstruct Loss = 0.0285, Cls Loss = 0.0788, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0815, Reg Loss = 9.3417, Reconstruct Loss = 0.0282, Cls Loss = 0.0786, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0817, Reg Loss = 9.3726, Reconstruct Loss = 0.0281, Cls Loss = 0.0788, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0821, Reg Loss = 9.3586, Reconstruct Loss = 0.0281, Cls Loss = 0.0792, Learning rate = 1.0000e-03\n",
      "Epoch [23/50], Training Loss: 0.0825, Training Accuracy: 97.51, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50], Validation Loss: 5.3468, Validation Accuracy: 46.19%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0409, Reg Loss = 10.6095, Reconstruct Loss = 0.0240, Cls Loss = 0.0384, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0802, Reg Loss = 10.5454, Reconstruct Loss = 0.0260, Cls Loss = 0.0775, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0827, Reg Loss = 10.0808, Reconstruct Loss = 0.0271, Cls Loss = 0.0799, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0836, Reg Loss = 10.0341, Reconstruct Loss = 0.0265, Cls Loss = 0.0809, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0816, Reg Loss = 10.1012, Reconstruct Loss = 0.0259, Cls Loss = 0.0789, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0830, Reg Loss = 10.2020, Reconstruct Loss = 0.0255, Cls Loss = 0.0803, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0825, Reg Loss = 10.1446, Reconstruct Loss = 0.0253, Cls Loss = 0.0799, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0838, Reg Loss = 9.9618, Reconstruct Loss = 0.0251, Cls Loss = 0.0812, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0825, Reg Loss = 9.9483, Reconstruct Loss = 0.0249, Cls Loss = 0.0799, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0825, Reg Loss = 9.8955, Reconstruct Loss = 0.0248, Cls Loss = 0.0799, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0820, Reg Loss = 9.8948, Reconstruct Loss = 0.0246, Cls Loss = 0.0794, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0818, Reg Loss = 9.8781, Reconstruct Loss = 0.0248, Cls Loss = 0.0792, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0819, Reg Loss = 9.7919, Reconstruct Loss = 0.0253, Cls Loss = 0.0793, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0820, Reg Loss = 9.7131, Reconstruct Loss = 0.0256, Cls Loss = 0.0793, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0817, Reg Loss = 9.6576, Reconstruct Loss = 0.0257, Cls Loss = 0.0791, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0819, Reg Loss = 9.6118, Reconstruct Loss = 0.0259, Cls Loss = 0.0792, Learning rate = 1.0000e-03\n",
      "Epoch [24/50], Training Loss: 0.0821, Training Accuracy: 97.52, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50], Validation Loss: 7.2780, Validation Accuracy: 42.47%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0753, Reg Loss = 8.8682, Reconstruct Loss = 0.0249, Cls Loss = 0.0728, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0820, Reg Loss = 8.7428, Reconstruct Loss = 0.0282, Cls Loss = 0.0791, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0774, Reg Loss = 9.3528, Reconstruct Loss = 0.0298, Cls Loss = 0.0743, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0795, Reg Loss = 9.6577, Reconstruct Loss = 0.0287, Cls Loss = 0.0765, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0770, Reg Loss = 9.6002, Reconstruct Loss = 0.0284, Cls Loss = 0.0741, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0777, Reg Loss = 9.6565, Reconstruct Loss = 0.0287, Cls Loss = 0.0747, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0785, Reg Loss = 9.7202, Reconstruct Loss = 0.0281, Cls Loss = 0.0756, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0780, Reg Loss = 9.6141, Reconstruct Loss = 0.0280, Cls Loss = 0.0751, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0783, Reg Loss = 9.5503, Reconstruct Loss = 0.0284, Cls Loss = 0.0753, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0789, Reg Loss = 9.4675, Reconstruct Loss = 0.0289, Cls Loss = 0.0760, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0786, Reg Loss = 9.4072, Reconstruct Loss = 0.0287, Cls Loss = 0.0756, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0784, Reg Loss = 9.4774, Reconstruct Loss = 0.0283, Cls Loss = 0.0755, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0778, Reg Loss = 9.4869, Reconstruct Loss = 0.0282, Cls Loss = 0.0748, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0784, Reg Loss = 9.4343, Reconstruct Loss = 0.0285, Cls Loss = 0.0754, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0781, Reg Loss = 9.3763, Reconstruct Loss = 0.0284, Cls Loss = 0.0751, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0777, Reg Loss = 9.3984, Reconstruct Loss = 0.0284, Cls Loss = 0.0748, Learning rate = 1.0000e-03\n",
      "Epoch [25/50], Training Loss: 0.0780, Training Accuracy: 97.68, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Validation Loss: 12.0483, Validation Accuracy: 32.03%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1893, Reg Loss = 9.8908, Reconstruct Loss = 0.0222, Cls Loss = 0.1870, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0794, Reg Loss = 9.4277, Reconstruct Loss = 0.0270, Cls Loss = 0.0766, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0759, Reg Loss = 9.3033, Reconstruct Loss = 0.0272, Cls Loss = 0.0731, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0763, Reg Loss = 9.1747, Reconstruct Loss = 0.0270, Cls Loss = 0.0735, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0742, Reg Loss = 9.0970, Reconstruct Loss = 0.0263, Cls Loss = 0.0714, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0731, Reg Loss = 9.0815, Reconstruct Loss = 0.0268, Cls Loss = 0.0703, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0741, Reg Loss = 9.0289, Reconstruct Loss = 0.0268, Cls Loss = 0.0713, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0755, Reg Loss = 9.0555, Reconstruct Loss = 0.0268, Cls Loss = 0.0727, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0762, Reg Loss = 9.0620, Reconstruct Loss = 0.0267, Cls Loss = 0.0734, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0756, Reg Loss = 9.0895, Reconstruct Loss = 0.0269, Cls Loss = 0.0728, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0746, Reg Loss = 9.0870, Reconstruct Loss = 0.0271, Cls Loss = 0.0718, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0743, Reg Loss = 9.0897, Reconstruct Loss = 0.0272, Cls Loss = 0.0715, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0748, Reg Loss = 9.0806, Reconstruct Loss = 0.0273, Cls Loss = 0.0720, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0747, Reg Loss = 9.1249, Reconstruct Loss = 0.0273, Cls Loss = 0.0719, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0744, Reg Loss = 9.1517, Reconstruct Loss = 0.0270, Cls Loss = 0.0716, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0741, Reg Loss = 9.1862, Reconstruct Loss = 0.0267, Cls Loss = 0.0714, Learning rate = 1.0000e-03\n",
      "Epoch [26/50], Training Loss: 0.0742, Training Accuracy: 97.81, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/50], Validation Loss: 7.4796, Validation Accuracy: 42.26%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0615, Reg Loss = 10.0244, Reconstruct Loss = 0.0243, Cls Loss = 0.0590, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0619, Reg Loss = 9.0721, Reconstruct Loss = 0.0247, Cls Loss = 0.0594, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0650, Reg Loss = 9.2333, Reconstruct Loss = 0.0255, Cls Loss = 0.0624, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0678, Reg Loss = 9.1206, Reconstruct Loss = 0.0267, Cls Loss = 0.0650, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0683, Reg Loss = 9.0222, Reconstruct Loss = 0.0276, Cls Loss = 0.0654, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0682, Reg Loss = 9.0381, Reconstruct Loss = 0.0278, Cls Loss = 0.0653, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0692, Reg Loss = 9.1350, Reconstruct Loss = 0.0279, Cls Loss = 0.0663, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0705, Reg Loss = 9.1678, Reconstruct Loss = 0.0276, Cls Loss = 0.0677, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0712, Reg Loss = 9.2303, Reconstruct Loss = 0.0273, Cls Loss = 0.0684, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0719, Reg Loss = 9.3432, Reconstruct Loss = 0.0271, Cls Loss = 0.0691, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0724, Reg Loss = 9.3690, Reconstruct Loss = 0.0270, Cls Loss = 0.0696, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0720, Reg Loss = 9.3442, Reconstruct Loss = 0.0268, Cls Loss = 0.0693, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0726, Reg Loss = 9.4142, Reconstruct Loss = 0.0267, Cls Loss = 0.0698, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0724, Reg Loss = 9.4796, Reconstruct Loss = 0.0264, Cls Loss = 0.0697, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0721, Reg Loss = 9.5054, Reconstruct Loss = 0.0262, Cls Loss = 0.0694, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0723, Reg Loss = 9.5263, Reconstruct Loss = 0.0261, Cls Loss = 0.0696, Learning rate = 1.0000e-03\n",
      "Epoch [27/50], Training Loss: 0.0727, Training Accuracy: 97.84, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50], Validation Loss: 15.2969, Validation Accuracy: 25.14%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0755, Reg Loss = 9.9238, Reconstruct Loss = 0.0266, Cls Loss = 0.0728, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0691, Reg Loss = 9.4259, Reconstruct Loss = 0.0258, Cls Loss = 0.0664, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0704, Reg Loss = 9.1049, Reconstruct Loss = 0.0270, Cls Loss = 0.0676, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0694, Reg Loss = 9.0601, Reconstruct Loss = 0.0266, Cls Loss = 0.0667, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0689, Reg Loss = 9.0925, Reconstruct Loss = 0.0262, Cls Loss = 0.0662, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0693, Reg Loss = 9.1411, Reconstruct Loss = 0.0264, Cls Loss = 0.0665, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0687, Reg Loss = 9.2283, Reconstruct Loss = 0.0262, Cls Loss = 0.0660, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0696, Reg Loss = 9.1732, Reconstruct Loss = 0.0262, Cls Loss = 0.0669, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0694, Reg Loss = 9.1985, Reconstruct Loss = 0.0266, Cls Loss = 0.0666, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0700, Reg Loss = 9.2194, Reconstruct Loss = 0.0268, Cls Loss = 0.0672, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0701, Reg Loss = 9.2556, Reconstruct Loss = 0.0268, Cls Loss = 0.0673, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0694, Reg Loss = 9.3104, Reconstruct Loss = 0.0267, Cls Loss = 0.0666, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0687, Reg Loss = 9.3807, Reconstruct Loss = 0.0264, Cls Loss = 0.0660, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0684, Reg Loss = 9.4240, Reconstruct Loss = 0.0261, Cls Loss = 0.0657, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0684, Reg Loss = 9.4446, Reconstruct Loss = 0.0258, Cls Loss = 0.0657, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0690, Reg Loss = 9.4785, Reconstruct Loss = 0.0257, Cls Loss = 0.0664, Learning rate = 1.0000e-03\n",
      "Epoch [28/50], Training Loss: 0.0695, Training Accuracy: 97.92, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50], Validation Loss: 7.8456, Validation Accuracy: 40.84%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0375, Reg Loss = 8.3818, Reconstruct Loss = 0.0250, Cls Loss = 0.0349, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0703, Reg Loss = 8.8478, Reconstruct Loss = 0.0251, Cls Loss = 0.0677, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0711, Reg Loss = 9.0680, Reconstruct Loss = 0.0246, Cls Loss = 0.0686, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0707, Reg Loss = 9.2497, Reconstruct Loss = 0.0244, Cls Loss = 0.0682, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0713, Reg Loss = 9.1665, Reconstruct Loss = 0.0239, Cls Loss = 0.0689, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0713, Reg Loss = 9.0317, Reconstruct Loss = 0.0240, Cls Loss = 0.0688, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0708, Reg Loss = 9.0345, Reconstruct Loss = 0.0240, Cls Loss = 0.0683, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0696, Reg Loss = 9.0382, Reconstruct Loss = 0.0240, Cls Loss = 0.0672, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0690, Reg Loss = 9.0882, Reconstruct Loss = 0.0238, Cls Loss = 0.0665, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0685, Reg Loss = 9.1199, Reconstruct Loss = 0.0236, Cls Loss = 0.0660, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0681, Reg Loss = 9.1184, Reconstruct Loss = 0.0237, Cls Loss = 0.0657, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0682, Reg Loss = 9.1922, Reconstruct Loss = 0.0236, Cls Loss = 0.0658, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0685, Reg Loss = 9.2354, Reconstruct Loss = 0.0236, Cls Loss = 0.0661, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0680, Reg Loss = 9.2659, Reconstruct Loss = 0.0239, Cls Loss = 0.0655, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0678, Reg Loss = 9.2748, Reconstruct Loss = 0.0242, Cls Loss = 0.0653, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0677, Reg Loss = 9.2749, Reconstruct Loss = 0.0242, Cls Loss = 0.0652, Learning rate = 1.0000e-03\n",
      "Epoch [29/50], Training Loss: 0.0676, Training Accuracy: 98.09, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50], Validation Loss: 13.7934, Validation Accuracy: 28.11%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0633, Reg Loss = 8.9175, Reconstruct Loss = 0.0211, Cls Loss = 0.0611, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0649, Reg Loss = 9.9681, Reconstruct Loss = 0.0233, Cls Loss = 0.0625, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0704, Reg Loss = 9.7885, Reconstruct Loss = 0.0239, Cls Loss = 0.0679, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0696, Reg Loss = 9.8146, Reconstruct Loss = 0.0244, Cls Loss = 0.0671, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0699, Reg Loss = 9.8287, Reconstruct Loss = 0.0243, Cls Loss = 0.0674, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0680, Reg Loss = 9.7213, Reconstruct Loss = 0.0243, Cls Loss = 0.0655, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0676, Reg Loss = 9.6682, Reconstruct Loss = 0.0242, Cls Loss = 0.0651, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0672, Reg Loss = 9.7031, Reconstruct Loss = 0.0242, Cls Loss = 0.0647, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0678, Reg Loss = 9.7498, Reconstruct Loss = 0.0243, Cls Loss = 0.0653, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0682, Reg Loss = 9.7766, Reconstruct Loss = 0.0243, Cls Loss = 0.0656, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0679, Reg Loss = 9.8001, Reconstruct Loss = 0.0245, Cls Loss = 0.0653, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0676, Reg Loss = 9.8245, Reconstruct Loss = 0.0246, Cls Loss = 0.0650, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0680, Reg Loss = 9.8921, Reconstruct Loss = 0.0245, Cls Loss = 0.0654, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0679, Reg Loss = 9.9429, Reconstruct Loss = 0.0244, Cls Loss = 0.0654, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0681, Reg Loss = 10.0061, Reconstruct Loss = 0.0245, Cls Loss = 0.0656, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0680, Reg Loss = 10.0378, Reconstruct Loss = 0.0245, Cls Loss = 0.0654, Learning rate = 1.0000e-03\n",
      "Epoch [30/50], Training Loss: 0.0681, Training Accuracy: 97.90, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Validation Loss: 22.8890, Validation Accuracy: 26.01%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0559, Reg Loss = 10.3679, Reconstruct Loss = 0.0219, Cls Loss = 0.0536, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0584, Reg Loss = 10.2665, Reconstruct Loss = 0.0230, Cls Loss = 0.0560, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0573, Reg Loss = 9.9633, Reconstruct Loss = 0.0237, Cls Loss = 0.0548, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0590, Reg Loss = 9.9128, Reconstruct Loss = 0.0234, Cls Loss = 0.0566, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0606, Reg Loss = 9.8846, Reconstruct Loss = 0.0232, Cls Loss = 0.0582, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0605, Reg Loss = 9.8122, Reconstruct Loss = 0.0230, Cls Loss = 0.0581, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0609, Reg Loss = 9.9377, Reconstruct Loss = 0.0229, Cls Loss = 0.0585, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0623, Reg Loss = 9.9302, Reconstruct Loss = 0.0228, Cls Loss = 0.0599, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0642, Reg Loss = 9.9582, Reconstruct Loss = 0.0229, Cls Loss = 0.0618, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0642, Reg Loss = 9.9529, Reconstruct Loss = 0.0229, Cls Loss = 0.0619, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0637, Reg Loss = 9.9226, Reconstruct Loss = 0.0228, Cls Loss = 0.0613, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0641, Reg Loss = 9.9640, Reconstruct Loss = 0.0228, Cls Loss = 0.0617, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0642, Reg Loss = 9.9625, Reconstruct Loss = 0.0227, Cls Loss = 0.0618, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0649, Reg Loss = 9.9668, Reconstruct Loss = 0.0227, Cls Loss = 0.0625, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0653, Reg Loss = 9.9828, Reconstruct Loss = 0.0229, Cls Loss = 0.0630, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0657, Reg Loss = 9.9826, Reconstruct Loss = 0.0229, Cls Loss = 0.0633, Learning rate = 1.0000e-03\n",
      "Epoch [31/50], Training Loss: 0.0656, Training Accuracy: 98.05, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50], Validation Loss: 19.1323, Validation Accuracy: 21.71%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0469, Reg Loss = 9.1809, Reconstruct Loss = 0.0313, Cls Loss = 0.0437, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0605, Reg Loss = 9.7012, Reconstruct Loss = 0.0248, Cls Loss = 0.0580, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0606, Reg Loss = 9.8043, Reconstruct Loss = 0.0238, Cls Loss = 0.0581, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0601, Reg Loss = 9.9345, Reconstruct Loss = 0.0232, Cls Loss = 0.0576, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0616, Reg Loss = 9.9308, Reconstruct Loss = 0.0227, Cls Loss = 0.0592, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0626, Reg Loss = 10.0563, Reconstruct Loss = 0.0227, Cls Loss = 0.0603, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0623, Reg Loss = 9.9819, Reconstruct Loss = 0.0231, Cls Loss = 0.0599, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0620, Reg Loss = 9.9186, Reconstruct Loss = 0.0229, Cls Loss = 0.0596, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0626, Reg Loss = 9.9245, Reconstruct Loss = 0.0227, Cls Loss = 0.0602, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0623, Reg Loss = 9.9183, Reconstruct Loss = 0.0225, Cls Loss = 0.0599, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0626, Reg Loss = 9.8950, Reconstruct Loss = 0.0226, Cls Loss = 0.0602, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0623, Reg Loss = 9.8641, Reconstruct Loss = 0.0227, Cls Loss = 0.0599, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0625, Reg Loss = 9.8088, Reconstruct Loss = 0.0228, Cls Loss = 0.0601, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0631, Reg Loss = 9.7618, Reconstruct Loss = 0.0227, Cls Loss = 0.0608, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0627, Reg Loss = 9.7565, Reconstruct Loss = 0.0225, Cls Loss = 0.0603, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0630, Reg Loss = 9.7273, Reconstruct Loss = 0.0225, Cls Loss = 0.0607, Learning rate = 1.0000e-03\n",
      "Epoch [32/50], Training Loss: 0.0631, Training Accuracy: 98.08, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Validation Loss: 15.3242, Validation Accuracy: 29.48%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0378, Reg Loss = 9.6511, Reconstruct Loss = 0.0245, Cls Loss = 0.0352, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0599, Reg Loss = 9.4878, Reconstruct Loss = 0.0243, Cls Loss = 0.0574, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0589, Reg Loss = 9.5013, Reconstruct Loss = 0.0238, Cls Loss = 0.0565, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0594, Reg Loss = 9.5181, Reconstruct Loss = 0.0233, Cls Loss = 0.0569, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0627, Reg Loss = 9.5012, Reconstruct Loss = 0.0232, Cls Loss = 0.0603, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0636, Reg Loss = 9.4992, Reconstruct Loss = 0.0241, Cls Loss = 0.0611, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0626, Reg Loss = 9.5673, Reconstruct Loss = 0.0247, Cls Loss = 0.0601, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0639, Reg Loss = 9.6218, Reconstruct Loss = 0.0247, Cls Loss = 0.0613, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0642, Reg Loss = 9.7013, Reconstruct Loss = 0.0246, Cls Loss = 0.0617, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0634, Reg Loss = 9.7298, Reconstruct Loss = 0.0247, Cls Loss = 0.0608, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0632, Reg Loss = 9.7343, Reconstruct Loss = 0.0248, Cls Loss = 0.0607, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0633, Reg Loss = 9.7213, Reconstruct Loss = 0.0249, Cls Loss = 0.0607, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0631, Reg Loss = 9.7324, Reconstruct Loss = 0.0248, Cls Loss = 0.0605, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0635, Reg Loss = 9.7581, Reconstruct Loss = 0.0246, Cls Loss = 0.0609, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0633, Reg Loss = 9.7609, Reconstruct Loss = 0.0245, Cls Loss = 0.0607, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0638, Reg Loss = 9.7639, Reconstruct Loss = 0.0243, Cls Loss = 0.0613, Learning rate = 1.0000e-03\n",
      "Epoch [33/50], Training Loss: 0.0636, Training Accuracy: 98.03, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 31.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50], Validation Loss: 20.6882, Validation Accuracy: 26.99%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0990, Reg Loss = 9.4353, Reconstruct Loss = 0.0213, Cls Loss = 0.0968, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0655, Reg Loss = 9.7059, Reconstruct Loss = 0.0226, Cls Loss = 0.0631, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0604, Reg Loss = 9.6730, Reconstruct Loss = 0.0231, Cls Loss = 0.0580, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0628, Reg Loss = 9.7257, Reconstruct Loss = 0.0236, Cls Loss = 0.0604, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0624, Reg Loss = 9.7734, Reconstruct Loss = 0.0236, Cls Loss = 0.0599, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0628, Reg Loss = 9.7722, Reconstruct Loss = 0.0236, Cls Loss = 0.0603, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0629, Reg Loss = 9.7805, Reconstruct Loss = 0.0236, Cls Loss = 0.0604, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0635, Reg Loss = 9.8533, Reconstruct Loss = 0.0236, Cls Loss = 0.0611, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0639, Reg Loss = 9.9171, Reconstruct Loss = 0.0239, Cls Loss = 0.0614, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0647, Reg Loss = 9.9341, Reconstruct Loss = 0.0240, Cls Loss = 0.0622, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0647, Reg Loss = 9.9191, Reconstruct Loss = 0.0242, Cls Loss = 0.0621, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0644, Reg Loss = 9.8981, Reconstruct Loss = 0.0242, Cls Loss = 0.0619, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0641, Reg Loss = 9.8905, Reconstruct Loss = 0.0240, Cls Loss = 0.0616, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0636, Reg Loss = 9.9100, Reconstruct Loss = 0.0238, Cls Loss = 0.0611, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0628, Reg Loss = 9.9499, Reconstruct Loss = 0.0236, Cls Loss = 0.0604, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0622, Reg Loss = 9.9325, Reconstruct Loss = 0.0235, Cls Loss = 0.0598, Learning rate = 1.0000e-03\n",
      "Epoch [34/50], Training Loss: 0.0627, Training Accuracy: 98.11, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Validation Loss: 24.0492, Validation Accuracy: 25.61%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0627, Reg Loss = 9.6338, Reconstruct Loss = 0.0262, Cls Loss = 0.0600, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0607, Reg Loss = 9.7523, Reconstruct Loss = 0.0232, Cls Loss = 0.0583, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0616, Reg Loss = 9.9841, Reconstruct Loss = 0.0226, Cls Loss = 0.0592, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0606, Reg Loss = 10.2219, Reconstruct Loss = 0.0224, Cls Loss = 0.0583, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0632, Reg Loss = 10.2605, Reconstruct Loss = 0.0222, Cls Loss = 0.0609, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0629, Reg Loss = 10.3212, Reconstruct Loss = 0.0221, Cls Loss = 0.0605, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0621, Reg Loss = 10.2849, Reconstruct Loss = 0.0223, Cls Loss = 0.0598, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0619, Reg Loss = 10.3557, Reconstruct Loss = 0.0223, Cls Loss = 0.0595, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0625, Reg Loss = 10.4075, Reconstruct Loss = 0.0223, Cls Loss = 0.0602, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0624, Reg Loss = 10.4206, Reconstruct Loss = 0.0227, Cls Loss = 0.0600, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0614, Reg Loss = 10.4376, Reconstruct Loss = 0.0227, Cls Loss = 0.0591, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0612, Reg Loss = 10.4358, Reconstruct Loss = 0.0227, Cls Loss = 0.0588, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0617, Reg Loss = 10.4472, Reconstruct Loss = 0.0227, Cls Loss = 0.0593, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0614, Reg Loss = 10.4562, Reconstruct Loss = 0.0225, Cls Loss = 0.0590, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0612, Reg Loss = 10.4676, Reconstruct Loss = 0.0225, Cls Loss = 0.0589, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0615, Reg Loss = 10.4258, Reconstruct Loss = 0.0226, Cls Loss = 0.0591, Learning rate = 1.0000e-03\n",
      "Epoch [35/50], Training Loss: 0.0612, Training Accuracy: 98.13, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Validation Loss: 19.4493, Validation Accuracy: 27.93%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0317, Reg Loss = 9.8078, Reconstruct Loss = 0.0241, Cls Loss = 0.0292, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0586, Reg Loss = 9.9515, Reconstruct Loss = 0.0252, Cls Loss = 0.0560, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0608, Reg Loss = 9.8764, Reconstruct Loss = 0.0247, Cls Loss = 0.0583, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0595, Reg Loss = 10.0852, Reconstruct Loss = 0.0244, Cls Loss = 0.0570, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0596, Reg Loss = 10.1747, Reconstruct Loss = 0.0241, Cls Loss = 0.0571, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0603, Reg Loss = 10.2285, Reconstruct Loss = 0.0240, Cls Loss = 0.0578, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0602, Reg Loss = 10.2324, Reconstruct Loss = 0.0241, Cls Loss = 0.0577, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0606, Reg Loss = 10.2384, Reconstruct Loss = 0.0240, Cls Loss = 0.0581, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0609, Reg Loss = 10.2821, Reconstruct Loss = 0.0238, Cls Loss = 0.0584, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0613, Reg Loss = 10.2969, Reconstruct Loss = 0.0238, Cls Loss = 0.0588, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0614, Reg Loss = 10.2928, Reconstruct Loss = 0.0239, Cls Loss = 0.0589, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0616, Reg Loss = 10.3269, Reconstruct Loss = 0.0239, Cls Loss = 0.0591, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0613, Reg Loss = 10.2641, Reconstruct Loss = 0.0239, Cls Loss = 0.0588, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0615, Reg Loss = 10.2237, Reconstruct Loss = 0.0237, Cls Loss = 0.0591, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0618, Reg Loss = 10.2428, Reconstruct Loss = 0.0236, Cls Loss = 0.0593, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0616, Reg Loss = 10.2546, Reconstruct Loss = 0.0234, Cls Loss = 0.0592, Learning rate = 1.0000e-03\n",
      "Epoch [36/50], Training Loss: 0.0617, Training Accuracy: 98.09, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 32.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Validation Loss: 22.0056, Validation Accuracy: 27.84%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0579, Reg Loss = 10.2134, Reconstruct Loss = 0.0246, Cls Loss = 0.0554, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0560, Reg Loss = 10.0282, Reconstruct Loss = 0.0212, Cls Loss = 0.0538, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0572, Reg Loss = 10.0011, Reconstruct Loss = 0.0211, Cls Loss = 0.0550, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.0604, Reg Loss = 9.9893, Reconstruct Loss = 0.0209, Cls Loss = 0.0582, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.0600, Reg Loss = 9.9510, Reconstruct Loss = 0.0209, Cls Loss = 0.0578, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.0611, Reg Loss = 9.9730, Reconstruct Loss = 0.0207, Cls Loss = 0.0589, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.0606, Reg Loss = 9.9686, Reconstruct Loss = 0.0209, Cls Loss = 0.0584, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.0617, Reg Loss = 9.9579, Reconstruct Loss = 0.0210, Cls Loss = 0.0595, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.0626, Reg Loss = 9.9394, Reconstruct Loss = 0.0211, Cls Loss = 0.0604, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.0620, Reg Loss = 9.9244, Reconstruct Loss = 0.0211, Cls Loss = 0.0598, Learning rate = 1.0000e-03\n",
      "Iteration 500: Loss = 0.0613, Reg Loss = 9.9170, Reconstruct Loss = 0.0211, Cls Loss = 0.0591, Learning rate = 1.0000e-03\n",
      "Iteration 550: Loss = 0.0612, Reg Loss = 9.9070, Reconstruct Loss = 0.0211, Cls Loss = 0.0590, Learning rate = 1.0000e-03\n",
      "Iteration 600: Loss = 0.0610, Reg Loss = 9.9756, Reconstruct Loss = 0.0214, Cls Loss = 0.0587, Learning rate = 1.0000e-03\n",
      "Iteration 650: Loss = 0.0609, Reg Loss = 9.9998, Reconstruct Loss = 0.0217, Cls Loss = 0.0586, Learning rate = 1.0000e-03\n",
      "Iteration 700: Loss = 0.0613, Reg Loss = 10.0232, Reconstruct Loss = 0.0217, Cls Loss = 0.0590, Learning rate = 1.0000e-03\n",
      "Iteration 750: Loss = 0.0614, Reg Loss = 10.0828, Reconstruct Loss = 0.0217, Cls Loss = 0.0591, Learning rate = 1.0000e-03\n",
      "Epoch [37/50], Training Loss: 0.0615, Training Accuracy: 98.13, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/50], Validation Loss: 19.1785, Validation Accuracy: 28.05%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.0302, Reg Loss = 10.2267, Reconstruct Loss = 0.0198, Cls Loss = 0.0281, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.0670, Reg Loss = 10.8312, Reconstruct Loss = 0.0209, Cls Loss = 0.0648, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.0613, Reg Loss = 10.6512, Reconstruct Loss = 0.0215, Cls Loss = 0.0590, Learning rate = 1.0000e-03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Iterate over the epochs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, args\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Train the hypernetwork to generate a model with random dimension for one epoch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     train_loss, dim_dict, gt_model_dict, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyper_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43mdim_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_model_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Step the scheduler\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[7], line 100\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, optimizer, criterion, dim_dict, gt_model_dict, epoch_idx, ema, args, device)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Update the EMA if specified\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ema:\n\u001b[1;32m--> 100\u001b[0m     \u001b[43mema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update the EMA after each training step\u001b[39;00m\n\u001b[0;32m    101\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Update the AverageMeter objects\u001b[39;00m\n",
      "File \u001b[1;32md:\\Main\\Education\\Skripsi\\Codes\\1-experiment\\code-trials\\neumeta-trial\\neumeta\\utils\\other_utils.py\u001b[0m, in \u001b[0;36mEMA.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate over the epochs\n",
    "for epoch in range(start_epoch, args.experiment.num_epochs):\n",
    "    # Train the hypernetwork to generate a model with random dimension for one epoch\n",
    "    train_loss, dim_dict, gt_model_dict, train_acc = train_one_epoch(hyper_model, train_loader, optimizer, criterion, \n",
    "                                                                     dim_dict, gt_model_dict, epoch_idx=epoch, ema=ema, \n",
    "                                                                     args=args, device=device)\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print the training loss and learning rate\n",
    "    print(f\"Epoch [{epoch+1}/{args.experiment.num_epochs}], Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc*100:.2f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    # If it's time to evaluate the model\n",
    "    if (epoch + 1) % args.experiment.eval_interval == 0:\n",
    "        # Apply EMA if it is specified\n",
    "        if ema:\n",
    "            ema.apply()  # Save the weights of original model created before training_loop\n",
    "        \n",
    "        # Sample the merged model (create model of same structure before training loop by using the hypernetwork)\n",
    "        # And then test the performance of the hypernetwork by seeing how good it is in generating the weights\n",
    "        model = sample_merge_model(hyper_model, model, args) \n",
    "        # Validate the merged model\n",
    "        val_loss, acc = validate_single(model, val_loader, val_criterion, args=args)\n",
    "\n",
    "        # If EMA is specified, restore the original weights\n",
    "        if ema:\n",
    "            ema.restore()  # Restore the original weights to the weights of the pretrained networks\n",
    "\n",
    "        # Log the validation loss and accuracy to wandb\n",
    "        wandb.log({\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": acc\n",
    "        })\n",
    "        # Print the validation loss and accuracy\n",
    "        print(f\"Epoch [{epoch+1}/{args.experiment.num_epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc*100:.2f}%\")\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # Save the checkpoint if the accuracy is better than the previous best\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            save_checkpoint(f\"{args.training.save_model_path}/cifar10_nerf_best.pth\",hyper_model,optimizer,ema,epoch,best_acc)\n",
    "            print(f\"Checkpoint saved at epoch {epoch} with accuracy: {best_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31152508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Cls Loss</td><td>█████▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▁▁</td></tr><tr><td>Learning rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Loss</td><td>█▇▇▇▆▇█▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▃▂▂▂▂▁▁▂▂▁</td></tr><tr><td>Reconstruct Loss</td><td>█▇▇▅▅▅▇▅▅▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Reg Loss</td><td>▁▁▁▇▇█████▇▆▆▅▄▆▆▆▅▅▄▄▄▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇</td></tr><tr><td>Training accuracy</td><td>▁▁▂▂▃▄▄▄▅▆▆▇▇▇██</td></tr><tr><td>Validation Accuracy</td><td>▂▁▂▁▁▁▂▆▆▇██▇▇▆█</td></tr><tr><td>Validation Loss</td><td>▃▃▂▃▃▃▂▂▁▂▃▄▂██▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Cls Loss</td><td>0.08349</td></tr><tr><td>Learning rate</td><td>0.001</td></tr><tr><td>Loss</td><td>0.0917</td></tr><tr><td>Reconstruct Loss</td><td>0.00755</td></tr><tr><td>Reg Loss</td><td>6.62567</td></tr><tr><td>Training accuracy</td><td>0.97296</td></tr><tr><td>Validation Accuracy</td><td>0.6941</td></tr><tr><td>Validation Loss</td><td>1.52875</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">20250528200306-densenet_train_36_48_mlp_256_4_coordnoise_unsmooth_50e_bs64</strong> at: <a href='https://wandb.ai/efradosuryadi-universitas-indonesia/dense-cifar-trial/runs/rj5hak1i' target=\"_blank\">https://wandb.ai/efradosuryadi-universitas-indonesia/dense-cifar-trial/runs/rj5hak1i</a><br> View project at: <a href='https://wandb.ai/efradosuryadi-universitas-indonesia/dense-cifar-trial' target=\"_blank\">https://wandb.ai/efradosuryadi-universitas-indonesia/dense-cifar-trial</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250528_200307-rj5hak1i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e92a8",
   "metadata": {},
   "source": [
    "### 7 Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac18d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_hypernet_path = args.training.save_model_path + '/cifar10_nerf_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa0d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toy/experiments/densenet_train_36_48_mlp_256_4_coordnoise_unsmooth_50e_bs64/cifar10_nerf_best.pth'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_hypernet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b8286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper model type: resmlp\n",
      "Using scalar 0.1\n",
      "num_freqs:  16 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "hyper_model_test = get_hypernetwork(args, number_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3b7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(saved_hypernet_path, map_location=\"cpu\")  # or \"cuda\" if using GPU\n",
    "hyper_model_test.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from toy/experiments/densenet_bc_40_12_baseline/densenet_bc_40_12_cifar10_baseline_best.pth\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 42.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'DenseNet', 'pretrained_path': 'toy/experiments/densenet_bc_40_12_baseline/densenet_bc_40_12_cifar10_baseline_best.pth', 'layers': 40, 'growth': 12, 'compression': 0.5, 'bottleneck': True, 'drop_rate': 0.0}: hidden_dim 48, Validation Loss: 1.3203, Validation Accuracy: 69.94%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in range(12, 49):\n",
    "    # Create a model for this given dimension\n",
    "    model_trained = create_model(args.model.type,\n",
    "                                 layers=args.model.layers,\n",
    "                                 growth=args.model.growth,\n",
    "                                 compression=args.model.compression,\n",
    "                                 bottleneck=args.model.bottleneck,\n",
    "                                 drop_rate=args.model.drop_rate,\n",
    "                                 path=args.model.pretrained_path,\n",
    "                                 hidden_dim=hidden_dim).to(device)\n",
    "    \n",
    "    # If EMA is specified, apply it\n",
    "    if ema:\n",
    "        print('Applying EMA')\n",
    "        ema.apply()\n",
    "\n",
    "    # Sample the merged model\n",
    "    accumulated_model = sample_merge_model(hyper_model_test, model_trained, args, K=100)\n",
    "\n",
    "    # Validate the merged model\n",
    "    val_loss, acc = validate_single(accumulated_model, val_loader, val_criterion, args=args)\n",
    "\n",
    "    # If EMA is specified, restore the original weights after applying EMA\n",
    "    if ema:\n",
    "        ema.restore()  # Restore the original weights after applying \n",
    "        \n",
    "    # Save the model\n",
    "    save_name = os.path.join(args.training.save_model_path, f\"cifar10_{accumulated_model.__class__.__name__}_dim{hidden_dim}_single.pth\")\n",
    "    torch.save(accumulated_model.state_dict(),save_name)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Test using model {args.model}: hidden_dim {hidden_dim}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc*100:.2f}%\")\n",
    "    print('\\n')\n",
    "\n",
    "    # Define the directory and filename structure\n",
    "    filename = f\"cifar10_results_{args.experiment.name}.txt\"\n",
    "    filepath = os.path.join(args.training.save_model_path, filename)\n",
    "\n",
    "    # Write the results. 'a' is used to append the results; a new file will be created if it doesn't exist.\n",
    "    with open(filepath, \"a\") as file:\n",
    "        file.write(f\"Hidden_dim: {hidden_dim}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc*100:.2f}%\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
