{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4faf3c9",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7f334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e39782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe9dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neumeta.models import create_mnist_model as create_model\n",
    "from neumeta.utils import (\n",
    "    parse_args, print_omegaconf,\n",
    "    load_checkpoint, save_checkpoint,\n",
    "    set_seed,\n",
    "    get_cifar10, get_dataset,\n",
    "    sample_coordinates, sample_subset, shuffle_coordinates_all,\n",
    "    get_hypernetwork, get_optimizer, \n",
    "    sample_weights,\n",
    "    weighted_regression_loss, validate_single, AverageMeter, EMA,\n",
    "    sample_single_model, sample_merge_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af52edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smooth.permute import PermutationManager, compute_tv_loss_for_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7cea0a",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad05e8",
   "metadata": {},
   "source": [
    "### 1 Find maximum dimension of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "504abeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_dim(model_cls):\n",
    "    \"\"\"Find maximum dimension of the model\"\"\"\n",
    "    # Get the learnable parameters of the model\n",
    "    checkpoint = model_cls.learnable_parameter \n",
    "\n",
    "    # Set the maximum value to the length of the checkpoint\n",
    "    max_value = len(checkpoint)\n",
    "\n",
    "    # Iterate over the new model's weight\n",
    "    for i, (k, tensor) in enumerate(checkpoint.items()):\n",
    "        # Handle 2D tensors (e.g., weight matrices) \n",
    "        if len(tensor.shape) == 4:\n",
    "            coords = [tensor.shape[0], tensor.shape[1]]\n",
    "            max_value = max(max_value, max(coords))\n",
    "        # Handle 1D tensors (e.g., biases)\n",
    "        elif len(tensor.shape) == 1:\n",
    "            max_value = max(max_value, tensor.shape[0])\n",
    "    \n",
    "    return max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6413443e",
   "metadata": {},
   "source": [
    "### 2 Initialize wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f49acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wandb(config):\n",
    "    import time\n",
    "    \"\"\"\n",
    "    Initializes Weights and Biases (wandb) with the given configuration.\n",
    "    \n",
    "    Args:\n",
    "        configuration (dict): Configuration parameters for the run.\n",
    "    \"\"\"\n",
    "    # Name the run using current time and configuration name\n",
    "    run_name = f\"{time.strftime('%Y%m%d%H%M%S')}-{config.experiment.name}\"\n",
    "    \n",
    "    wandb.init(project=\"ninr-trial\", name=run_name, config=dict(config), group='LeNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0cc73b",
   "metadata": {},
   "source": [
    "### 3 Initialize model dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed520bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_dict(args, device):\n",
    "    \"\"\"\n",
    "    Initializes a dictionary of models for each dimension in the given range, along with ground truth models for the starting dimension.\n",
    "\n",
    "    Args:\n",
    "        args: An object containing the arguments for initializing the models.\n",
    "\n",
    "    Returns:\n",
    "        dim_dict: A dictionary containing the models for each dimension, along with their corresponding coordinates, keys, indices, size, and ground truth models.\n",
    "        gt_model_dict: A dictionary containing the ground truth models for the starting dimension.\n",
    "    \"\"\"\n",
    "    dim_dict = {}\n",
    "    gt_model_dict = {}\n",
    "    \n",
    "    # Create a model for each dimension in dimensions range\n",
    "    for dim in args.dimensions.range:\n",
    "        model_cls = create_model(args.model.type,\n",
    "                                 hidden_dim=dim,\n",
    "                                 path=args.model.pretrained_path,\n",
    "                                 # smooth=args.model.smooth (Not used in LeNet)\n",
    "                                 ).to(device)\n",
    "        model_cls.eval()\n",
    "        # Sample the coordinates, keys, indices, and the size for the model\n",
    "        coords_tensor, keys_list, indices_list, size_list = sample_coordinates(model_cls)\n",
    "        # Add the model, coordinates, keys, indices, size, and key mask to the dictionary\n",
    "        dim_dict[f\"{dim}\"] = (model_cls, coords_tensor, keys_list, indices_list, size_list, None)\n",
    "\n",
    "        # Print to makes line better\n",
    "        print('\\n')\n",
    "        \n",
    "        # If the dimension is the starting dimension (the dimension of pretrained_model), add the ground truth model to the dictionary\n",
    "        if dim == args.dimensions.start:\n",
    "            print(f\"Loading model for dim {dim}\")\n",
    "            model_trained = create_model(args.model.type, \n",
    "                                         hidden_dim=dim, \n",
    "                                         path=args.model.pretrained_path, \n",
    "                                         # smooth=args.model.smooth\n",
    "                                         ).to(device)\n",
    "            model_trained.load_state_dict(torch.load(args.model.pretrained_path))\n",
    "            model_trained.eval()\n",
    "\n",
    "            # Smooths the ground truth model\n",
    "            print(\"TV original model: \", compute_tv_loss_for_network(model_trained, lambda_tv=1.0).item())\n",
    "            input_tensor = torch.randn(1, 1, 28, 28).to(device)\n",
    "            permute_func = PermutationManager(model_trained, input_tensor)\n",
    "            permute_dict = permute_func.compute_permute_dict()\n",
    "            model_trained = permute_func.apply_permutations(permute_dict, ignored_keys=[\n",
    "                ('conv_1.weight', 'in_channels'), \n",
    "                ('linear.weight', 'out_channels'), \n",
    "                ('linear.bias', 'out_channels')\n",
    "            ])\n",
    "            print(\"TV permutated model: \", compute_tv_loss_for_network(model_trained, lambda_tv=1.0).item())\n",
    "\n",
    "            gt_model_dict[f'{dim}'] = copy.deepcopy(model_trained)\n",
    "    \n",
    "    return dim_dict, gt_model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b8aa0",
   "metadata": {},
   "source": [
    "### 4 Training function for target model of a random dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab93eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model for one epoch\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, dim_dict, gt_model_dict, epoch_idx, ema=None, args=None, device='cpu'):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize AverageMeter objects to track the losses\n",
    "    losses = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    reg_losses = AverageMeter()\n",
    "    reconstruct_losses = AverageMeter()\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Preprocess input\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        # Move the data to the device\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        # Choose a random hidden dimension\n",
    "        hidden_dim = random.choice(args.dimensions.range)\n",
    "        # Get the model class, coordinates, keys, indices, size, and key mask for the chosen dimension\n",
    "        model_cls, coords_tensor, keys_list, indices_list, size_list, key_mask = dim_dict[f\"{hidden_dim}\"]\n",
    "        # Sample a subset the input tensor of the coordinates, keys, indices, size, and selected keys\n",
    "        coords_tensor, keys_list, indices_list, size_list, selected_keys = sample_subset(coords_tensor,\n",
    "                                                                                         keys_list,\n",
    "                                                                                         indices_list,\n",
    "                                                                                         size_list,\n",
    "                                                                                         key_mask,\n",
    "                                                                                         ratio=args.ratio)\n",
    "        # Add noise to the coordinates if specified\n",
    "        if args.training.coordinate_noise > 0.0:\n",
    "            coords_tensor = coords_tensor + (torch.rand_like(coords_tensor) - 0.5) * args.training.coordinate_noise\n",
    "\n",
    "\n",
    "        # Main task of hypernetwork and target network\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        # Sample the weights for the target model using hypernetwork\n",
    "        model_cls, reconstructed_weights = sample_weights(model, model_cls,\n",
    "                                                          coords_tensor, keys_list, indices_list, size_list, key_mask, selected_keys,\n",
    "                                                          device=device, NORM=args.dimensions.norm)\n",
    "        # Forward pass\n",
    "        predict = model_cls(x)\n",
    "\n",
    "\n",
    "        # Compute losses\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        # Compute classification loss\n",
    "        cls_loss = criterion(predict, target) \n",
    "        # Compute regularization loss\n",
    "        reg_loss = sum([torch.norm(w, p=2) for w in reconstructed_weights])\n",
    "        # Compute reconstruction loss if ground truth model is available\n",
    "        if f\"{hidden_dim}\" in gt_model_dict:\n",
    "            gt_model = gt_model_dict[f\"{hidden_dim}\"]\n",
    "            gt_selected_weights = [\n",
    "                w for k, w in gt_model.learnable_parameter.items() if k in selected_keys]\n",
    "\n",
    "            reconstruct_loss = weighted_regression_loss(\n",
    "                reconstructed_weights, gt_selected_weights)\n",
    "        else:\n",
    "            reconstruct_loss = torch.tensor(0.0)\n",
    "        # Compute the total loss\n",
    "        loss = args.hyper_model.loss_weight.ce_weight * cls_loss + args.hyper_model.loss_weight.reg_weight * \\\n",
    "            reg_loss + args.hyper_model.loss_weight.recon_weight * reconstruct_loss\n",
    "\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        # Zero the gradients of the updated weights\n",
    "        for updated_weight in model_cls.parameters():\n",
    "            updated_weight.grad = None\n",
    "\n",
    "        # Compute the gradients of the reconstructed weights\n",
    "        loss.backward(retain_graph=True)\n",
    "        torch.autograd.backward(reconstructed_weights, [\n",
    "                                w.grad for k, w in model_cls.named_parameters() if k in selected_keys])\n",
    "        \n",
    "        # Clip the gradients if specified\n",
    "        if args.training.get('clip_grad', 0.0) > 0:\n",
    "            torch.nn.utils.clip_grad_value_(\n",
    "                model.parameters(), args.training.clip_grad)\n",
    "            \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the EMA if specified\n",
    "        if ema:\n",
    "            ema.update()  # Update the EMA after each training step\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update the AverageMeter objects\n",
    "        losses.update(loss.item())\n",
    "        cls_losses.update(cls_loss.item())\n",
    "        reg_losses.update(reg_loss.item())\n",
    "        reconstruct_losses.update(reconstruct_loss.item())\n",
    "\n",
    "\n",
    "        # Log (or plot) losses\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        # Log the losses and learning rate to wandb\n",
    "        if batch_idx % args.experiment.log_interval == 0:\n",
    "            wandb.log({\n",
    "                \"Loss\": losses.avg,\n",
    "                \"Cls Loss\": cls_losses.avg,\n",
    "                \"Reg Loss\": reg_losses.avg,\n",
    "                \"Reconstruct Loss\": reconstruct_losses.avg,\n",
    "                # \"Recon Weight\": args.hyper_model.loss_weight.recon_weight,  # Newly added for LeNet\n",
    "                \"Learning rate\": optimizer.param_groups[0]['lr']\n",
    "            }, step=batch_idx + epoch_idx * len(train_loader))\n",
    "            # Print the losses and learning rate\n",
    "            print(\n",
    "                f\"Iteration {batch_idx}: Loss = {losses.avg:.4f}, Reg Loss = {reg_losses.avg:.4f}, Reconstruct Loss = {reconstruct_losses.avg:.4f}, Cls Loss = {cls_losses.avg:.4f}, Learning rate = {optimizer.param_groups[0]['lr']:.4e}\")\n",
    "    \n",
    "    # Returns the training loss, structure of network in each dimension, and the original structure of pretrained network\n",
    "    return losses.avg, dim_dict, gt_model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d8965",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e292aae",
   "metadata": {},
   "source": [
    "### 0 Set device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "670c10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd9af3",
   "metadata": {},
   "source": [
    "### 1 Parsing arguments for inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a21d5c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = 'neumeta/config/mnist/LeNet_mnist_8-32_coodnoise.yaml'\n",
    "RATIO = '1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6fd9c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "argv_train = ['--config', CONFIG_PATH, '--ratio', RATIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aebf601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base config from toy/experiments/base_config.yaml\n",
      "+--------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|                 Key                  |                                               Value                                                |\n",
      "+--------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|           experiment.name            |                                       mnist_lenet_8-32-noise                                       |\n",
      "|         experiment.recononly         |                                                 0                                                  |\n",
      "|        experiment.num_epochs         |                                                 50                                                 |\n",
      "|       experiment.log_interval        |                                                 50                                                 |\n",
      "|       experiment.eval_interval       |                                                 1                                                  |\n",
      "|           experiment.seed            |                                                 42                                                 |\n",
      "|              model.type              |                                               LeNet                                                |\n",
      "|        model.pretrained_path         |                                    toy/mnist_MnistNet_dim32.pth                                    |\n",
      "|             model.smooth             |                                               False                                                |\n",
      "|        training.learning_rate        |                                               0.001                                                |\n",
      "|         training.batch_size          |                                                128                                                 |\n",
      "|      training.coordinate_noise       |                                                1.0                                                 |\n",
      "|          training.lr_steps           |                                             [100, 150]                                             |\n",
      "|        training.weight_decay         |                                                0.0                                                 |\n",
      "|          training.clip_grad          |                                                0.0                                                 |\n",
      "|       training.save_model_path       |                              toy/experiments/mnist_lenet_8-32-noise/                               |\n",
      "|           training.dataset           |                                               mnist                                                |\n",
      "|        hyper_model.input_dim         |                                                 6                                                  |\n",
      "|        hyper_model.hidden_dim        |                                                256                                                 |\n",
      "|        hyper_model.num_layers        |                                                 4                                                  |\n",
      "|        hyper_model.num_freqs         |                                                 16                                                 |\n",
      "|        hyper_model.output_dim        |                                                 25                                                 |\n",
      "|        hyper_model.ema_decay         |                                               0.995                                                |\n",
      "|  hyper_model.loss_weight.ce_weight   |                                                1.0                                                 |\n",
      "|  hyper_model.loss_weight.reg_weight  |                                               0.0001                                               |\n",
      "| hyper_model.loss_weight.recon_weight |                                                1.0                                                 |\n",
      "|  hyper_model.loss_weight.kd_weight   |                                                0.1                                                 |\n",
      "|           hyper_model.type           |                                               resmlp                                               |\n",
      "|           dimensions.range           | [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32] |\n",
      "|           dimensions.test            |                                                 32                                                 |\n",
      "|           dimensions.norm            |                                                 32                                                 |\n",
      "|           dimensions.start           |                                                 32                                                 |\n",
      "|             base_config              |                                  toy/experiments/base_config.yaml                                  |\n",
      "|                config                |                        neumeta/config/mnist/LeNet_mnist_8-32_coodnoise.yaml                        |\n",
      "|                ratio                 |                                                1.0                                                 |\n",
      "|             resume_from              |                                                None                                                |\n",
      "|              load_from               |                                                None                                                |\n",
      "|           test_result_path           |                                                None                                                |\n",
      "|                 test                 |                                               False                                                |\n",
      "+--------------------------------------+----------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "args = parse_args(argv_train)  # Parse arguments\n",
    "print_omegaconf(args)  # Print arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84954fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed... 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.experiment.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2086b8",
   "metadata": {},
   "source": [
    "### 2 Get training and validation data (in dataloader format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "633b90a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: mnist with batch size: 128 and strong transform: None\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_dataset(args.training.dataset, args.training.batch_size, strong_transform=args.training.get('strong_aug', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1685e32a",
   "metadata": {},
   "source": [
    "### 3 Create target model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d8f75",
   "metadata": {},
   "source": [
    "#### 3.0 Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c47436d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(args.model.type,\n",
    "                     hidden_dim=args.dimensions.start,\n",
    "                     path=args.model.pretrained_path,\n",
    "                     # smooth=args.model.smooth\n",
    "                     ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20c49cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistNet(\n",
       "  (conv_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "  (conv_2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "  (conv_3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "  (f_1): ReLU()\n",
       "  (f_2): ReLU()\n",
       "  (f_3): ReLU()\n",
       "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (linear): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load LeNet pretrained weight\n",
    "model.load_state_dict(torch.load(args.model.pretrained_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb133f",
   "metadata": {},
   "source": [
    "#### 3.1 Print the structure and shape of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c09f9614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistNet(\n",
       "  (conv_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "  (conv_2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "  (conv_3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "  (f_1): ReLU()\n",
       "  (f_2): ReLU()\n",
       "  (f_3): ReLU()\n",
       "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (linear): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b86d2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_1.weight torch.Size([32, 1, 3, 3])\n",
      "conv_1.bias torch.Size([32])\n",
      "conv_2.weight torch.Size([64, 32, 5, 5])\n",
      "conv_2.bias torch.Size([64])\n",
      "conv_3.weight torch.Size([128, 64, 5, 5])\n",
      "conv_3.bias torch.Size([128])\n",
      "linear.weight torch.Size([10, 128])\n",
      "linear.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i, (k, tensor) in enumerate(model.learnable_parameter.items()):\n",
    "    print(k, tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f7b45",
   "metadata": {},
   "source": [
    "#### 3.2 The maximum dimension of the target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bad8c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the maximum dimension of the model\n",
    "# print(f'Maximum DIM: {find_max_dim(model)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195929e",
   "metadata": {},
   "source": [
    "#### 3.3 Validate the accuracy of pretrained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74d75f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Permutated model Validation Loss: 0.0621, Validation Accuracy: 98.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate the model for the starting dimension (its pretrained form)\n",
    "val_loss, acc = validate_single(model, val_loader, nn.CrossEntropyLoss(), args=args)\n",
    "print(f'Initial Permutated model Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb61dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the learnable parameters of the model\n",
    "checkpoint = model.learnable_parameter\n",
    "# Get the number of parameters\n",
    "number_param = len(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b2fe99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters keys: ['conv_1.weight', 'conv_1.bias', 'conv_2.weight', 'conv_2.bias', 'conv_3.weight', 'conv_3.bias', 'linear.weight', 'linear.bias']\n",
      "Number of parameters to be learned: 8\n"
     ]
    }
   ],
   "source": [
    "# Print the keys of the parameters and the number of parameters\n",
    "print(f\"Parameters keys: {model.keys}\")\n",
    "print(f\"Number of parameters to be learned: {number_param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5fa6f",
   "metadata": {},
   "source": [
    "### 4 Create the hypernetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa4a2f",
   "metadata": {},
   "source": [
    "#### 4.0 Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a82e9f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper model type: resmlp\n",
      "Using scalar 0.1\n",
      "num_freqs:  16 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Get the hypermodel\n",
    "hyper_model = get_hypernetwork(args, number_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297bd73",
   "metadata": {},
   "source": [
    "#### 4.1 Print model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "903829b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeRF_ResMLP_Compose(\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (model): ModuleList(\n",
       "    (0-7): 8 x NeRF_MLP_Residual_Scaled(\n",
       "      (initial_layer): Linear(in_features=198, out_features=256, bias=True)\n",
       "      (residual_blocks): ModuleList(\n",
       "        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (scalars): ParameterList(\n",
       "          (0): Parameter containing: [torch.float32 of size  (cuda:0)]\n",
       "          (1): Parameter containing: [torch.float32 of size  (cuda:0)]\n",
       "          (2): Parameter containing: [torch.float32 of size  (cuda:0)]\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "      (output_layer): Linear(in_features=256, out_features=25, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d592e",
   "metadata": {},
   "source": [
    "#### 4.2 Initialize EMA to track only a smooth version of the model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11a4812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the EMA\n",
    "ema = EMA(hyper_model, decay=args.hyper_model.ema_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fffae0",
   "metadata": {},
   "source": [
    "### 5 Get Loss function, Optimizer, and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "079aa373",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion, val_criterion, optimizer, scheduler = get_optimizer(args, hyper_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7561c42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: CrossEntropyLoss()\n",
      "Val_criterion: CrossEntropyLoss()\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.MultiStepLR object at 0x0000021C47E4FFD0>\n"
     ]
    }
   ],
   "source": [
    "print(f'Criterion: {criterion}\\nVal_criterion: {val_criterion}\\nOptimizer: {optimizer}\\nScheduler: {scheduler}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7906454",
   "metadata": {},
   "source": [
    "### 6 Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d1d4f",
   "metadata": {},
   "source": [
    "#### 6.1 Initialize training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2cdca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the starting epoch and best accuracy\n",
    "start_epoch = 0\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5373217",
   "metadata": {},
   "source": [
    "#### 6.2 Directory to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35a2746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory to save the model\n",
    "os.makedirs(args.training.save_model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504af23",
   "metadata": {},
   "source": [
    "#### 6.3 Resume training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "714db3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.resume_from:\n",
    "        print(f\"Resuming from checkpoint: {args.resume_from}\")\n",
    "        checkpoint_info = load_checkpoint(args.resume_from, hyper_model, optimizer, ema)\n",
    "        start_epoch = checkpoint_info['epoch']\n",
    "        best_acc = checkpoint_info['best_acc']\n",
    "        print(f\"Resuming from epoch: {start_epoch}, best accuracy: {best_acc*100:.2f}%\")\n",
    "        # Note: If there are more elements to retrieve, do so here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a3248",
   "metadata": {},
   "source": [
    "#### 6.4 Initialize wandb for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d9fd46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mefradosuryadi\u001b[0m (\u001b[33mefradosuryadi-universitas-indonesia\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Main\\Education\\Skripsi\\Codes\\1-experiment\\code-trials\\neumeta-trial\\wandb\\run-20250520_180301-zkhan3tm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efradosuryadi-universitas-indonesia/ninr-trial/runs/zkhan3tm' target=\"_blank\">20250520180300-mnist_lenet_8-32-noise</a></strong> to <a href='https://wandb.ai/efradosuryadi-universitas-indonesia/ninr-trial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efradosuryadi-universitas-indonesia/ninr-trial' target=\"_blank\">https://wandb.ai/efradosuryadi-universitas-indonesia/ninr-trial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efradosuryadi-universitas-indonesia/ninr-trial/runs/zkhan3tm' target=\"_blank\">https://wandb.ai/efradosuryadi-universitas-indonesia/ninr-trial/runs/zkhan3tm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "initialize_wandb(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46056e65",
   "metadata": {},
   "source": [
    "#### 6.5 Initialize model dictionary for each dimension and shuffle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "641149b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loading model for dim 32\n",
      "TV original model:  3088.543212890625\n",
      "TV permutated model:  2175.4375\n"
     ]
    }
   ],
   "source": [
    "# Initialize model dictionary\n",
    "dim_dict, gt_model_dict = init_model_dict(args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d01d9a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'32': MnistNet(\n",
       "   (conv_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "   (conv_2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "   (conv_3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "   (f_1): ReLU()\n",
       "   (f_2): ReLU()\n",
       "   (f_3): ReLU()\n",
       "   (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "   (linear): Linear(in_features=128, out_features=10, bias=True)\n",
       " )}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26d9c955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'8': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=32, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8.,  8.,  1.],\n",
       "          [ 0.,  1.,  0.,  8.,  8.,  1.],\n",
       "          [ 0.,  2.,  0.,  8.,  8.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '9': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(9, 18, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(18, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=36, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8.,  9.,  1.],\n",
       "          [ 0.,  1.,  0.,  8.,  9.,  1.],\n",
       "          [ 0.,  2.,  0.,  8.,  9.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '10': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=40, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 10.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 10.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 10.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '11': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(11, 22, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(22, 44, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=44, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 11.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 11.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 11.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '12': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(24, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=48, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 12.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 12.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 12.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '13': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(13, 26, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(26, 52, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=52, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 13.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 13.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 13.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '14': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(14, 28, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(28, 56, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=56, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 14.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 14.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 14.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '15': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(15, 30, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(30, 60, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=60, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 15.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 15.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 15.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '16': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 16.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 16.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 16.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '17': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(17, 34, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(34, 68, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=68, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 17.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 17.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 17.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '18': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(18, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(36, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=72, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 18.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 18.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 18.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '19': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(19, 38, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(38, 76, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=76, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 19.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 19.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 19.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '20': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(40, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=80, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 20.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 20.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 20.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '21': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(21, 42, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(42, 84, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=84, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 21.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 21.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 21.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '22': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(22, 44, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(44, 88, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=88, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 22.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 22.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 22.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '23': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(23, 46, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(46, 92, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=92, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 23.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 23.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 23.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '24': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(24, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(48, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=96, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 24.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 24.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 24.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '25': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(25, 50, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(50, 100, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=100, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 25.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 25.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 25.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '26': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(26, 52, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(52, 104, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=104, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 26.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 26.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 26.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '27': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(27, 54, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(54, 108, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=108, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 27.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 27.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 27.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '28': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(28, 56, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(56, 112, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=112, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 28.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 28.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 28.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '29': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(29, 58, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(58, 116, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=116, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 29.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 29.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 29.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '30': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(30, 60, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(60, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=120, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 30.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 30.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 30.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '31': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(31, 62, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(62, 124, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=124, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 31.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 31.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 31.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None),\n",
       " '32': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=128, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 32.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 32.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 32.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  None)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7476d68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'8': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=32, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8.,  8.,  1.],\n",
       "          [ 0.,  1.,  0.,  8.,  8.,  1.],\n",
       "          [ 0.,  2.,  0.,  8.,  8.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '9': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(9, 18, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(18, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=36, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8.,  9.,  1.],\n",
       "          [ 0.,  1.,  0.,  8.,  9.,  1.],\n",
       "          [ 0.,  2.,  0.,  8.,  9.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '10': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=40, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 10.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 10.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 10.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '11': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(11, 22, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(22, 44, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=44, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 11.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 11.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 11.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '12': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(24, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=48, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 12.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 12.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 12.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '13': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(13, 26, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(26, 52, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=52, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 13.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 13.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 13.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '14': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(14, 28, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(28, 56, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=56, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 14.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 14.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 14.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '15': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(15, 30, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(30, 60, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=60, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 15.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 15.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 15.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '16': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 16.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 16.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 16.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '17': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(17, 34, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(34, 68, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=68, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 17.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 17.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 17.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '18': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(18, 36, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(36, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=72, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 18.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 18.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 18.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '19': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(19, 38, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(38, 76, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=76, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 19.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 19.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 19.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '20': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(40, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=80, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 20.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 20.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 20.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '21': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(21, 42, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(42, 84, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=84, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 21.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 21.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 21.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '22': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(22, 44, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(44, 88, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=88, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 22.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 22.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 22.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '23': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(23, 46, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(46, 92, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=92, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 23.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 23.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 23.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '24': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(24, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(48, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=96, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 24.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 24.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 24.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '25': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(25, 50, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(50, 100, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=100, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 25.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 25.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 25.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '26': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(26, 52, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(52, 104, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=104, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 26.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 26.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 26.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '27': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(27, 54, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(54, 108, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=108, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 27.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 27.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 27.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '28': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(28, 56, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(56, 112, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=112, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 28.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 28.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 28.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '29': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(29, 58, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(58, 116, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=116, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 29.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 29.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 29.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '30': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(30, 60, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(60, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=120, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 30.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 30.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 30.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '31': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(31, 62, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(62, 124, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=124, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 31.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 31.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 31.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])}),\n",
       " '32': (MnistNet(\n",
       "    (conv_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
       "    (conv_2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (conv_3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
       "    (f_1): ReLU()\n",
       "    (f_2): ReLU()\n",
       "    (f_3): ReLU()\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (linear): Linear(in_features=128, out_features=10, bias=True)\n",
       "  ),\n",
       "  tensor([[ 0.,  0.,  0.,  8., 32.,  1.],\n",
       "          [ 0.,  1.,  0.,  8., 32.,  1.],\n",
       "          [ 0.,  2.,  0.,  8., 32.,  1.],\n",
       "          ...,\n",
       "          [ 7.,  7.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  8.,  0.,  8., 10.,  1.],\n",
       "          [ 7.,  9.,  0.,  8., 10.,  1.]]),\n",
       "  array(['conv_1.weight', 'conv_1.weight', 'conv_1.weight', ...,\n",
       "         'linear.bias', 'linear.bias', 'linear.bias'], dtype='<U13'),\n",
       "  tensor([[0, 0, 3, 3],\n",
       "          [1, 0, 3, 3],\n",
       "          [2, 0, 3, 3],\n",
       "          ...,\n",
       "          [7, 0, 0, 0],\n",
       "          [8, 0, 0, 0],\n",
       "          [9, 0, 0, 0]]),\n",
       "  tensor([4, 4, 4,  ..., 1, 1, 1]),\n",
       "  {np.str_('conv_1.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_1.weight'): tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_2.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.bias'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('conv_3.weight'): tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "   np.str_('linear.bias'): tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       "   np.str_('linear.weight'): tensor([0, 0, 0,  ..., 0, 0, 0])})}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_dict = shuffle_coordinates_all(dim_dict)\n",
    "dim_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d6ad4",
   "metadata": {},
   "source": [
    "#### 6.6 Hypernetwork training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "332580b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.experiment.num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4f486f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 2.3828, Reg Loss = 11.6718, Reconstruct Loss = 0.0000, Cls Loss = 2.3817, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 2.3194, Reg Loss = 10.1162, Reconstruct Loss = 0.0010, Cls Loss = 2.3174, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 2.3180, Reg Loss = 9.9594, Reconstruct Loss = 0.0007, Cls Loss = 2.3163, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 2.3146, Reg Loss = 9.3854, Reconstruct Loss = 0.0007, Cls Loss = 2.3129, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 2.3007, Reg Loss = 14.8758, Reconstruct Loss = 0.0029, Cls Loss = 2.2963, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 2.2684, Reg Loss = 25.5748, Reconstruct Loss = 0.0035, Cls Loss = 2.2623, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 2.2431, Reg Loss = 34.4771, Reconstruct Loss = 0.0029, Cls Loss = 2.2368, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 2.2250, Reg Loss = 40.6304, Reconstruct Loss = 0.0032, Cls Loss = 2.2177, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 2.2087, Reg Loss = 46.4492, Reconstruct Loss = 0.0028, Cls Loss = 2.2013, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 2.1970, Reg Loss = 50.6070, Reconstruct Loss = 0.0030, Cls Loss = 2.1889, Learning rate = 1.0000e-03\n",
      "Epoch [1/50], Training Loss: 2.1925, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 54.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Validation Loss: 2.1636, Validation Accuracy: 20.85%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 0 with accuracy: 20.85%\n",
      "Iteration 0: Loss = 1.9897, Reg Loss = 84.2579, Reconstruct Loss = 0.0000, Cls Loss = 1.9812, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 2.1158, Reg Loss = 85.4009, Reconstruct Loss = 0.0069, Cls Loss = 2.1004, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 2.1027, Reg Loss = 82.5665, Reconstruct Loss = 0.0057, Cls Loss = 2.0888, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 2.0919, Reg Loss = 82.5984, Reconstruct Loss = 0.0038, Cls Loss = 2.0799, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 2.0891, Reg Loss = 81.0112, Reconstruct Loss = 0.0063, Cls Loss = 2.0747, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 2.0877, Reg Loss = 78.5290, Reconstruct Loss = 0.0074, Cls Loss = 2.0725, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 2.0844, Reg Loss = 76.2417, Reconstruct Loss = 0.0071, Cls Loss = 2.0697, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 2.0773, Reg Loss = 74.4646, Reconstruct Loss = 0.0073, Cls Loss = 2.0625, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 2.0730, Reg Loss = 73.3591, Reconstruct Loss = 0.0076, Cls Loss = 2.0581, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 2.0727, Reg Loss = 72.7073, Reconstruct Loss = 0.0073, Cls Loss = 2.0581, Learning rate = 1.0000e-03\n",
      "Epoch [2/50], Training Loss: 2.0712, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 55.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Validation Loss: 2.0498, Validation Accuracy: 19.61%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 2.1258, Reg Loss = 71.7127, Reconstruct Loss = 0.0000, Cls Loss = 2.1186, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 2.1568, Reg Loss = 73.8484, Reconstruct Loss = 0.0077, Cls Loss = 2.1417, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 2.1069, Reg Loss = 75.2805, Reconstruct Loss = 0.0077, Cls Loss = 2.0916, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 2.0954, Reg Loss = 74.3859, Reconstruct Loss = 0.0085, Cls Loss = 2.0795, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 2.0860, Reg Loss = 73.2251, Reconstruct Loss = 0.0064, Cls Loss = 2.0723, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 2.0816, Reg Loss = 72.1324, Reconstruct Loss = 0.0081, Cls Loss = 2.0662, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 2.0751, Reg Loss = 70.8331, Reconstruct Loss = 0.0077, Cls Loss = 2.0603, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 2.0681, Reg Loss = 70.2187, Reconstruct Loss = 0.0082, Cls Loss = 2.0529, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 2.0644, Reg Loss = 69.8109, Reconstruct Loss = 0.0075, Cls Loss = 2.0500, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 2.0585, Reg Loss = 69.2814, Reconstruct Loss = 0.0070, Cls Loss = 2.0446, Learning rate = 1.0000e-03\n",
      "Epoch [3/50], Training Loss: 2.0568, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Validation Loss: 2.0209, Validation Accuracy: 21.73%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 2 with accuracy: 21.73%\n",
      "Iteration 0: Loss = 2.0241, Reg Loss = 63.8759, Reconstruct Loss = 0.0000, Cls Loss = 2.0178, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 2.0533, Reg Loss = 61.5759, Reconstruct Loss = 0.0110, Cls Loss = 2.0362, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 2.0374, Reg Loss = 61.7143, Reconstruct Loss = 0.0067, Cls Loss = 2.0245, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 2.0316, Reg Loss = 62.9099, Reconstruct Loss = 0.0045, Cls Loss = 2.0208, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 2.0269, Reg Loss = 63.2430, Reconstruct Loss = 0.0050, Cls Loss = 2.0155, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 2.0228, Reg Loss = 63.4183, Reconstruct Loss = 0.0048, Cls Loss = 2.0117, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 2.0164, Reg Loss = 63.9445, Reconstruct Loss = 0.0044, Cls Loss = 2.0056, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 2.0106, Reg Loss = 64.8504, Reconstruct Loss = 0.0054, Cls Loss = 1.9987, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 2.0048, Reg Loss = 65.4854, Reconstruct Loss = 0.0060, Cls Loss = 1.9923, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.9984, Reg Loss = 66.7642, Reconstruct Loss = 0.0063, Cls Loss = 1.9854, Learning rate = 1.0000e-03\n",
      "Epoch [4/50], Training Loss: 1.9943, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Validation Loss: 1.9219, Validation Accuracy: 27.99%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 3 with accuracy: 27.99%\n",
      "Iteration 0: Loss = 1.8504, Reg Loss = 87.1445, Reconstruct Loss = 0.0000, Cls Loss = 1.8416, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 1.8924, Reg Loss = 86.2481, Reconstruct Loss = 0.0135, Cls Loss = 1.8703, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 1.8803, Reg Loss = 85.9406, Reconstruct Loss = 0.0110, Cls Loss = 1.8607, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 1.8564, Reg Loss = 83.6909, Reconstruct Loss = 0.0086, Cls Loss = 1.8394, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 1.8499, Reg Loss = 83.1222, Reconstruct Loss = 0.0081, Cls Loss = 1.8334, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 1.8407, Reg Loss = 82.6490, Reconstruct Loss = 0.0094, Cls Loss = 1.8230, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 1.8434, Reg Loss = 81.9813, Reconstruct Loss = 0.0083, Cls Loss = 1.8269, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 1.8381, Reg Loss = 82.7208, Reconstruct Loss = 0.0083, Cls Loss = 1.8215, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 1.8289, Reg Loss = 83.8227, Reconstruct Loss = 0.0091, Cls Loss = 1.8114, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.8173, Reg Loss = 84.4437, Reconstruct Loss = 0.0094, Cls Loss = 1.7995, Learning rate = 1.0000e-03\n",
      "Epoch [5/50], Training Loss: 1.8133, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Validation Loss: 1.7398, Validation Accuracy: 28.00%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 4 with accuracy: 28.00%\n",
      "Iteration 0: Loss = 2.0296, Reg Loss = 81.6159, Reconstruct Loss = 0.0000, Cls Loss = 2.0214, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 1.7834, Reg Loss = 94.0362, Reconstruct Loss = 0.0111, Cls Loss = 1.7630, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 1.7468, Reg Loss = 94.6226, Reconstruct Loss = 0.0080, Cls Loss = 1.7293, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 1.7287, Reg Loss = 94.7949, Reconstruct Loss = 0.0101, Cls Loss = 1.7091, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 1.7164, Reg Loss = 94.4905, Reconstruct Loss = 0.0104, Cls Loss = 1.6966, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 1.7090, Reg Loss = 95.0189, Reconstruct Loss = 0.0094, Cls Loss = 1.6901, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 1.7022, Reg Loss = 94.9538, Reconstruct Loss = 0.0094, Cls Loss = 1.6833, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 1.6962, Reg Loss = 94.3480, Reconstruct Loss = 0.0093, Cls Loss = 1.6775, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 1.6918, Reg Loss = 94.0572, Reconstruct Loss = 0.0092, Cls Loss = 1.6732, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.6874, Reg Loss = 93.6650, Reconstruct Loss = 0.0096, Cls Loss = 1.6684, Learning rate = 1.0000e-03\n",
      "Epoch [6/50], Training Loss: 1.6875, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 59.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Validation Loss: 1.5930, Validation Accuracy: 34.56%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 5 with accuracy: 34.56%\n",
      "Iteration 0: Loss = 1.4436, Reg Loss = 92.0683, Reconstruct Loss = 0.0000, Cls Loss = 1.4344, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 1.6196, Reg Loss = 88.2438, Reconstruct Loss = 0.0073, Cls Loss = 1.6035, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 1.6300, Reg Loss = 86.7180, Reconstruct Loss = 0.0055, Cls Loss = 1.6158, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 1.6339, Reg Loss = 85.3734, Reconstruct Loss = 0.0064, Cls Loss = 1.6190, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 1.6410, Reg Loss = 84.6970, Reconstruct Loss = 0.0087, Cls Loss = 1.6239, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 1.6410, Reg Loss = 84.0228, Reconstruct Loss = 0.0088, Cls Loss = 1.6238, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 1.6353, Reg Loss = 83.7846, Reconstruct Loss = 0.0091, Cls Loss = 1.6178, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 1.6341, Reg Loss = 83.4776, Reconstruct Loss = 0.0095, Cls Loss = 1.6163, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 1.6288, Reg Loss = 83.3650, Reconstruct Loss = 0.0089, Cls Loss = 1.6116, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.6269, Reg Loss = 82.9792, Reconstruct Loss = 0.0097, Cls Loss = 1.6089, Learning rate = 1.0000e-03\n",
      "Epoch [7/50], Training Loss: 1.6255, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 56.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Validation Loss: 1.5703, Validation Accuracy: 34.94%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 6 with accuracy: 34.94%\n",
      "Iteration 0: Loss = 1.5532, Reg Loss = 76.3154, Reconstruct Loss = 0.0000, Cls Loss = 1.5456, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 1.5981, Reg Loss = 77.7150, Reconstruct Loss = 0.0093, Cls Loss = 1.5810, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 1.5958, Reg Loss = 78.5416, Reconstruct Loss = 0.0067, Cls Loss = 1.5813, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 1.5961, Reg Loss = 78.8540, Reconstruct Loss = 0.0058, Cls Loss = 1.5824, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 1.5960, Reg Loss = 78.3896, Reconstruct Loss = 0.0089, Cls Loss = 1.5792, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 1.5963, Reg Loss = 77.9999, Reconstruct Loss = 0.0091, Cls Loss = 1.5794, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 1.5920, Reg Loss = 77.9520, Reconstruct Loss = 0.0076, Cls Loss = 1.5766, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 1.5825, Reg Loss = 78.4704, Reconstruct Loss = 0.0073, Cls Loss = 1.5674, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 1.5774, Reg Loss = 78.7752, Reconstruct Loss = 0.0079, Cls Loss = 1.5616, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.5679, Reg Loss = 79.3371, Reconstruct Loss = 0.0070, Cls Loss = 1.5529, Learning rate = 1.0000e-03\n",
      "Epoch [8/50], Training Loss: 1.5654, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Validation Loss: 1.5425, Validation Accuracy: 45.34%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 7 with accuracy: 45.34%\n",
      "Iteration 0: Loss = 1.5288, Reg Loss = 78.4068, Reconstruct Loss = 0.0000, Cls Loss = 1.5209, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 1.4687, Reg Loss = 84.3875, Reconstruct Loss = 0.0052, Cls Loss = 1.4551, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 1.4464, Reg Loss = 85.5270, Reconstruct Loss = 0.0052, Cls Loss = 1.4327, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 1.4531, Reg Loss = 84.4830, Reconstruct Loss = 0.0129, Cls Loss = 1.4317, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 1.4562, Reg Loss = 83.8119, Reconstruct Loss = 0.0115, Cls Loss = 1.4364, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 1.4459, Reg Loss = 83.7982, Reconstruct Loss = 0.0112, Cls Loss = 1.4264, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 1.4423, Reg Loss = 83.2111, Reconstruct Loss = 0.0099, Cls Loss = 1.4240, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 1.4388, Reg Loss = 82.4791, Reconstruct Loss = 0.0101, Cls Loss = 1.4204, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 1.4320, Reg Loss = 82.0998, Reconstruct Loss = 0.0097, Cls Loss = 1.4141, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.4247, Reg Loss = 81.8305, Reconstruct Loss = 0.0096, Cls Loss = 1.4069, Learning rate = 1.0000e-03\n",
      "Epoch [9/50], Training Loss: 1.4211, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 58.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Validation Loss: 1.3079, Validation Accuracy: 48.18%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 8 with accuracy: 48.18%\n",
      "Iteration 0: Loss = 1.5264, Reg Loss = 85.7052, Reconstruct Loss = 0.0000, Cls Loss = 1.5179, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 1.2944, Reg Loss = 79.3669, Reconstruct Loss = 0.0122, Cls Loss = 1.2743, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 1.3120, Reg Loss = 79.0329, Reconstruct Loss = 0.0082, Cls Loss = 1.2959, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 1.3143, Reg Loss = 78.8184, Reconstruct Loss = 0.0100, Cls Loss = 1.2964, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 1.3138, Reg Loss = 78.3893, Reconstruct Loss = 0.0105, Cls Loss = 1.2955, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 1.3124, Reg Loss = 78.3636, Reconstruct Loss = 0.0091, Cls Loss = 1.2955, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 1.3109, Reg Loss = 78.5814, Reconstruct Loss = 0.0076, Cls Loss = 1.2955, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 1.3095, Reg Loss = 78.9624, Reconstruct Loss = 0.0080, Cls Loss = 1.2936, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 1.3068, Reg Loss = 78.8153, Reconstruct Loss = 0.0074, Cls Loss = 1.2915, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.3036, Reg Loss = 78.9501, Reconstruct Loss = 0.0070, Cls Loss = 1.2887, Learning rate = 1.0000e-03\n",
      "Epoch [10/50], Training Loss: 1.3044, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 55.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Validation Loss: 1.1786, Validation Accuracy: 56.95%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 9 with accuracy: 56.95%\n",
      "Iteration 0: Loss = 1.4018, Reg Loss = 71.8321, Reconstruct Loss = 0.0000, Cls Loss = 1.3946, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 1.3135, Reg Loss = 79.0335, Reconstruct Loss = 0.0034, Cls Loss = 1.3023, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 1.2908, Reg Loss = 79.7084, Reconstruct Loss = 0.0017, Cls Loss = 1.2811, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 1.2834, Reg Loss = 79.5901, Reconstruct Loss = 0.0037, Cls Loss = 1.2717, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 1.2733, Reg Loss = 80.2010, Reconstruct Loss = 0.0038, Cls Loss = 1.2615, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 1.2654, Reg Loss = 80.7228, Reconstruct Loss = 0.0048, Cls Loss = 1.2525, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 1.2655, Reg Loss = 80.7283, Reconstruct Loss = 0.0059, Cls Loss = 1.2516, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 1.2570, Reg Loss = 80.8904, Reconstruct Loss = 0.0070, Cls Loss = 1.2420, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 1.2548, Reg Loss = 81.3978, Reconstruct Loss = 0.0067, Cls Loss = 1.2400, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.2487, Reg Loss = 81.6603, Reconstruct Loss = 0.0065, Cls Loss = 1.2340, Learning rate = 1.0000e-03\n",
      "Epoch [11/50], Training Loss: 1.2470, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Validation Loss: 1.1611, Validation Accuracy: 54.12%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 1.2745, Reg Loss = 74.7574, Reconstruct Loss = 0.0000, Cls Loss = 1.2670, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 1.2641, Reg Loss = 82.6557, Reconstruct Loss = 0.0206, Cls Loss = 1.2353, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 1.2253, Reg Loss = 83.4716, Reconstruct Loss = 0.0143, Cls Loss = 1.2027, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 1.2168, Reg Loss = 83.8024, Reconstruct Loss = 0.0111, Cls Loss = 1.1973, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 1.2105, Reg Loss = 83.6409, Reconstruct Loss = 0.0109, Cls Loss = 1.1913, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 1.2038, Reg Loss = 83.5626, Reconstruct Loss = 0.0096, Cls Loss = 1.1858, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 1.2004, Reg Loss = 83.2781, Reconstruct Loss = 0.0097, Cls Loss = 1.1823, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 1.2003, Reg Loss = 82.9834, Reconstruct Loss = 0.0090, Cls Loss = 1.1829, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 1.2020, Reg Loss = 82.8949, Reconstruct Loss = 0.0090, Cls Loss = 1.1848, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.1999, Reg Loss = 82.8487, Reconstruct Loss = 0.0096, Cls Loss = 1.1821, Learning rate = 1.0000e-03\n",
      "Epoch [12/50], Training Loss: 1.2012, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Validation Loss: 1.0854, Validation Accuracy: 62.91%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 11 with accuracy: 62.91%\n",
      "Iteration 0: Loss = 1.1142, Reg Loss = 77.0061, Reconstruct Loss = 0.0000, Cls Loss = 1.1065, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 1.1644, Reg Loss = 82.6709, Reconstruct Loss = 0.0048, Cls Loss = 1.1513, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 1.1716, Reg Loss = 81.9297, Reconstruct Loss = 0.0069, Cls Loss = 1.1565, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 1.1677, Reg Loss = 82.5968, Reconstruct Loss = 0.0088, Cls Loss = 1.1506, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 1.1625, Reg Loss = 82.8605, Reconstruct Loss = 0.0104, Cls Loss = 1.1438, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 1.1549, Reg Loss = 82.7746, Reconstruct Loss = 0.0122, Cls Loss = 1.1344, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 1.1537, Reg Loss = 82.5160, Reconstruct Loss = 0.0110, Cls Loss = 1.1345, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 1.1507, Reg Loss = 82.5922, Reconstruct Loss = 0.0100, Cls Loss = 1.1324, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 1.1427, Reg Loss = 82.8395, Reconstruct Loss = 0.0094, Cls Loss = 1.1250, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.1404, Reg Loss = 82.8989, Reconstruct Loss = 0.0094, Cls Loss = 1.1227, Learning rate = 1.0000e-03\n",
      "Epoch [13/50], Training Loss: 1.1419, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Validation Loss: 1.0465, Validation Accuracy: 64.31%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 12 with accuracy: 64.31%\n",
      "Iteration 0: Loss = 1.3181, Reg Loss = 77.8500, Reconstruct Loss = 0.0000, Cls Loss = 1.3103, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 1.1329, Reg Loss = 81.7974, Reconstruct Loss = 0.0085, Cls Loss = 1.1163, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 1.1266, Reg Loss = 83.0349, Reconstruct Loss = 0.0043, Cls Loss = 1.1140, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 1.1205, Reg Loss = 83.2570, Reconstruct Loss = 0.0045, Cls Loss = 1.1077, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 1.1113, Reg Loss = 83.0288, Reconstruct Loss = 0.0057, Cls Loss = 1.0973, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 1.1132, Reg Loss = 83.2034, Reconstruct Loss = 0.0074, Cls Loss = 1.0975, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 1.1112, Reg Loss = 83.3857, Reconstruct Loss = 0.0076, Cls Loss = 1.0952, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 1.1025, Reg Loss = 83.1638, Reconstruct Loss = 0.0081, Cls Loss = 1.0861, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 1.0966, Reg Loss = 83.1419, Reconstruct Loss = 0.0078, Cls Loss = 1.0805, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.0902, Reg Loss = 82.9746, Reconstruct Loss = 0.0080, Cls Loss = 1.0739, Learning rate = 1.0000e-03\n",
      "Epoch [14/50], Training Loss: 1.0878, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 60.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Validation Loss: 0.9655, Validation Accuracy: 67.68%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 13 with accuracy: 67.68%\n",
      "Iteration 0: Loss = 0.9838, Reg Loss = 69.4891, Reconstruct Loss = 0.0000, Cls Loss = 0.9769, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 1.0633, Reg Loss = 80.1050, Reconstruct Loss = 0.0172, Cls Loss = 1.0381, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 1.0441, Reg Loss = 81.5505, Reconstruct Loss = 0.0109, Cls Loss = 1.0251, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 1.0374, Reg Loss = 81.7882, Reconstruct Loss = 0.0105, Cls Loss = 1.0187, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 1.0355, Reg Loss = 81.4472, Reconstruct Loss = 0.0090, Cls Loss = 1.0183, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 1.0290, Reg Loss = 81.1436, Reconstruct Loss = 0.0095, Cls Loss = 1.0114, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 1.0238, Reg Loss = 80.7352, Reconstruct Loss = 0.0108, Cls Loss = 1.0050, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 1.0193, Reg Loss = 80.4832, Reconstruct Loss = 0.0101, Cls Loss = 1.0011, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 1.0228, Reg Loss = 80.3551, Reconstruct Loss = 0.0117, Cls Loss = 1.0030, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 1.0209, Reg Loss = 80.0957, Reconstruct Loss = 0.0110, Cls Loss = 1.0019, Learning rate = 1.0000e-03\n",
      "Epoch [15/50], Training Loss: 1.0185, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 54.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Validation Loss: 0.8734, Validation Accuracy: 73.56%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 14 with accuracy: 73.56%\n",
      "Iteration 0: Loss = 0.9533, Reg Loss = 77.7465, Reconstruct Loss = 0.0000, Cls Loss = 0.9455, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.9899, Reg Loss = 80.8359, Reconstruct Loss = 0.0105, Cls Loss = 0.9713, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.9911, Reg Loss = 80.0213, Reconstruct Loss = 0.0106, Cls Loss = 0.9725, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.9786, Reg Loss = 78.4918, Reconstruct Loss = 0.0071, Cls Loss = 0.9637, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.9721, Reg Loss = 78.3093, Reconstruct Loss = 0.0089, Cls Loss = 0.9554, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.9732, Reg Loss = 77.9566, Reconstruct Loss = 0.0086, Cls Loss = 0.9568, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.9745, Reg Loss = 77.7486, Reconstruct Loss = 0.0115, Cls Loss = 0.9552, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.9712, Reg Loss = 77.6679, Reconstruct Loss = 0.0122, Cls Loss = 0.9512, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.9674, Reg Loss = 77.6590, Reconstruct Loss = 0.0128, Cls Loss = 0.9468, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.9640, Reg Loss = 77.7080, Reconstruct Loss = 0.0114, Cls Loss = 0.9448, Learning rate = 1.0000e-03\n",
      "Epoch [16/50], Training Loss: 0.9634, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 54.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Validation Loss: 0.8346, Validation Accuracy: 72.79%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.8268, Reg Loss = 77.5219, Reconstruct Loss = 0.0000, Cls Loss = 0.8191, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.9185, Reg Loss = 78.1783, Reconstruct Loss = 0.0000, Cls Loss = 0.9107, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.9353, Reg Loss = 79.5333, Reconstruct Loss = 0.0107, Cls Loss = 0.9167, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.9376, Reg Loss = 79.5960, Reconstruct Loss = 0.0137, Cls Loss = 0.9160, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.9229, Reg Loss = 79.3679, Reconstruct Loss = 0.0132, Cls Loss = 0.9017, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.9202, Reg Loss = 79.1170, Reconstruct Loss = 0.0128, Cls Loss = 0.8995, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.9163, Reg Loss = 78.5704, Reconstruct Loss = 0.0131, Cls Loss = 0.8953, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.9096, Reg Loss = 78.1651, Reconstruct Loss = 0.0127, Cls Loss = 0.8890, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.9021, Reg Loss = 78.1621, Reconstruct Loss = 0.0112, Cls Loss = 0.8831, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.9022, Reg Loss = 78.0596, Reconstruct Loss = 0.0105, Cls Loss = 0.8839, Learning rate = 1.0000e-03\n",
      "Epoch [17/50], Training Loss: 0.9010, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:03<00:00, 25.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Validation Loss: 0.7790, Validation Accuracy: 77.14%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 16 with accuracy: 77.14%\n",
      "Iteration 0: Loss = 0.9311, Reg Loss = 80.2649, Reconstruct Loss = 0.0000, Cls Loss = 0.9231, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.8518, Reg Loss = 77.4005, Reconstruct Loss = 0.0108, Cls Loss = 0.8332, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.8492, Reg Loss = 76.9578, Reconstruct Loss = 0.0198, Cls Loss = 0.8218, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.8461, Reg Loss = 76.7032, Reconstruct Loss = 0.0132, Cls Loss = 0.8252, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.8387, Reg Loss = 77.3415, Reconstruct Loss = 0.0162, Cls Loss = 0.8148, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.8278, Reg Loss = 77.3810, Reconstruct Loss = 0.0130, Cls Loss = 0.8071, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.8279, Reg Loss = 77.3043, Reconstruct Loss = 0.0142, Cls Loss = 0.8060, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.8276, Reg Loss = 77.1965, Reconstruct Loss = 0.0122, Cls Loss = 0.8077, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.8181, Reg Loss = 77.4201, Reconstruct Loss = 0.0107, Cls Loss = 0.7997, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.8164, Reg Loss = 77.3205, Reconstruct Loss = 0.0114, Cls Loss = 0.7973, Learning rate = 1.0000e-03\n",
      "Epoch [18/50], Training Loss: 0.8132, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:03<00:00, 24.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Validation Loss: 0.6731, Validation Accuracy: 79.66%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 17 with accuracy: 79.66%\n",
      "Iteration 0: Loss = 1.1698, Reg Loss = 67.9531, Reconstruct Loss = 0.0000, Cls Loss = 1.1630, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.7692, Reg Loss = 75.8098, Reconstruct Loss = 0.0153, Cls Loss = 0.7463, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.7617, Reg Loss = 75.9635, Reconstruct Loss = 0.0138, Cls Loss = 0.7402, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.7617, Reg Loss = 75.3843, Reconstruct Loss = 0.0144, Cls Loss = 0.7398, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.7666, Reg Loss = 75.3170, Reconstruct Loss = 0.0186, Cls Loss = 0.7405, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.7642, Reg Loss = 75.3836, Reconstruct Loss = 0.0189, Cls Loss = 0.7377, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.7497, Reg Loss = 75.1986, Reconstruct Loss = 0.0174, Cls Loss = 0.7248, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.7506, Reg Loss = 75.2832, Reconstruct Loss = 0.0149, Cls Loss = 0.7281, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.7493, Reg Loss = 75.3398, Reconstruct Loss = 0.0150, Cls Loss = 0.7267, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.7421, Reg Loss = 75.3420, Reconstruct Loss = 0.0151, Cls Loss = 0.7195, Learning rate = 1.0000e-03\n",
      "Epoch [19/50], Training Loss: 0.7407, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:03<00:00, 25.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Validation Loss: 0.5659, Validation Accuracy: 82.57%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 18 with accuracy: 82.57%\n",
      "Iteration 0: Loss = 0.8489, Reg Loss = 78.4439, Reconstruct Loss = 0.0000, Cls Loss = 0.8410, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.6639, Reg Loss = 75.7837, Reconstruct Loss = 0.0135, Cls Loss = 0.6429, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.6635, Reg Loss = 75.6298, Reconstruct Loss = 0.0097, Cls Loss = 0.6463, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.6617, Reg Loss = 75.7780, Reconstruct Loss = 0.0105, Cls Loss = 0.6436, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.6627, Reg Loss = 75.6393, Reconstruct Loss = 0.0106, Cls Loss = 0.6445, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.6629, Reg Loss = 75.5566, Reconstruct Loss = 0.0124, Cls Loss = 0.6429, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.6593, Reg Loss = 75.1832, Reconstruct Loss = 0.0103, Cls Loss = 0.6414, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.6564, Reg Loss = 75.1121, Reconstruct Loss = 0.0098, Cls Loss = 0.6390, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.6504, Reg Loss = 75.2715, Reconstruct Loss = 0.0102, Cls Loss = 0.6327, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.6488, Reg Loss = 75.2789, Reconstruct Loss = 0.0096, Cls Loss = 0.6316, Learning rate = 1.0000e-03\n",
      "Epoch [20/50], Training Loss: 0.6469, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 26.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Validation Loss: 0.5069, Validation Accuracy: 85.22%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 19 with accuracy: 85.22%\n",
      "Iteration 0: Loss = 0.5683, Reg Loss = 69.0108, Reconstruct Loss = 0.0000, Cls Loss = 0.5614, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.6348, Reg Loss = 75.4033, Reconstruct Loss = 0.0118, Cls Loss = 0.6154, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.6308, Reg Loss = 75.8486, Reconstruct Loss = 0.0115, Cls Loss = 0.6117, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.6192, Reg Loss = 75.6403, Reconstruct Loss = 0.0154, Cls Loss = 0.5962, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.6140, Reg Loss = 75.1585, Reconstruct Loss = 0.0169, Cls Loss = 0.5896, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.6126, Reg Loss = 74.8568, Reconstruct Loss = 0.0178, Cls Loss = 0.5873, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.6093, Reg Loss = 74.6950, Reconstruct Loss = 0.0166, Cls Loss = 0.5852, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.6074, Reg Loss = 74.6277, Reconstruct Loss = 0.0164, Cls Loss = 0.5835, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.6055, Reg Loss = 74.4540, Reconstruct Loss = 0.0166, Cls Loss = 0.5814, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.6056, Reg Loss = 74.3846, Reconstruct Loss = 0.0163, Cls Loss = 0.5819, Learning rate = 1.0000e-03\n",
      "Epoch [21/50], Training Loss: 0.6037, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:03<00:00, 24.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Validation Loss: 0.4686, Validation Accuracy: 86.40%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 20 with accuracy: 86.40%\n",
      "Iteration 0: Loss = 0.7001, Reg Loss = 60.6686, Reconstruct Loss = 0.0000, Cls Loss = 0.6941, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.5934, Reg Loss = 70.7896, Reconstruct Loss = 0.0048, Cls Loss = 0.5815, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.5688, Reg Loss = 71.6947, Reconstruct Loss = 0.0048, Cls Loss = 0.5568, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.5515, Reg Loss = 72.2543, Reconstruct Loss = 0.0082, Cls Loss = 0.5360, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.5513, Reg Loss = 71.8376, Reconstruct Loss = 0.0062, Cls Loss = 0.5379, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.5507, Reg Loss = 71.8857, Reconstruct Loss = 0.0072, Cls Loss = 0.5363, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.5501, Reg Loss = 71.7766, Reconstruct Loss = 0.0077, Cls Loss = 0.5352, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.5496, Reg Loss = 71.8219, Reconstruct Loss = 0.0088, Cls Loss = 0.5336, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.5476, Reg Loss = 71.8185, Reconstruct Loss = 0.0085, Cls Loss = 0.5320, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.5450, Reg Loss = 71.7998, Reconstruct Loss = 0.0081, Cls Loss = 0.5297, Learning rate = 1.0000e-03\n",
      "Epoch [22/50], Training Loss: 0.5441, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Validation Loss: 0.4389, Validation Accuracy: 86.62%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 21 with accuracy: 86.62%\n",
      "Iteration 0: Loss = 0.5600, Reg Loss = 77.7009, Reconstruct Loss = 0.0000, Cls Loss = 0.5522, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.5110, Reg Loss = 71.3886, Reconstruct Loss = 0.0053, Cls Loss = 0.4986, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.5105, Reg Loss = 71.2684, Reconstruct Loss = 0.0053, Cls Loss = 0.4981, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.5088, Reg Loss = 71.3861, Reconstruct Loss = 0.0069, Cls Loss = 0.4948, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.5106, Reg Loss = 71.4679, Reconstruct Loss = 0.0091, Cls Loss = 0.4943, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.5104, Reg Loss = 71.4403, Reconstruct Loss = 0.0083, Cls Loss = 0.4949, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.5114, Reg Loss = 71.4117, Reconstruct Loss = 0.0089, Cls Loss = 0.4954, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.5113, Reg Loss = 71.2492, Reconstruct Loss = 0.0085, Cls Loss = 0.4957, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.5128, Reg Loss = 71.1142, Reconstruct Loss = 0.0074, Cls Loss = 0.4982, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.5078, Reg Loss = 71.2333, Reconstruct Loss = 0.0066, Cls Loss = 0.4941, Learning rate = 1.0000e-03\n",
      "Epoch [23/50], Training Loss: 0.5066, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50], Validation Loss: 0.3692, Validation Accuracy: 89.96%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 22 with accuracy: 89.96%\n",
      "Iteration 0: Loss = 0.4876, Reg Loss = 72.4189, Reconstruct Loss = 0.0000, Cls Loss = 0.4804, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.4996, Reg Loss = 70.1117, Reconstruct Loss = 0.0153, Cls Loss = 0.4773, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.4946, Reg Loss = 70.5766, Reconstruct Loss = 0.0205, Cls Loss = 0.4670, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.4814, Reg Loss = 70.3072, Reconstruct Loss = 0.0173, Cls Loss = 0.4571, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.4807, Reg Loss = 70.1020, Reconstruct Loss = 0.0163, Cls Loss = 0.4574, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.4805, Reg Loss = 70.1019, Reconstruct Loss = 0.0146, Cls Loss = 0.4589, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.4802, Reg Loss = 70.0051, Reconstruct Loss = 0.0128, Cls Loss = 0.4604, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.4808, Reg Loss = 69.8934, Reconstruct Loss = 0.0117, Cls Loss = 0.4621, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.4757, Reg Loss = 70.0868, Reconstruct Loss = 0.0127, Cls Loss = 0.4560, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.4761, Reg Loss = 70.0574, Reconstruct Loss = 0.0118, Cls Loss = 0.4573, Learning rate = 1.0000e-03\n",
      "Epoch [24/50], Training Loss: 0.4773, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 55.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50], Validation Loss: 0.3366, Validation Accuracy: 90.13%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 23 with accuracy: 90.13%\n",
      "Iteration 0: Loss = 0.5035, Reg Loss = 72.1487, Reconstruct Loss = 0.2216, Cls Loss = 0.2747, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.4657, Reg Loss = 69.0437, Reconstruct Loss = 0.0131, Cls Loss = 0.4457, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.4495, Reg Loss = 69.4000, Reconstruct Loss = 0.0094, Cls Loss = 0.4331, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.4505, Reg Loss = 69.4698, Reconstruct Loss = 0.0094, Cls Loss = 0.4341, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.4443, Reg Loss = 69.5824, Reconstruct Loss = 0.0080, Cls Loss = 0.4293, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.4416, Reg Loss = 69.5599, Reconstruct Loss = 0.0075, Cls Loss = 0.4272, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.4425, Reg Loss = 69.3962, Reconstruct Loss = 0.0076, Cls Loss = 0.4280, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.4437, Reg Loss = 69.2517, Reconstruct Loss = 0.0072, Cls Loss = 0.4295, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.4426, Reg Loss = 69.2853, Reconstruct Loss = 0.0079, Cls Loss = 0.4278, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.4424, Reg Loss = 69.3285, Reconstruct Loss = 0.0075, Cls Loss = 0.4279, Learning rate = 1.0000e-03\n",
      "Epoch [25/50], Training Loss: 0.4436, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 58.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Validation Loss: 0.3164, Validation Accuracy: 90.88%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 24 with accuracy: 90.88%\n",
      "Iteration 0: Loss = 0.4496, Reg Loss = 66.2635, Reconstruct Loss = 0.0000, Cls Loss = 0.4429, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.4057, Reg Loss = 69.2718, Reconstruct Loss = 0.0058, Cls Loss = 0.3931, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.4099, Reg Loss = 69.8949, Reconstruct Loss = 0.0079, Cls Loss = 0.3949, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.4080, Reg Loss = 69.5371, Reconstruct Loss = 0.0070, Cls Loss = 0.3941, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.4300, Reg Loss = 69.0923, Reconstruct Loss = 0.0103, Cls Loss = 0.4128, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.4287, Reg Loss = 68.9679, Reconstruct Loss = 0.0118, Cls Loss = 0.4101, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.4281, Reg Loss = 68.6160, Reconstruct Loss = 0.0098, Cls Loss = 0.4114, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.4266, Reg Loss = 68.3762, Reconstruct Loss = 0.0084, Cls Loss = 0.4113, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.4226, Reg Loss = 68.4012, Reconstruct Loss = 0.0085, Cls Loss = 0.4072, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.4194, Reg Loss = 68.2730, Reconstruct Loss = 0.0085, Cls Loss = 0.4040, Learning rate = 1.0000e-03\n",
      "Epoch [26/50], Training Loss: 0.4206, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 56.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/50], Validation Loss: 0.2986, Validation Accuracy: 91.64%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 25 with accuracy: 91.64%\n",
      "Iteration 0: Loss = 0.4414, Reg Loss = 65.1355, Reconstruct Loss = 0.0000, Cls Loss = 0.4349, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.3953, Reg Loss = 67.4670, Reconstruct Loss = 0.0093, Cls Loss = 0.3792, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.3932, Reg Loss = 67.1987, Reconstruct Loss = 0.0067, Cls Loss = 0.3798, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.3953, Reg Loss = 67.3983, Reconstruct Loss = 0.0100, Cls Loss = 0.3786, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.3963, Reg Loss = 67.6244, Reconstruct Loss = 0.0094, Cls Loss = 0.3802, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.4019, Reg Loss = 67.6319, Reconstruct Loss = 0.0092, Cls Loss = 0.3859, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.3990, Reg Loss = 67.6446, Reconstruct Loss = 0.0084, Cls Loss = 0.3839, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.3994, Reg Loss = 67.7844, Reconstruct Loss = 0.0097, Cls Loss = 0.3829, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.3995, Reg Loss = 67.6733, Reconstruct Loss = 0.0099, Cls Loss = 0.3828, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.4005, Reg Loss = 67.4982, Reconstruct Loss = 0.0097, Cls Loss = 0.3841, Learning rate = 1.0000e-03\n",
      "Epoch [27/50], Training Loss: 0.4016, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 55.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50], Validation Loss: 0.2761, Validation Accuracy: 92.34%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 26 with accuracy: 92.34%\n",
      "Iteration 0: Loss = 0.4027, Reg Loss = 67.8740, Reconstruct Loss = 0.0000, Cls Loss = 0.3959, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.3595, Reg Loss = 67.2611, Reconstruct Loss = 0.0032, Cls Loss = 0.3496, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.3800, Reg Loss = 67.9854, Reconstruct Loss = 0.0035, Cls Loss = 0.3697, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.3757, Reg Loss = 68.1421, Reconstruct Loss = 0.0048, Cls Loss = 0.3641, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.3712, Reg Loss = 68.5387, Reconstruct Loss = 0.0049, Cls Loss = 0.3595, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.3713, Reg Loss = 68.6608, Reconstruct Loss = 0.0057, Cls Loss = 0.3586, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.3732, Reg Loss = 68.4245, Reconstruct Loss = 0.0052, Cls Loss = 0.3612, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.3732, Reg Loss = 68.3834, Reconstruct Loss = 0.0056, Cls Loss = 0.3608, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.3726, Reg Loss = 68.3411, Reconstruct Loss = 0.0059, Cls Loss = 0.3599, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.3760, Reg Loss = 68.2908, Reconstruct Loss = 0.0057, Cls Loss = 0.3635, Learning rate = 1.0000e-03\n",
      "Epoch [28/50], Training Loss: 0.3773, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50], Validation Loss: 0.2812, Validation Accuracy: 91.78%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.4038, Reg Loss = 72.6365, Reconstruct Loss = 0.0000, Cls Loss = 0.3966, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.3831, Reg Loss = 69.0644, Reconstruct Loss = 0.0050, Cls Loss = 0.3713, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.3813, Reg Loss = 68.5217, Reconstruct Loss = 0.0046, Cls Loss = 0.3698, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.3747, Reg Loss = 68.5725, Reconstruct Loss = 0.0059, Cls Loss = 0.3619, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.3707, Reg Loss = 68.8059, Reconstruct Loss = 0.0056, Cls Loss = 0.3583, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.3706, Reg Loss = 68.7120, Reconstruct Loss = 0.0055, Cls Loss = 0.3583, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.3675, Reg Loss = 68.5756, Reconstruct Loss = 0.0052, Cls Loss = 0.3554, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.3686, Reg Loss = 68.5384, Reconstruct Loss = 0.0082, Cls Loss = 0.3535, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.3669, Reg Loss = 68.3450, Reconstruct Loss = 0.0072, Cls Loss = 0.3528, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.3664, Reg Loss = 68.2694, Reconstruct Loss = 0.0072, Cls Loss = 0.3523, Learning rate = 1.0000e-03\n",
      "Epoch [29/50], Training Loss: 0.3663, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 58.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50], Validation Loss: 0.2771, Validation Accuracy: 92.14%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.3359, Reg Loss = 70.7402, Reconstruct Loss = 0.0000, Cls Loss = 0.3288, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.3515, Reg Loss = 68.1726, Reconstruct Loss = 0.0075, Cls Loss = 0.3372, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.3643, Reg Loss = 68.2131, Reconstruct Loss = 0.0075, Cls Loss = 0.3500, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.3713, Reg Loss = 68.4420, Reconstruct Loss = 0.0081, Cls Loss = 0.3564, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.3663, Reg Loss = 68.4118, Reconstruct Loss = 0.0078, Cls Loss = 0.3517, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.3686, Reg Loss = 68.1530, Reconstruct Loss = 0.0082, Cls Loss = 0.3535, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.3666, Reg Loss = 68.0313, Reconstruct Loss = 0.0068, Cls Loss = 0.3530, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.3672, Reg Loss = 67.9926, Reconstruct Loss = 0.0068, Cls Loss = 0.3536, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.3640, Reg Loss = 68.0014, Reconstruct Loss = 0.0069, Cls Loss = 0.3503, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.3630, Reg Loss = 67.9460, Reconstruct Loss = 0.0070, Cls Loss = 0.3492, Learning rate = 1.0000e-03\n",
      "Epoch [30/50], Training Loss: 0.3624, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Validation Loss: 0.2644, Validation Accuracy: 92.51%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 29 with accuracy: 92.51%\n",
      "Iteration 0: Loss = 0.3256, Reg Loss = 71.0495, Reconstruct Loss = 0.0000, Cls Loss = 0.3185, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.3664, Reg Loss = 68.0919, Reconstruct Loss = 0.0070, Cls Loss = 0.3526, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.3642, Reg Loss = 67.5287, Reconstruct Loss = 0.0082, Cls Loss = 0.3492, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.3623, Reg Loss = 67.5349, Reconstruct Loss = 0.0096, Cls Loss = 0.3460, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.3680, Reg Loss = 67.3567, Reconstruct Loss = 0.0085, Cls Loss = 0.3528, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.3609, Reg Loss = 67.3315, Reconstruct Loss = 0.0074, Cls Loss = 0.3468, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.3596, Reg Loss = 67.3443, Reconstruct Loss = 0.0077, Cls Loss = 0.3451, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.3556, Reg Loss = 67.5369, Reconstruct Loss = 0.0066, Cls Loss = 0.3422, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.3522, Reg Loss = 67.7113, Reconstruct Loss = 0.0066, Cls Loss = 0.3388, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.3502, Reg Loss = 67.8577, Reconstruct Loss = 0.0067, Cls Loss = 0.3367, Learning rate = 1.0000e-03\n",
      "Epoch [31/50], Training Loss: 0.3510, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 56.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50], Validation Loss: 0.2652, Validation Accuracy: 92.26%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.4010, Reg Loss = 64.3212, Reconstruct Loss = 0.0000, Cls Loss = 0.3946, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.3536, Reg Loss = 68.4525, Reconstruct Loss = 0.0035, Cls Loss = 0.3433, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.3573, Reg Loss = 68.3075, Reconstruct Loss = 0.0076, Cls Loss = 0.3428, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.3503, Reg Loss = 68.1370, Reconstruct Loss = 0.0051, Cls Loss = 0.3385, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.3423, Reg Loss = 68.0896, Reconstruct Loss = 0.0055, Cls Loss = 0.3300, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.3432, Reg Loss = 67.9193, Reconstruct Loss = 0.0050, Cls Loss = 0.3314, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.3405, Reg Loss = 68.0311, Reconstruct Loss = 0.0048, Cls Loss = 0.3289, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.3403, Reg Loss = 68.0031, Reconstruct Loss = 0.0051, Cls Loss = 0.3284, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.3400, Reg Loss = 68.0398, Reconstruct Loss = 0.0049, Cls Loss = 0.3283, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.3398, Reg Loss = 68.0754, Reconstruct Loss = 0.0060, Cls Loss = 0.3270, Learning rate = 1.0000e-03\n",
      "Epoch [32/50], Training Loss: 0.3400, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 56.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Validation Loss: 0.2532, Validation Accuracy: 93.03%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 31 with accuracy: 93.03%\n",
      "Iteration 0: Loss = 0.3181, Reg Loss = 62.0092, Reconstruct Loss = 0.0000, Cls Loss = 0.3119, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.3404, Reg Loss = 67.5647, Reconstruct Loss = 0.0000, Cls Loss = 0.3337, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.3192, Reg Loss = 67.7281, Reconstruct Loss = 0.0000, Cls Loss = 0.3124, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.3274, Reg Loss = 67.8505, Reconstruct Loss = 0.0041, Cls Loss = 0.3166, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.3264, Reg Loss = 67.9706, Reconstruct Loss = 0.0040, Cls Loss = 0.3156, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.3240, Reg Loss = 68.2208, Reconstruct Loss = 0.0032, Cls Loss = 0.3140, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.3302, Reg Loss = 68.2065, Reconstruct Loss = 0.0032, Cls Loss = 0.3202, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.3269, Reg Loss = 68.3932, Reconstruct Loss = 0.0044, Cls Loss = 0.3156, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.3262, Reg Loss = 68.3942, Reconstruct Loss = 0.0047, Cls Loss = 0.3147, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.3280, Reg Loss = 68.3212, Reconstruct Loss = 0.0041, Cls Loss = 0.3170, Learning rate = 1.0000e-03\n",
      "Epoch [33/50], Training Loss: 0.3260, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 55.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50], Validation Loss: 0.2437, Validation Accuracy: 92.99%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1909, Reg Loss = 73.3430, Reconstruct Loss = 0.0000, Cls Loss = 0.1836, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.3119, Reg Loss = 69.3858, Reconstruct Loss = 0.0000, Cls Loss = 0.3050, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.3255, Reg Loss = 68.9429, Reconstruct Loss = 0.0071, Cls Loss = 0.3116, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.3280, Reg Loss = 68.5775, Reconstruct Loss = 0.0067, Cls Loss = 0.3145, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.3258, Reg Loss = 68.3320, Reconstruct Loss = 0.0056, Cls Loss = 0.3133, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.3226, Reg Loss = 68.4897, Reconstruct Loss = 0.0070, Cls Loss = 0.3087, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.3221, Reg Loss = 68.6097, Reconstruct Loss = 0.0070, Cls Loss = 0.3082, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.3263, Reg Loss = 68.4724, Reconstruct Loss = 0.0065, Cls Loss = 0.3130, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.3232, Reg Loss = 68.6260, Reconstruct Loss = 0.0066, Cls Loss = 0.3098, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.3277, Reg Loss = 68.6246, Reconstruct Loss = 0.0071, Cls Loss = 0.3137, Learning rate = 1.0000e-03\n",
      "Epoch [34/50], Training Loss: 0.3292, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Validation Loss: 0.2421, Validation Accuracy: 93.17%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 33 with accuracy: 93.17%\n",
      "Iteration 0: Loss = 0.2302, Reg Loss = 67.8178, Reconstruct Loss = 0.0000, Cls Loss = 0.2234, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.3193, Reg Loss = 68.2334, Reconstruct Loss = 0.0051, Cls Loss = 0.3074, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.3177, Reg Loss = 68.2094, Reconstruct Loss = 0.0039, Cls Loss = 0.3070, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.3139, Reg Loss = 68.3591, Reconstruct Loss = 0.0038, Cls Loss = 0.3033, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.3120, Reg Loss = 68.6988, Reconstruct Loss = 0.0042, Cls Loss = 0.3010, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.3125, Reg Loss = 68.7178, Reconstruct Loss = 0.0054, Cls Loss = 0.3002, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.3124, Reg Loss = 68.5690, Reconstruct Loss = 0.0055, Cls Loss = 0.3000, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.3127, Reg Loss = 68.6815, Reconstruct Loss = 0.0056, Cls Loss = 0.3002, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.3138, Reg Loss = 68.5449, Reconstruct Loss = 0.0056, Cls Loss = 0.3013, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.3127, Reg Loss = 68.4674, Reconstruct Loss = 0.0053, Cls Loss = 0.3006, Learning rate = 1.0000e-03\n",
      "Epoch [35/50], Training Loss: 0.3124, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 56.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Validation Loss: 0.2412, Validation Accuracy: 93.03%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.2731, Reg Loss = 71.4054, Reconstruct Loss = 0.0000, Cls Loss = 0.2659, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2915, Reg Loss = 70.1661, Reconstruct Loss = 0.0064, Cls Loss = 0.2781, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.3067, Reg Loss = 69.7964, Reconstruct Loss = 0.0072, Cls Loss = 0.2925, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.3061, Reg Loss = 69.1975, Reconstruct Loss = 0.0082, Cls Loss = 0.2909, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.3067, Reg Loss = 68.8646, Reconstruct Loss = 0.0074, Cls Loss = 0.2924, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.3059, Reg Loss = 68.9854, Reconstruct Loss = 0.0068, Cls Loss = 0.2923, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.3065, Reg Loss = 69.0020, Reconstruct Loss = 0.0071, Cls Loss = 0.2925, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.3025, Reg Loss = 69.0593, Reconstruct Loss = 0.0064, Cls Loss = 0.2892, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.3038, Reg Loss = 69.0007, Reconstruct Loss = 0.0056, Cls Loss = 0.2913, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.3049, Reg Loss = 69.1159, Reconstruct Loss = 0.0060, Cls Loss = 0.2920, Learning rate = 1.0000e-03\n",
      "Epoch [36/50], Training Loss: 0.3062, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 58.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Validation Loss: 0.2358, Validation Accuracy: 93.25%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 35 with accuracy: 93.25%\n",
      "Iteration 0: Loss = 0.2855, Reg Loss = 70.7850, Reconstruct Loss = 0.0000, Cls Loss = 0.2784, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2815, Reg Loss = 70.5998, Reconstruct Loss = 0.0059, Cls Loss = 0.2686, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2875, Reg Loss = 70.1609, Reconstruct Loss = 0.0047, Cls Loss = 0.2758, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2922, Reg Loss = 70.1631, Reconstruct Loss = 0.0042, Cls Loss = 0.2810, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2965, Reg Loss = 70.0183, Reconstruct Loss = 0.0045, Cls Loss = 0.2850, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2905, Reg Loss = 70.0608, Reconstruct Loss = 0.0042, Cls Loss = 0.2793, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2928, Reg Loss = 69.8442, Reconstruct Loss = 0.0043, Cls Loss = 0.2815, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2928, Reg Loss = 69.8714, Reconstruct Loss = 0.0054, Cls Loss = 0.2804, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2920, Reg Loss = 69.8284, Reconstruct Loss = 0.0059, Cls Loss = 0.2791, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2935, Reg Loss = 69.7837, Reconstruct Loss = 0.0061, Cls Loss = 0.2804, Learning rate = 1.0000e-03\n",
      "Epoch [37/50], Training Loss: 0.2933, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 58.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/50], Validation Loss: 0.2337, Validation Accuracy: 93.36%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 36 with accuracy: 93.36%\n",
      "Iteration 0: Loss = 0.2266, Reg Loss = 73.5127, Reconstruct Loss = 0.0000, Cls Loss = 0.2192, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2977, Reg Loss = 69.8954, Reconstruct Loss = 0.0076, Cls Loss = 0.2831, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2859, Reg Loss = 69.4830, Reconstruct Loss = 0.0049, Cls Loss = 0.2741, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2879, Reg Loss = 69.8601, Reconstruct Loss = 0.0033, Cls Loss = 0.2776, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2871, Reg Loss = 69.8897, Reconstruct Loss = 0.0040, Cls Loss = 0.2761, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2851, Reg Loss = 70.2056, Reconstruct Loss = 0.0038, Cls Loss = 0.2743, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2855, Reg Loss = 70.2409, Reconstruct Loss = 0.0061, Cls Loss = 0.2724, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2848, Reg Loss = 70.2540, Reconstruct Loss = 0.0061, Cls Loss = 0.2716, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2847, Reg Loss = 70.2861, Reconstruct Loss = 0.0054, Cls Loss = 0.2723, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2843, Reg Loss = 70.4297, Reconstruct Loss = 0.0060, Cls Loss = 0.2713, Learning rate = 1.0000e-03\n",
      "Epoch [38/50], Training Loss: 0.2829, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 58.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50], Validation Loss: 0.2095, Validation Accuracy: 93.84%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 37 with accuracy: 93.84%\n",
      "Iteration 0: Loss = 0.2434, Reg Loss = 71.0692, Reconstruct Loss = 0.0000, Cls Loss = 0.2363, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2900, Reg Loss = 70.6695, Reconstruct Loss = 0.0077, Cls Loss = 0.2752, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2943, Reg Loss = 71.1797, Reconstruct Loss = 0.0053, Cls Loss = 0.2819, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2870, Reg Loss = 71.3208, Reconstruct Loss = 0.0035, Cls Loss = 0.2763, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2838, Reg Loss = 71.1806, Reconstruct Loss = 0.0049, Cls Loss = 0.2719, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2826, Reg Loss = 71.0198, Reconstruct Loss = 0.0054, Cls Loss = 0.2701, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2835, Reg Loss = 70.7406, Reconstruct Loss = 0.0064, Cls Loss = 0.2700, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2831, Reg Loss = 70.6531, Reconstruct Loss = 0.0066, Cls Loss = 0.2695, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2835, Reg Loss = 70.5708, Reconstruct Loss = 0.0066, Cls Loss = 0.2698, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2819, Reg Loss = 70.4734, Reconstruct Loss = 0.0066, Cls Loss = 0.2682, Learning rate = 1.0000e-03\n",
      "Epoch [39/50], Training Loss: 0.2825, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 56.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/50], Validation Loss: 0.2206, Validation Accuracy: 93.53%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.3662, Reg Loss = 65.9061, Reconstruct Loss = 0.0000, Cls Loss = 0.3596, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2644, Reg Loss = 69.6911, Reconstruct Loss = 0.0086, Cls Loss = 0.2488, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2838, Reg Loss = 68.9570, Reconstruct Loss = 0.0081, Cls Loss = 0.2689, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2808, Reg Loss = 68.4184, Reconstruct Loss = 0.0066, Cls Loss = 0.2673, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2776, Reg Loss = 68.2481, Reconstruct Loss = 0.0055, Cls Loss = 0.2653, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2771, Reg Loss = 68.2811, Reconstruct Loss = 0.0049, Cls Loss = 0.2654, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2765, Reg Loss = 68.5032, Reconstruct Loss = 0.0044, Cls Loss = 0.2652, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2754, Reg Loss = 68.8386, Reconstruct Loss = 0.0045, Cls Loss = 0.2640, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2745, Reg Loss = 69.0631, Reconstruct Loss = 0.0048, Cls Loss = 0.2628, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2758, Reg Loss = 69.1720, Reconstruct Loss = 0.0048, Cls Loss = 0.2641, Learning rate = 1.0000e-03\n",
      "Epoch [40/50], Training Loss: 0.2760, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Validation Loss: 0.2022, Validation Accuracy: 94.04%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 39 with accuracy: 94.04%\n",
      "Iteration 0: Loss = 0.2845, Reg Loss = 63.5304, Reconstruct Loss = 0.0000, Cls Loss = 0.2781, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2646, Reg Loss = 70.2348, Reconstruct Loss = 0.0056, Cls Loss = 0.2519, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2617, Reg Loss = 70.4408, Reconstruct Loss = 0.0059, Cls Loss = 0.2487, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2562, Reg Loss = 70.2054, Reconstruct Loss = 0.0052, Cls Loss = 0.2440, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2610, Reg Loss = 70.1962, Reconstruct Loss = 0.0058, Cls Loss = 0.2482, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2647, Reg Loss = 70.2326, Reconstruct Loss = 0.0062, Cls Loss = 0.2515, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2673, Reg Loss = 70.2506, Reconstruct Loss = 0.0052, Cls Loss = 0.2551, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2698, Reg Loss = 70.3363, Reconstruct Loss = 0.0044, Cls Loss = 0.2583, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2683, Reg Loss = 70.5503, Reconstruct Loss = 0.0048, Cls Loss = 0.2565, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2674, Reg Loss = 70.5189, Reconstruct Loss = 0.0049, Cls Loss = 0.2554, Learning rate = 1.0000e-03\n",
      "Epoch [41/50], Training Loss: 0.2681, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/50], Validation Loss: 0.2037, Validation Accuracy: 94.36%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 40 with accuracy: 94.36%\n",
      "Iteration 0: Loss = 0.4508, Reg Loss = 70.1484, Reconstruct Loss = 0.0000, Cls Loss = 0.4438, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2708, Reg Loss = 68.6780, Reconstruct Loss = 0.0018, Cls Loss = 0.2621, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2664, Reg Loss = 69.3378, Reconstruct Loss = 0.0030, Cls Loss = 0.2565, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2676, Reg Loss = 69.5209, Reconstruct Loss = 0.0038, Cls Loss = 0.2568, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2678, Reg Loss = 69.6677, Reconstruct Loss = 0.0029, Cls Loss = 0.2580, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2630, Reg Loss = 69.6267, Reconstruct Loss = 0.0031, Cls Loss = 0.2530, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2591, Reg Loss = 69.5455, Reconstruct Loss = 0.0042, Cls Loss = 0.2479, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2593, Reg Loss = 69.5607, Reconstruct Loss = 0.0036, Cls Loss = 0.2487, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2606, Reg Loss = 69.5807, Reconstruct Loss = 0.0037, Cls Loss = 0.2499, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2596, Reg Loss = 69.6645, Reconstruct Loss = 0.0033, Cls Loss = 0.2494, Learning rate = 1.0000e-03\n",
      "Epoch [42/50], Training Loss: 0.2588, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 54.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50], Validation Loss: 0.1851, Validation Accuracy: 94.58%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 41 with accuracy: 94.58%\n",
      "Iteration 0: Loss = 0.2851, Reg Loss = 72.6000, Reconstruct Loss = 0.0000, Cls Loss = 0.2778, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2509, Reg Loss = 72.4837, Reconstruct Loss = 0.0073, Cls Loss = 0.2364, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2568, Reg Loss = 71.1737, Reconstruct Loss = 0.0037, Cls Loss = 0.2460, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2599, Reg Loss = 70.6415, Reconstruct Loss = 0.0037, Cls Loss = 0.2491, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2584, Reg Loss = 70.4315, Reconstruct Loss = 0.0033, Cls Loss = 0.2480, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2577, Reg Loss = 70.3123, Reconstruct Loss = 0.0034, Cls Loss = 0.2472, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2566, Reg Loss = 70.3189, Reconstruct Loss = 0.0029, Cls Loss = 0.2467, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2592, Reg Loss = 70.4407, Reconstruct Loss = 0.0039, Cls Loss = 0.2482, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2583, Reg Loss = 70.4891, Reconstruct Loss = 0.0041, Cls Loss = 0.2472, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2593, Reg Loss = 70.4498, Reconstruct Loss = 0.0036, Cls Loss = 0.2486, Learning rate = 1.0000e-03\n",
      "Epoch [43/50], Training Loss: 0.2586, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/50], Validation Loss: 0.2000, Validation Accuracy: 94.10%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.2668, Reg Loss = 75.1230, Reconstruct Loss = 0.0000, Cls Loss = 0.2593, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2324, Reg Loss = 72.4375, Reconstruct Loss = 0.0000, Cls Loss = 0.2252, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2461, Reg Loss = 71.5807, Reconstruct Loss = 0.0038, Cls Loss = 0.2352, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2425, Reg Loss = 71.3724, Reconstruct Loss = 0.0045, Cls Loss = 0.2309, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2422, Reg Loss = 71.1035, Reconstruct Loss = 0.0047, Cls Loss = 0.2304, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2439, Reg Loss = 70.9730, Reconstruct Loss = 0.0051, Cls Loss = 0.2317, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2442, Reg Loss = 70.7128, Reconstruct Loss = 0.0048, Cls Loss = 0.2323, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2464, Reg Loss = 70.7334, Reconstruct Loss = 0.0045, Cls Loss = 0.2348, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2480, Reg Loss = 70.7487, Reconstruct Loss = 0.0044, Cls Loss = 0.2366, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2484, Reg Loss = 70.7724, Reconstruct Loss = 0.0039, Cls Loss = 0.2374, Learning rate = 1.0000e-03\n",
      "Epoch [44/50], Training Loss: 0.2485, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 56.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50], Validation Loss: 0.2001, Validation Accuracy: 93.77%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.1513, Reg Loss = 73.4253, Reconstruct Loss = 0.0000, Cls Loss = 0.1439, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2300, Reg Loss = 71.7829, Reconstruct Loss = 0.0032, Cls Loss = 0.2196, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2344, Reg Loss = 71.5295, Reconstruct Loss = 0.0023, Cls Loss = 0.2249, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2348, Reg Loss = 71.0981, Reconstruct Loss = 0.0039, Cls Loss = 0.2238, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2359, Reg Loss = 70.9907, Reconstruct Loss = 0.0042, Cls Loss = 0.2246, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2317, Reg Loss = 70.8469, Reconstruct Loss = 0.0040, Cls Loss = 0.2205, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2326, Reg Loss = 70.6096, Reconstruct Loss = 0.0039, Cls Loss = 0.2217, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2330, Reg Loss = 70.4998, Reconstruct Loss = 0.0033, Cls Loss = 0.2226, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2341, Reg Loss = 70.4358, Reconstruct Loss = 0.0029, Cls Loss = 0.2242, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2331, Reg Loss = 70.4863, Reconstruct Loss = 0.0037, Cls Loss = 0.2224, Learning rate = 1.0000e-03\n",
      "Epoch [45/50], Training Loss: 0.2334, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50], Validation Loss: 0.1682, Validation Accuracy: 95.01%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 44 with accuracy: 95.01%\n",
      "Iteration 0: Loss = 0.2857, Reg Loss = 71.3004, Reconstruct Loss = 0.0000, Cls Loss = 0.2785, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2071, Reg Loss = 70.8065, Reconstruct Loss = 0.0014, Cls Loss = 0.1986, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2098, Reg Loss = 71.4228, Reconstruct Loss = 0.0023, Cls Loss = 0.2003, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2213, Reg Loss = 71.1432, Reconstruct Loss = 0.0021, Cls Loss = 0.2121, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2208, Reg Loss = 71.0972, Reconstruct Loss = 0.0022, Cls Loss = 0.2114, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2224, Reg Loss = 71.1308, Reconstruct Loss = 0.0024, Cls Loss = 0.2129, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2219, Reg Loss = 71.0818, Reconstruct Loss = 0.0020, Cls Loss = 0.2128, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2285, Reg Loss = 70.9974, Reconstruct Loss = 0.0019, Cls Loss = 0.2195, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2326, Reg Loss = 70.9521, Reconstruct Loss = 0.0025, Cls Loss = 0.2230, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2317, Reg Loss = 70.9021, Reconstruct Loss = 0.0028, Cls Loss = 0.2219, Learning rate = 1.0000e-03\n",
      "Epoch [46/50], Training Loss: 0.2323, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 57.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50], Validation Loss: 0.1724, Validation Accuracy: 94.88%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.2228, Reg Loss = 71.2874, Reconstruct Loss = 0.0731, Cls Loss = 0.1426, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2155, Reg Loss = 68.8987, Reconstruct Loss = 0.0055, Cls Loss = 0.2031, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2308, Reg Loss = 69.7682, Reconstruct Loss = 0.0051, Cls Loss = 0.2187, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2294, Reg Loss = 69.8294, Reconstruct Loss = 0.0039, Cls Loss = 0.2185, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2264, Reg Loss = 70.0183, Reconstruct Loss = 0.0032, Cls Loss = 0.2162, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2227, Reg Loss = 70.0777, Reconstruct Loss = 0.0029, Cls Loss = 0.2129, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2249, Reg Loss = 69.9987, Reconstruct Loss = 0.0024, Cls Loss = 0.2156, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2233, Reg Loss = 70.0915, Reconstruct Loss = 0.0022, Cls Loss = 0.2141, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2234, Reg Loss = 70.0144, Reconstruct Loss = 0.0022, Cls Loss = 0.2142, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2230, Reg Loss = 70.1601, Reconstruct Loss = 0.0022, Cls Loss = 0.2138, Learning rate = 1.0000e-03\n",
      "Epoch [47/50], Training Loss: 0.2226, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 54.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/50], Validation Loss: 0.1744, Validation Accuracy: 94.59%\n",
      "\n",
      "\n",
      "\n",
      "Iteration 0: Loss = 0.2412, Reg Loss = 69.3469, Reconstruct Loss = 0.0000, Cls Loss = 0.2343, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2118, Reg Loss = 70.2150, Reconstruct Loss = 0.0028, Cls Loss = 0.2020, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2171, Reg Loss = 69.7298, Reconstruct Loss = 0.0033, Cls Loss = 0.2068, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2178, Reg Loss = 69.4249, Reconstruct Loss = 0.0051, Cls Loss = 0.2058, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2192, Reg Loss = 69.4637, Reconstruct Loss = 0.0060, Cls Loss = 0.2063, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2202, Reg Loss = 69.3275, Reconstruct Loss = 0.0052, Cls Loss = 0.2080, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2214, Reg Loss = 69.2300, Reconstruct Loss = 0.0046, Cls Loss = 0.2099, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2240, Reg Loss = 69.2904, Reconstruct Loss = 0.0044, Cls Loss = 0.2126, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2219, Reg Loss = 69.5181, Reconstruct Loss = 0.0040, Cls Loss = 0.2109, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2211, Reg Loss = 69.6571, Reconstruct Loss = 0.0036, Cls Loss = 0.2105, Learning rate = 1.0000e-03\n",
      "Epoch [48/50], Training Loss: 0.2212, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 56.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50], Validation Loss: 0.1579, Validation Accuracy: 95.23%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 47 with accuracy: 95.23%\n",
      "Iteration 0: Loss = 0.2193, Reg Loss = 69.5850, Reconstruct Loss = 0.0000, Cls Loss = 0.2124, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2356, Reg Loss = 70.1508, Reconstruct Loss = 0.0026, Cls Loss = 0.2260, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2369, Reg Loss = 70.4525, Reconstruct Loss = 0.0033, Cls Loss = 0.2266, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2312, Reg Loss = 70.6285, Reconstruct Loss = 0.0035, Cls Loss = 0.2206, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2254, Reg Loss = 70.5150, Reconstruct Loss = 0.0030, Cls Loss = 0.2154, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2252, Reg Loss = 70.3911, Reconstruct Loss = 0.0029, Cls Loss = 0.2153, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2233, Reg Loss = 70.4327, Reconstruct Loss = 0.0031, Cls Loss = 0.2132, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2197, Reg Loss = 70.6315, Reconstruct Loss = 0.0030, Cls Loss = 0.2096, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2189, Reg Loss = 70.5552, Reconstruct Loss = 0.0028, Cls Loss = 0.2091, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2188, Reg Loss = 70.5006, Reconstruct Loss = 0.0031, Cls Loss = 0.2087, Learning rate = 1.0000e-03\n",
      "Epoch [49/50], Training Loss: 0.2188, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 53.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/50], Validation Loss: 0.1551, Validation Accuracy: 95.29%\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved at epoch 48 with accuracy: 95.29%\n",
      "Iteration 0: Loss = 0.1248, Reg Loss = 67.5277, Reconstruct Loss = 0.0000, Cls Loss = 0.1180, Learning rate = 1.0000e-03\n",
      "Iteration 50: Loss = 0.2144, Reg Loss = 69.6593, Reconstruct Loss = 0.0012, Cls Loss = 0.2062, Learning rate = 1.0000e-03\n",
      "Iteration 100: Loss = 0.2141, Reg Loss = 69.2963, Reconstruct Loss = 0.0018, Cls Loss = 0.2054, Learning rate = 1.0000e-03\n",
      "Iteration 150: Loss = 0.2151, Reg Loss = 69.5883, Reconstruct Loss = 0.0024, Cls Loss = 0.2057, Learning rate = 1.0000e-03\n",
      "Iteration 200: Loss = 0.2142, Reg Loss = 69.5703, Reconstruct Loss = 0.0028, Cls Loss = 0.2044, Learning rate = 1.0000e-03\n",
      "Iteration 250: Loss = 0.2126, Reg Loss = 69.6513, Reconstruct Loss = 0.0025, Cls Loss = 0.2031, Learning rate = 1.0000e-03\n",
      "Iteration 300: Loss = 0.2110, Reg Loss = 69.5766, Reconstruct Loss = 0.0031, Cls Loss = 0.2010, Learning rate = 1.0000e-03\n",
      "Iteration 350: Loss = 0.2111, Reg Loss = 69.6396, Reconstruct Loss = 0.0030, Cls Loss = 0.2011, Learning rate = 1.0000e-03\n",
      "Iteration 400: Loss = 0.2106, Reg Loss = 69.6531, Reconstruct Loss = 0.0029, Cls Loss = 0.2007, Learning rate = 1.0000e-03\n",
      "Iteration 450: Loss = 0.2085, Reg Loss = 69.6076, Reconstruct Loss = 0.0030, Cls Loss = 0.1986, Learning rate = 1.0000e-03\n",
      "Epoch [50/50], Training Loss: 0.2079, Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 55.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Validation Loss: 0.1527, Validation Accuracy: 95.09%\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the epochs\n",
    "for epoch in range(start_epoch, args.experiment.num_epochs):\n",
    "    train_loss, dim_dict, gt_model_dict = train_one_epoch(hyper_model, train_loader, optimizer, criterion, \n",
    "                                                          dim_dict, gt_model_dict, epoch_idx=epoch, ema=ema, \n",
    "                                                          args=args, device=device)\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print the training loss and learning rate\n",
    "    print(f\"Epoch [{epoch+1}/{args.experiment.num_epochs}], Training Loss: {train_loss:.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    # If it's time to evaluate the model\n",
    "    if (epoch + 1) % args.experiment.eval_interval == 0:\n",
    "        # Apply EMA if it is specified\n",
    "        if ema:\n",
    "            ema.apply()  # Save the weights of original model created before training_loop\n",
    "        \n",
    "        # Sample the merged model (create model of same structure before training loop by using the hypernetwork)\n",
    "        # And then test the performance of the hypernetwork by seeing how good it is in generating the weights\n",
    "        model = sample_merge_model(hyper_model, model, args) \n",
    "        # Validate the merged model\n",
    "        val_loss, acc = validate_single(model, val_loader, val_criterion, args=args)\n",
    "\n",
    "        # If EMA is specified, restore the original weights\n",
    "        if ema:\n",
    "            ema.restore()  # Restore the original weights to the weights of the pretrained networks\n",
    "\n",
    "        # Log the validation loss and accuracy to wandb\n",
    "        wandb.log({\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": acc\n",
    "        })\n",
    "        # Print the validation loss and accuracy\n",
    "        print(f\"Epoch [{epoch+1}/{args.experiment.num_epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc*100:.2f}%\")\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # Save the checkpoint if the accuracy is better than the previous best\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            save_checkpoint(f\"{args.training.save_model_path}/mnist_nerf_best.pth\",hyper_model,optimizer,ema,epoch,best_acc)\n",
    "            print(f\"Checkpoint saved at epoch {epoch} with accuracy: {best_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ee9bae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 51.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Permutated model Validation Loss: 0.2343, Validation Accuracy: 92.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate the model for the starting dimension (its pretrained form)\n",
    "val_loss, acc = validate_single(dim_dict['8'][0], val_loader, nn.CrossEntropyLoss(), args=args)\n",
    "print(f'Initial Permutated model Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b21c607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 46.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Permutated model Validation Loss: 0.1670, Validation Accuracy: 95.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate the model for the starting dimension (its pretrained form)\n",
    "val_loss, acc = validate_single(dim_dict['32'][0], val_loader, nn.CrossEntropyLoss(), args=args)\n",
    "print(f'Initial Permutated model Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17b7b188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 47.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Permutated model Validation Loss: 0.0621, Validation Accuracy: 98.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate the model for the starting dimension (its pretrained form)\n",
    "val_loss, acc = validate_single(gt_model_dict['32'], val_loader, nn.CrossEntropyLoss(), args=args)\n",
    "print(f'Initial Permutated model Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15464fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Cls Loss</td><td>██▇▇▇▇▆▆▆▅▄▄▄▄▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Learning rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Loss</td><td>█▇▇▇█▇▇▇▆▆▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Reconstruct Loss</td><td>▁▂▃▃▁▃▄▄▄▃▄▁█▂▅▆▇▄▄▃▁▄▃▄▃▃▃▂▃▃▃▃▃▃▃▂▁▂▂▂</td></tr><tr><td>Reg Loss</td><td>▁█▇████████▇▇▇▇▇▇▇▇▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Validation Accuracy</td><td>▁▁▁▂▂▂▃▄▄▄▅▅▆▆▆▇▇▇██████████████████████</td></tr><tr><td>Validation Loss</td><td>███▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Cls Loss</td><td>0.1986</td></tr><tr><td>Learning rate</td><td>0.001</td></tr><tr><td>Loss</td><td>0.20854</td></tr><tr><td>Reconstruct Loss</td><td>0.00297</td></tr><tr><td>Reg Loss</td><td>69.60761</td></tr><tr><td>Validation Accuracy</td><td>0.9509</td></tr><tr><td>Validation Loss</td><td>0.15271</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">20250520180300-mnist_lenet_8-32-noise</strong> at: <a href='https://wandb.ai/efradosuryadi-universitas-indonesia/ninr-trial/runs/zkhan3tm' target=\"_blank\">https://wandb.ai/efradosuryadi-universitas-indonesia/ninr-trial/runs/zkhan3tm</a><br> View project at: <a href='https://wandb.ai/efradosuryadi-universitas-indonesia/ninr-trial' target=\"_blank\">https://wandb.ai/efradosuryadi-universitas-indonesia/ninr-trial</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250520_180301-zkhan3tm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# End the wandb tracking\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6054a4",
   "metadata": {},
   "source": [
    "### 7 Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43945b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_hypernet_path = args.training.save_model_path + 'mnist_nerf_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d63bd793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toy/experiments/mnist_lenet_8-32-noise/mnist_nerf_best.pth'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_hypernet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f139172f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper model type: resmlp\n",
      "Using scalar 0.1\n",
      "num_freqs:  16 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "hyper_model_test = get_hypernetwork(args, number_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4431caa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(saved_hypernet_path, map_location=\"cpu\")  # or \"cuda\" if using GPU\n",
    "hyper_model_test.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4eaa63eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 68.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 4, Validation Loss: 1.4290, Validation Accuracy: 60.85%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 68.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 5, Validation Loss: 1.0547, Validation Accuracy: 70.98%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 67.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 6, Validation Loss: 0.9845, Validation Accuracy: 72.42%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 66.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 7, Validation Loss: 0.2909, Validation Accuracy: 91.31%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 64.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 8, Validation Loss: 0.2174, Validation Accuracy: 93.59%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 61.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 9, Validation Loss: 0.3409, Validation Accuracy: 89.95%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 59.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 10, Validation Loss: 0.2725, Validation Accuracy: 91.37%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 55.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 11, Validation Loss: 0.2486, Validation Accuracy: 92.92%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 55.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 12, Validation Loss: 0.1967, Validation Accuracy: 94.17%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 52.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 13, Validation Loss: 0.1894, Validation Accuracy: 94.27%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 52.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 14, Validation Loss: 0.2270, Validation Accuracy: 93.36%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 53.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 15, Validation Loss: 0.1856, Validation Accuracy: 94.59%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 52.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 16, Validation Loss: 0.2015, Validation Accuracy: 94.30%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 50.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 17, Validation Loss: 0.1769, Validation Accuracy: 94.80%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 67.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 18, Validation Loss: 0.1825, Validation Accuracy: 94.71%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 46.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 19, Validation Loss: 0.1751, Validation Accuracy: 94.89%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 68.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 20, Validation Loss: 0.1867, Validation Accuracy: 94.72%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 59.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 21, Validation Loss: 0.1895, Validation Accuracy: 94.50%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 65.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 22, Validation Loss: 0.1827, Validation Accuracy: 94.81%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 62.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 23, Validation Loss: 0.1945, Validation Accuracy: 94.37%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 65.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 24, Validation Loss: 0.1689, Validation Accuracy: 94.88%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 50.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 25, Validation Loss: 0.1736, Validation Accuracy: 94.92%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 50.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 26, Validation Loss: 0.1953, Validation Accuracy: 94.18%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 47.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 27, Validation Loss: 0.1883, Validation Accuracy: 94.59%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 49.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 28, Validation Loss: 0.1845, Validation Accuracy: 94.67%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 50.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 29, Validation Loss: 0.2140, Validation Accuracy: 94.04%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 48.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 30, Validation Loss: 0.1817, Validation Accuracy: 94.73%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 51.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 31, Validation Loss: 0.1744, Validation Accuracy: 94.96%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 49.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 32, Validation Loss: 0.1858, Validation Accuracy: 94.43%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 43.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 33, Validation Loss: 0.1866, Validation Accuracy: 94.78%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 46.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 34, Validation Loss: 0.1861, Validation Accuracy: 94.46%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 42.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 35, Validation Loss: 0.1685, Validation Accuracy: 95.04%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 45.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 36, Validation Loss: 0.1711, Validation Accuracy: 94.99%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 43.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 37, Validation Loss: 0.1733, Validation Accuracy: 95.03%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 45.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 38, Validation Loss: 0.1895, Validation Accuracy: 94.37%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 44.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 39, Validation Loss: 0.1790, Validation Accuracy: 94.68%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 44.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 40, Validation Loss: 0.1702, Validation Accuracy: 94.93%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 42.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 41, Validation Loss: 0.1857, Validation Accuracy: 94.47%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 44.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 42, Validation Loss: 0.1702, Validation Accuracy: 94.88%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 45.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 43, Validation Loss: 0.1814, Validation Accuracy: 94.75%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 43.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 44, Validation Loss: 0.1982, Validation Accuracy: 94.50%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 44.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 45, Validation Loss: 0.1774, Validation Accuracy: 94.93%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 38.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 46, Validation Loss: 0.1801, Validation Accuracy: 95.00%\n",
      "\n",
      "\n",
      "Applying EMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 40.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using model {'type': 'LeNet', 'pretrained_path': 'toy/mnist_MnistNet_dim32.pth', 'smooth': False}: hidden_dim 47, Validation Loss: 0.1736, Validation Accuracy: 94.89%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in range(4, 48):\n",
    "    # Create a model for this given dimension\n",
    "    model = create_model(args.model.type,\n",
    "                         hidden_dim=hidden_dim,\n",
    "                         path=args.model.pretrained_path,\n",
    "                         # smooth=args.model.smooth\n",
    "                         ).to(device)\n",
    "    \n",
    "    # If EMA is specified, apply it\n",
    "    if ema:\n",
    "        print('Applying EMA')\n",
    "        ema.apply()\n",
    "\n",
    "    # Sample the merged model\n",
    "    accumulated_model = sample_merge_model(hyper_model_test, model, args, K=100)\n",
    "\n",
    "    # Validate the merged model\n",
    "    val_loss, acc = validate_single(accumulated_model, val_loader, val_criterion, args=args)\n",
    "\n",
    "    # If EMA is specified, restore the original weights after applying EMA\n",
    "    if ema:\n",
    "        ema.restore()  # Restore the original weights after applying \n",
    "        \n",
    "    # Save the model\n",
    "    save_name = os.path.join(args.training.save_model_path, f\"mnist_{accumulated_model.__class__.__name__}_dim{hidden_dim}_single.pth\")\n",
    "    torch.save(accumulated_model.state_dict(),save_name)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Test using model {args.model}: hidden_dim {hidden_dim}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc*100:.2f}%\")\n",
    "    print('\\n')\n",
    "\n",
    "    # Define the directory and filename structure\n",
    "    filename = f\"mnist_results_{args.experiment.name}.txt\"\n",
    "    filepath = os.path.join(args.training.save_model_path, filename)\n",
    "\n",
    "    # Write the results. 'a' is used to append the results; a new file will be created if it doesn't exist.\n",
    "    with open(filepath, \"a\") as file:\n",
    "        file.write(f\"Hidden_dim: {hidden_dim}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {acc*100:.2f}%\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
